{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "kDrAqK6XzKfF",
        "4p_SQ-_ld6E4",
        "mO6WRJyzuEiV"
      ],
      "mount_file_id": "1plHJGYlGb7Wq3Zxegxzq59CHKaIGf6fG",
      "authorship_tag": "ABX9TyOPfBEqA4qk1s2GOBk/RcCb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard",
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/marikaitiprim/AreaPolygonization-Project/blob/main/MusicGenreClass.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q1: Feedforward Neural Network**"
      ],
      "metadata": {
        "id": "kDrAqK6XzKfF"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " ***Step 1: Load Data (mfccs)***"
      ],
      "metadata": {
        "id": "yvjM6MXz7U41"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gbwfOIOyzJq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "#load files as arrays\n",
        "train_mfccs_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/train/mfccs/X.npy\")\n",
        "train_mfccs_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/train/mfccs/labels.npy\")\n",
        "test_mfccs_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/test/mfccs/X.npy\")\n",
        "test_mfccs_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/test/mfccs/labels.npy\")\n",
        "val_mfccs_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/val/mfccs/X.npy\")\n",
        "val_mfccs_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/val/mfccs/labels.npy\")\n",
        "\n",
        "#give a number to each category\n",
        "categories = [\"classical\",\"hiphop\",\"rock_metal_hardrock\",\"blues\"]\n",
        "num_categories = dict(zip(categories,range(0,4)))\n",
        "\n",
        "#for test set\n",
        "testlabels = (pd.Series(test_mfccs_labels)).map(num_categories)\n",
        "testlabels = list(testlabels)\n",
        "test_data = list(zip(test_mfccs_X,testlabels))\n",
        "\n",
        "#for train set\n",
        "trainlabels = (pd.Series(train_mfccs_labels)).map(num_categories)\n",
        "trainlabels = list(trainlabels)\n",
        "train_data = list(zip(train_mfccs_X,trainlabels))\n",
        "\n",
        "#for val set\n",
        "vallabels = (pd.Series(val_mfccs_labels)).map(num_categories)\n",
        "vallabels = list(vallabels)\n",
        "val_data = list(zip(val_mfccs_X,vallabels))\n",
        "\n",
        "#Dataloaders\n",
        "train_dataloader = DataLoader(train_data,batch_size = 16, shuffle=True)\n",
        "test_dataloader = DataLoader(test_data,batch_size=16,shuffle=False)\n",
        "val_dataloader = DataLoader(val_data,batch_size=16,shuffle=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Define Neural Network***"
      ],
      "metadata": {
        "id": "FmroV1pp-KZu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "class FNN(nn.Module):\n",
        "  def __init__(self, input_dim=26, hidden_dim1=128,hidden_dim2=32, output_dim=4):\n",
        "    super(FNN, self).__init__()\n",
        "    # Linear function 1: 26 --> 128\n",
        "    self.fc1 = nn.Linear(input_dim, hidden_dim1)\n",
        "\n",
        "    # Linear function 2: 128 --> 32\n",
        "    self.fc2 = nn.Linear(hidden_dim1, hidden_dim2)\n",
        "\n",
        "    # Linear function 3: 32 --> 4\n",
        "    self.fc3 = nn.Linear(hidden_dim2, output_dim)\n",
        "\n",
        "  def forward(self, x):\n",
        "    # Linear function 1\n",
        "    out = self.fc1(x)\n",
        "\n",
        "    # Linear function 2\n",
        "    out = self.fc2(out)\n",
        "\n",
        "    # Linear function 3 (readout)\n",
        "    out = self.fc3(out)\n",
        "\n",
        "    return out"
      ],
      "metadata": {
        "id": "ay7BVRW_-y3o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: Define training procedure***"
      ],
      "metadata": {
        "id": "YRSkkdpDFx83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainFNN(num_epochs,optimizer,dataloader,lossfun,fnn):\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "    for i, (X, labels) in enumerate(dataloader):\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = fnn(X)\n",
        "      loss = lossfun(output, labels)\n",
        "\n",
        "      # backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        loss,current = loss.item(), i*len(X)\n",
        "        print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(\"\\n\")\n",
        "  return fnn"
      ],
      "metadata": {
        "id": "2FyK_9G6F6My"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 4: Define evaluation procedure***"
      ],
      "metadata": {
        "id": "2lt5ATgzfbzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def testFNN(dataloader,lossfun,fnn):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  # Iterate through test dataset\n",
        "  with torch.no_grad():\n",
        "    for X, labels in dataloader:\n",
        "      X = X.type(torch.float)\n",
        "      output = fnn(X)\n",
        "      test_loss += lossfun(output, labels).item()\n",
        "      correct += (output.argmax(1)==labels).type(torch.float).sum().item()\n",
        "      y_true.append(labels)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_true = torch.cat(y_true)\n",
        "  y_pred = torch.cat(y_pred)\n",
        "  correct /= size\n",
        "  accuracy = 100*correct\n",
        "  test_loss /= size\n",
        "  f1 = f1_score(y_true,y_pred,average='macro')\n",
        "  cm = confusion_matrix(y_true,y_pred)\n",
        "  return test_loss,f1,accuracy,cm"
      ],
      "metadata": {
        "id": "tTRzh9Z8ZaOw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 5: Network training***"
      ],
      "metadata": {
        "id": "inG1jUZNO1PP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "fnn_model_cpu = FNN()\n",
        "optimizer = torch.optim.SGD(fnn_model_cpu.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "start_train_cpu_time = time.time()\n",
        "fnn_model_cpu = trainFNN(num_epochs,optimizer,train_dataloader,lossfun,fnn_model_cpu)\n",
        "train_cpu_time = time.time() - start_train_cpu_time"
      ],
      "metadata": {
        "id": "y0wj7hyAO_OK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b2b72e4-14f5-4a57-a342-5999583a3919"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "loss: 1.244753 [    0/ 3200]\n",
            "loss: 1.358246 [  160/ 3200]\n",
            "loss: 1.419749 [  320/ 3200]\n",
            "loss: 1.354711 [  480/ 3200]\n",
            "loss: 1.392903 [  640/ 3200]\n",
            "loss: 1.383587 [  800/ 3200]\n",
            "loss: 1.359425 [  960/ 3200]\n",
            "loss: 1.422691 [ 1120/ 3200]\n",
            "loss: 1.332255 [ 1280/ 3200]\n",
            "loss: 1.404573 [ 1440/ 3200]\n",
            "loss: 1.378997 [ 1600/ 3200]\n",
            "loss: 1.391558 [ 1760/ 3200]\n",
            "loss: 1.330912 [ 1920/ 3200]\n",
            "loss: 1.431878 [ 2080/ 3200]\n",
            "loss: 1.328807 [ 2240/ 3200]\n",
            "loss: 1.430459 [ 2400/ 3200]\n",
            "loss: 1.345749 [ 2560/ 3200]\n",
            "loss: 1.369426 [ 2720/ 3200]\n",
            "loss: 1.281264 [ 2880/ 3200]\n",
            "loss: 1.378105 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "loss: 1.332696 [    0/ 3200]\n",
            "loss: 1.314952 [  160/ 3200]\n",
            "loss: 1.411778 [  320/ 3200]\n",
            "loss: 1.358252 [  480/ 3200]\n",
            "loss: 1.352906 [  640/ 3200]\n",
            "loss: 1.344516 [  800/ 3200]\n",
            "loss: 1.308321 [  960/ 3200]\n",
            "loss: 1.356390 [ 1120/ 3200]\n",
            "loss: 1.334760 [ 1280/ 3200]\n",
            "loss: 1.313471 [ 1440/ 3200]\n",
            "loss: 1.366642 [ 1600/ 3200]\n",
            "loss: 1.356893 [ 1760/ 3200]\n",
            "loss: 1.350443 [ 1920/ 3200]\n",
            "loss: 1.420684 [ 2080/ 3200]\n",
            "loss: 1.385525 [ 2240/ 3200]\n",
            "loss: 1.320118 [ 2400/ 3200]\n",
            "loss: 1.352848 [ 2560/ 3200]\n",
            "loss: 1.431856 [ 2720/ 3200]\n",
            "loss: 1.304026 [ 2880/ 3200]\n",
            "loss: 1.305167 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "loss: 1.264765 [    0/ 3200]\n",
            "loss: 1.337178 [  160/ 3200]\n",
            "loss: 1.302289 [  320/ 3200]\n",
            "loss: 1.304486 [  480/ 3200]\n",
            "loss: 1.337734 [  640/ 3200]\n",
            "loss: 1.265779 [  800/ 3200]\n",
            "loss: 1.390191 [  960/ 3200]\n",
            "loss: 1.367715 [ 1120/ 3200]\n",
            "loss: 1.217214 [ 1280/ 3200]\n",
            "loss: 1.349618 [ 1440/ 3200]\n",
            "loss: 1.279508 [ 1600/ 3200]\n",
            "loss: 1.348117 [ 1760/ 3200]\n",
            "loss: 1.308140 [ 1920/ 3200]\n",
            "loss: 1.381090 [ 2080/ 3200]\n",
            "loss: 1.305859 [ 2240/ 3200]\n",
            "loss: 1.333147 [ 2400/ 3200]\n",
            "loss: 1.273386 [ 2560/ 3200]\n",
            "loss: 1.312117 [ 2720/ 3200]\n",
            "loss: 1.337180 [ 2880/ 3200]\n",
            "loss: 1.296684 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "loss: 1.316190 [    0/ 3200]\n",
            "loss: 1.276677 [  160/ 3200]\n",
            "loss: 1.258806 [  320/ 3200]\n",
            "loss: 1.404006 [  480/ 3200]\n",
            "loss: 1.267047 [  640/ 3200]\n",
            "loss: 1.292525 [  800/ 3200]\n",
            "loss: 1.332278 [  960/ 3200]\n",
            "loss: 1.246294 [ 1120/ 3200]\n",
            "loss: 1.380710 [ 1280/ 3200]\n",
            "loss: 1.267595 [ 1440/ 3200]\n",
            "loss: 1.275677 [ 1600/ 3200]\n",
            "loss: 1.291077 [ 1760/ 3200]\n",
            "loss: 1.246260 [ 1920/ 3200]\n",
            "loss: 1.243975 [ 2080/ 3200]\n",
            "loss: 1.267956 [ 2240/ 3200]\n",
            "loss: 1.249473 [ 2400/ 3200]\n",
            "loss: 1.381307 [ 2560/ 3200]\n",
            "loss: 1.266126 [ 2720/ 3200]\n",
            "loss: 1.395650 [ 2880/ 3200]\n",
            "loss: 1.265383 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "loss: 1.245552 [    0/ 3200]\n",
            "loss: 1.335388 [  160/ 3200]\n",
            "loss: 1.262017 [  320/ 3200]\n",
            "loss: 1.362080 [  480/ 3200]\n",
            "loss: 1.230698 [  640/ 3200]\n",
            "loss: 1.168003 [  800/ 3200]\n",
            "loss: 1.183067 [  960/ 3200]\n",
            "loss: 1.368087 [ 1120/ 3200]\n",
            "loss: 1.287752 [ 1280/ 3200]\n",
            "loss: 1.315117 [ 1440/ 3200]\n",
            "loss: 1.330799 [ 1600/ 3200]\n",
            "loss: 1.281575 [ 1760/ 3200]\n",
            "loss: 1.358581 [ 1920/ 3200]\n",
            "loss: 1.280730 [ 2080/ 3200]\n",
            "loss: 1.279417 [ 2240/ 3200]\n",
            "loss: 1.225261 [ 2400/ 3200]\n",
            "loss: 1.311149 [ 2560/ 3200]\n",
            "loss: 1.288819 [ 2720/ 3200]\n",
            "loss: 1.234160 [ 2880/ 3200]\n",
            "loss: 1.316448 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "loss: 1.292129 [    0/ 3200]\n",
            "loss: 1.169189 [  160/ 3200]\n",
            "loss: 1.288512 [  320/ 3200]\n",
            "loss: 1.216139 [  480/ 3200]\n",
            "loss: 1.204018 [  640/ 3200]\n",
            "loss: 1.170150 [  800/ 3200]\n",
            "loss: 1.188079 [  960/ 3200]\n",
            "loss: 1.197712 [ 1120/ 3200]\n",
            "loss: 1.307617 [ 1280/ 3200]\n",
            "loss: 1.273497 [ 1440/ 3200]\n",
            "loss: 1.318429 [ 1600/ 3200]\n",
            "loss: 1.117561 [ 1760/ 3200]\n",
            "loss: 1.186919 [ 1920/ 3200]\n",
            "loss: 1.251057 [ 2080/ 3200]\n",
            "loss: 1.242006 [ 2240/ 3200]\n",
            "loss: 1.335522 [ 2400/ 3200]\n",
            "loss: 1.240414 [ 2560/ 3200]\n",
            "loss: 1.173462 [ 2720/ 3200]\n",
            "loss: 1.188995 [ 2880/ 3200]\n",
            "loss: 1.173836 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "loss: 1.149049 [    0/ 3200]\n",
            "loss: 1.213485 [  160/ 3200]\n",
            "loss: 1.225833 [  320/ 3200]\n",
            "loss: 1.216344 [  480/ 3200]\n",
            "loss: 1.254108 [  640/ 3200]\n",
            "loss: 1.133641 [  800/ 3200]\n",
            "loss: 1.196697 [  960/ 3200]\n",
            "loss: 1.148570 [ 1120/ 3200]\n",
            "loss: 1.321249 [ 1280/ 3200]\n",
            "loss: 1.186253 [ 1440/ 3200]\n",
            "loss: 1.278253 [ 1600/ 3200]\n",
            "loss: 1.182421 [ 1760/ 3200]\n",
            "loss: 1.153431 [ 1920/ 3200]\n",
            "loss: 1.305626 [ 2080/ 3200]\n",
            "loss: 1.210158 [ 2240/ 3200]\n",
            "loss: 1.128929 [ 2400/ 3200]\n",
            "loss: 1.180294 [ 2560/ 3200]\n",
            "loss: 1.051499 [ 2720/ 3200]\n",
            "loss: 1.076335 [ 2880/ 3200]\n",
            "loss: 1.178324 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "loss: 1.322902 [    0/ 3200]\n",
            "loss: 1.349079 [  160/ 3200]\n",
            "loss: 1.097371 [  320/ 3200]\n",
            "loss: 1.251757 [  480/ 3200]\n",
            "loss: 1.158173 [  640/ 3200]\n",
            "loss: 1.182562 [  800/ 3200]\n",
            "loss: 1.160298 [  960/ 3200]\n",
            "loss: 1.377988 [ 1120/ 3200]\n",
            "loss: 1.086082 [ 1280/ 3200]\n",
            "loss: 1.265617 [ 1440/ 3200]\n",
            "loss: 1.218840 [ 1600/ 3200]\n",
            "loss: 1.180027 [ 1760/ 3200]\n",
            "loss: 1.114370 [ 1920/ 3200]\n",
            "loss: 1.115735 [ 2080/ 3200]\n",
            "loss: 1.194212 [ 2240/ 3200]\n",
            "loss: 1.329803 [ 2400/ 3200]\n",
            "loss: 1.141443 [ 2560/ 3200]\n",
            "loss: 1.101920 [ 2720/ 3200]\n",
            "loss: 1.055400 [ 2880/ 3200]\n",
            "loss: 1.252965 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "loss: 1.241655 [    0/ 3200]\n",
            "loss: 1.200089 [  160/ 3200]\n",
            "loss: 1.155735 [  320/ 3200]\n",
            "loss: 1.229832 [  480/ 3200]\n",
            "loss: 1.169901 [  640/ 3200]\n",
            "loss: 1.234408 [  800/ 3200]\n",
            "loss: 1.321877 [  960/ 3200]\n",
            "loss: 1.194161 [ 1120/ 3200]\n",
            "loss: 1.145335 [ 1280/ 3200]\n",
            "loss: 1.172559 [ 1440/ 3200]\n",
            "loss: 1.179909 [ 1600/ 3200]\n",
            "loss: 1.161217 [ 1760/ 3200]\n",
            "loss: 1.234777 [ 1920/ 3200]\n",
            "loss: 1.192222 [ 2080/ 3200]\n",
            "loss: 1.126103 [ 2240/ 3200]\n",
            "loss: 1.265757 [ 2400/ 3200]\n",
            "loss: 1.241349 [ 2560/ 3200]\n",
            "loss: 1.210929 [ 2720/ 3200]\n",
            "loss: 1.082219 [ 2880/ 3200]\n",
            "loss: 1.163626 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "loss: 1.189195 [    0/ 3200]\n",
            "loss: 1.012773 [  160/ 3200]\n",
            "loss: 1.075371 [  320/ 3200]\n",
            "loss: 1.254851 [  480/ 3200]\n",
            "loss: 1.155796 [  640/ 3200]\n",
            "loss: 1.050015 [  800/ 3200]\n",
            "loss: 1.318028 [  960/ 3200]\n",
            "loss: 1.243067 [ 1120/ 3200]\n",
            "loss: 0.915887 [ 1280/ 3200]\n",
            "loss: 1.037031 [ 1440/ 3200]\n",
            "loss: 1.096212 [ 1600/ 3200]\n",
            "loss: 1.127306 [ 1760/ 3200]\n",
            "loss: 1.160787 [ 1920/ 3200]\n",
            "loss: 0.925498 [ 2080/ 3200]\n",
            "loss: 1.094832 [ 2240/ 3200]\n",
            "loss: 1.154484 [ 2400/ 3200]\n",
            "loss: 1.039233 [ 2560/ 3200]\n",
            "loss: 1.095512 [ 2720/ 3200]\n",
            "loss: 1.194156 [ 2880/ 3200]\n",
            "loss: 0.974992 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "loss: 0.965993 [    0/ 3200]\n",
            "loss: 1.080657 [  160/ 3200]\n",
            "loss: 1.123472 [  320/ 3200]\n",
            "loss: 1.167195 [  480/ 3200]\n",
            "loss: 1.188739 [  640/ 3200]\n",
            "loss: 1.008475 [  800/ 3200]\n",
            "loss: 1.216083 [  960/ 3200]\n",
            "loss: 1.123539 [ 1120/ 3200]\n",
            "loss: 1.022254 [ 1280/ 3200]\n",
            "loss: 1.001216 [ 1440/ 3200]\n",
            "loss: 1.092911 [ 1600/ 3200]\n",
            "loss: 1.408236 [ 1760/ 3200]\n",
            "loss: 1.237686 [ 1920/ 3200]\n",
            "loss: 1.113482 [ 2080/ 3200]\n",
            "loss: 1.267249 [ 2240/ 3200]\n",
            "loss: 0.947349 [ 2400/ 3200]\n",
            "loss: 1.112634 [ 2560/ 3200]\n",
            "loss: 1.211586 [ 2720/ 3200]\n",
            "loss: 1.075334 [ 2880/ 3200]\n",
            "loss: 0.926566 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "loss: 0.872012 [    0/ 3200]\n",
            "loss: 0.998459 [  160/ 3200]\n",
            "loss: 1.224123 [  320/ 3200]\n",
            "loss: 1.063311 [  480/ 3200]\n",
            "loss: 1.215410 [  640/ 3200]\n",
            "loss: 1.227564 [  800/ 3200]\n",
            "loss: 1.220384 [  960/ 3200]\n",
            "loss: 1.035848 [ 1120/ 3200]\n",
            "loss: 1.088239 [ 1280/ 3200]\n",
            "loss: 1.149505 [ 1440/ 3200]\n",
            "loss: 1.246524 [ 1600/ 3200]\n",
            "loss: 1.114412 [ 1760/ 3200]\n",
            "loss: 1.154460 [ 1920/ 3200]\n",
            "loss: 1.030303 [ 2080/ 3200]\n",
            "loss: 0.946930 [ 2240/ 3200]\n",
            "loss: 1.072522 [ 2400/ 3200]\n",
            "loss: 1.113806 [ 2560/ 3200]\n",
            "loss: 0.917910 [ 2720/ 3200]\n",
            "loss: 1.060397 [ 2880/ 3200]\n",
            "loss: 1.031495 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "loss: 1.055386 [    0/ 3200]\n",
            "loss: 0.959540 [  160/ 3200]\n",
            "loss: 1.264903 [  320/ 3200]\n",
            "loss: 1.099887 [  480/ 3200]\n",
            "loss: 1.225268 [  640/ 3200]\n",
            "loss: 1.074251 [  800/ 3200]\n",
            "loss: 1.016780 [  960/ 3200]\n",
            "loss: 1.035874 [ 1120/ 3200]\n",
            "loss: 1.062529 [ 1280/ 3200]\n",
            "loss: 1.009656 [ 1440/ 3200]\n",
            "loss: 1.110499 [ 1600/ 3200]\n",
            "loss: 1.403665 [ 1760/ 3200]\n",
            "loss: 1.012418 [ 1920/ 3200]\n",
            "loss: 0.937646 [ 2080/ 3200]\n",
            "loss: 1.199222 [ 2240/ 3200]\n",
            "loss: 1.146895 [ 2400/ 3200]\n",
            "loss: 1.146022 [ 2560/ 3200]\n",
            "loss: 1.014308 [ 2720/ 3200]\n",
            "loss: 0.904941 [ 2880/ 3200]\n",
            "loss: 1.183625 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "loss: 1.321653 [    0/ 3200]\n",
            "loss: 1.158391 [  160/ 3200]\n",
            "loss: 0.994692 [  320/ 3200]\n",
            "loss: 1.082732 [  480/ 3200]\n",
            "loss: 0.934081 [  640/ 3200]\n",
            "loss: 1.189336 [  800/ 3200]\n",
            "loss: 1.340014 [  960/ 3200]\n",
            "loss: 1.160725 [ 1120/ 3200]\n",
            "loss: 1.119184 [ 1280/ 3200]\n",
            "loss: 1.082544 [ 1440/ 3200]\n",
            "loss: 1.148710 [ 1600/ 3200]\n",
            "loss: 1.233390 [ 1760/ 3200]\n",
            "loss: 1.272207 [ 1920/ 3200]\n",
            "loss: 1.134461 [ 2080/ 3200]\n",
            "loss: 0.894388 [ 2240/ 3200]\n",
            "loss: 1.214436 [ 2400/ 3200]\n",
            "loss: 1.384627 [ 2560/ 3200]\n",
            "loss: 1.084184 [ 2720/ 3200]\n",
            "loss: 1.012879 [ 2880/ 3200]\n",
            "loss: 1.009854 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "loss: 1.015546 [    0/ 3200]\n",
            "loss: 1.182828 [  160/ 3200]\n",
            "loss: 0.922761 [  320/ 3200]\n",
            "loss: 0.875218 [  480/ 3200]\n",
            "loss: 0.951422 [  640/ 3200]\n",
            "loss: 1.292904 [  800/ 3200]\n",
            "loss: 0.949545 [  960/ 3200]\n",
            "loss: 1.048522 [ 1120/ 3200]\n",
            "loss: 0.828331 [ 1280/ 3200]\n",
            "loss: 1.258538 [ 1440/ 3200]\n",
            "loss: 1.043025 [ 1600/ 3200]\n",
            "loss: 1.172741 [ 1760/ 3200]\n",
            "loss: 1.042702 [ 1920/ 3200]\n",
            "loss: 1.014518 [ 2080/ 3200]\n",
            "loss: 1.088859 [ 2240/ 3200]\n",
            "loss: 0.977008 [ 2400/ 3200]\n",
            "loss: 0.990503 [ 2560/ 3200]\n",
            "loss: 1.003165 [ 2720/ 3200]\n",
            "loss: 1.069087 [ 2880/ 3200]\n",
            "loss: 0.957225 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "loss: 0.855871 [    0/ 3200]\n",
            "loss: 1.050957 [  160/ 3200]\n",
            "loss: 1.073023 [  320/ 3200]\n",
            "loss: 1.218441 [  480/ 3200]\n",
            "loss: 1.319024 [  640/ 3200]\n",
            "loss: 1.014182 [  800/ 3200]\n",
            "loss: 1.117048 [  960/ 3200]\n",
            "loss: 1.066291 [ 1120/ 3200]\n",
            "loss: 1.129259 [ 1280/ 3200]\n",
            "loss: 1.050310 [ 1440/ 3200]\n",
            "loss: 1.049269 [ 1600/ 3200]\n",
            "loss: 1.052637 [ 1760/ 3200]\n",
            "loss: 1.229213 [ 1920/ 3200]\n",
            "loss: 1.016110 [ 2080/ 3200]\n",
            "loss: 0.909514 [ 2240/ 3200]\n",
            "loss: 0.931300 [ 2400/ 3200]\n",
            "loss: 1.109378 [ 2560/ 3200]\n",
            "loss: 0.964110 [ 2720/ 3200]\n",
            "loss: 1.151076 [ 2880/ 3200]\n",
            "loss: 0.912102 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "loss: 1.041265 [    0/ 3200]\n",
            "loss: 0.964102 [  160/ 3200]\n",
            "loss: 0.750609 [  320/ 3200]\n",
            "loss: 0.952896 [  480/ 3200]\n",
            "loss: 1.037879 [  640/ 3200]\n",
            "loss: 1.075191 [  800/ 3200]\n",
            "loss: 1.082073 [  960/ 3200]\n",
            "loss: 1.311373 [ 1120/ 3200]\n",
            "loss: 1.383523 [ 1280/ 3200]\n",
            "loss: 1.193281 [ 1440/ 3200]\n",
            "loss: 1.107089 [ 1600/ 3200]\n",
            "loss: 0.864593 [ 1760/ 3200]\n",
            "loss: 0.977088 [ 1920/ 3200]\n",
            "loss: 0.950636 [ 2080/ 3200]\n",
            "loss: 1.153255 [ 2240/ 3200]\n",
            "loss: 1.169942 [ 2400/ 3200]\n",
            "loss: 1.052201 [ 2560/ 3200]\n",
            "loss: 0.990433 [ 2720/ 3200]\n",
            "loss: 0.836834 [ 2880/ 3200]\n",
            "loss: 1.070024 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "loss: 0.930815 [    0/ 3200]\n",
            "loss: 0.907686 [  160/ 3200]\n",
            "loss: 1.231607 [  320/ 3200]\n",
            "loss: 0.893421 [  480/ 3200]\n",
            "loss: 0.802914 [  640/ 3200]\n",
            "loss: 0.836620 [  800/ 3200]\n",
            "loss: 0.919520 [  960/ 3200]\n",
            "loss: 1.143122 [ 1120/ 3200]\n",
            "loss: 1.023632 [ 1280/ 3200]\n",
            "loss: 1.064811 [ 1440/ 3200]\n",
            "loss: 1.221920 [ 1600/ 3200]\n",
            "loss: 0.861381 [ 1760/ 3200]\n",
            "loss: 0.861840 [ 1920/ 3200]\n",
            "loss: 0.870001 [ 2080/ 3200]\n",
            "loss: 0.891975 [ 2240/ 3200]\n",
            "loss: 0.794378 [ 2400/ 3200]\n",
            "loss: 1.118615 [ 2560/ 3200]\n",
            "loss: 1.194676 [ 2720/ 3200]\n",
            "loss: 1.140043 [ 2880/ 3200]\n",
            "loss: 0.972296 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "loss: 1.065740 [    0/ 3200]\n",
            "loss: 0.923438 [  160/ 3200]\n",
            "loss: 0.970356 [  320/ 3200]\n",
            "loss: 0.981583 [  480/ 3200]\n",
            "loss: 1.031050 [  640/ 3200]\n",
            "loss: 0.833037 [  800/ 3200]\n",
            "loss: 1.211638 [  960/ 3200]\n",
            "loss: 1.209487 [ 1120/ 3200]\n",
            "loss: 0.825784 [ 1280/ 3200]\n",
            "loss: 0.962605 [ 1440/ 3200]\n",
            "loss: 1.053887 [ 1600/ 3200]\n",
            "loss: 0.750497 [ 1760/ 3200]\n",
            "loss: 0.798552 [ 1920/ 3200]\n",
            "loss: 0.873803 [ 2080/ 3200]\n",
            "loss: 0.772826 [ 2240/ 3200]\n",
            "loss: 1.086897 [ 2400/ 3200]\n",
            "loss: 1.157946 [ 2560/ 3200]\n",
            "loss: 0.867422 [ 2720/ 3200]\n",
            "loss: 1.177628 [ 2880/ 3200]\n",
            "loss: 0.784553 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "loss: 0.897126 [    0/ 3200]\n",
            "loss: 1.006549 [  160/ 3200]\n",
            "loss: 0.647494 [  320/ 3200]\n",
            "loss: 1.205770 [  480/ 3200]\n",
            "loss: 1.298021 [  640/ 3200]\n",
            "loss: 0.883389 [  800/ 3200]\n",
            "loss: 1.009843 [  960/ 3200]\n",
            "loss: 0.979272 [ 1120/ 3200]\n",
            "loss: 0.993875 [ 1280/ 3200]\n",
            "loss: 0.963256 [ 1440/ 3200]\n",
            "loss: 0.975207 [ 1600/ 3200]\n",
            "loss: 0.803207 [ 1760/ 3200]\n",
            "loss: 1.025876 [ 1920/ 3200]\n",
            "loss: 1.041965 [ 2080/ 3200]\n",
            "loss: 0.994506 [ 2240/ 3200]\n",
            "loss: 0.740985 [ 2400/ 3200]\n",
            "loss: 0.862764 [ 2560/ 3200]\n",
            "loss: 1.034496 [ 2720/ 3200]\n",
            "loss: 0.769695 [ 2880/ 3200]\n",
            "loss: 0.932073 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "loss: 0.883127 [    0/ 3200]\n",
            "loss: 0.947781 [  160/ 3200]\n",
            "loss: 1.089903 [  320/ 3200]\n",
            "loss: 0.861500 [  480/ 3200]\n",
            "loss: 0.821685 [  640/ 3200]\n",
            "loss: 1.018452 [  800/ 3200]\n",
            "loss: 1.071610 [  960/ 3200]\n",
            "loss: 1.135387 [ 1120/ 3200]\n",
            "loss: 0.747979 [ 1280/ 3200]\n",
            "loss: 0.998504 [ 1440/ 3200]\n",
            "loss: 0.672616 [ 1600/ 3200]\n",
            "loss: 1.003440 [ 1760/ 3200]\n",
            "loss: 0.927913 [ 1920/ 3200]\n",
            "loss: 0.728014 [ 2080/ 3200]\n",
            "loss: 0.992594 [ 2240/ 3200]\n",
            "loss: 1.295884 [ 2400/ 3200]\n",
            "loss: 0.911513 [ 2560/ 3200]\n",
            "loss: 0.893202 [ 2720/ 3200]\n",
            "loss: 0.940689 [ 2880/ 3200]\n",
            "loss: 0.955856 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "loss: 0.947532 [    0/ 3200]\n",
            "loss: 1.052319 [  160/ 3200]\n",
            "loss: 1.019998 [  320/ 3200]\n",
            "loss: 0.962983 [  480/ 3200]\n",
            "loss: 0.931635 [  640/ 3200]\n",
            "loss: 0.843111 [  800/ 3200]\n",
            "loss: 0.966254 [  960/ 3200]\n",
            "loss: 1.311666 [ 1120/ 3200]\n",
            "loss: 0.767305 [ 1280/ 3200]\n",
            "loss: 1.017133 [ 1440/ 3200]\n",
            "loss: 1.061840 [ 1600/ 3200]\n",
            "loss: 1.013495 [ 1760/ 3200]\n",
            "loss: 0.947353 [ 1920/ 3200]\n",
            "loss: 1.123994 [ 2080/ 3200]\n",
            "loss: 1.071523 [ 2240/ 3200]\n",
            "loss: 0.976148 [ 2400/ 3200]\n",
            "loss: 1.011461 [ 2560/ 3200]\n",
            "loss: 1.237067 [ 2720/ 3200]\n",
            "loss: 0.927487 [ 2880/ 3200]\n",
            "loss: 1.147391 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "loss: 0.939842 [    0/ 3200]\n",
            "loss: 1.078291 [  160/ 3200]\n",
            "loss: 1.046351 [  320/ 3200]\n",
            "loss: 1.026373 [  480/ 3200]\n",
            "loss: 1.184297 [  640/ 3200]\n",
            "loss: 0.640183 [  800/ 3200]\n",
            "loss: 0.789867 [  960/ 3200]\n",
            "loss: 0.816416 [ 1120/ 3200]\n",
            "loss: 0.786427 [ 1280/ 3200]\n",
            "loss: 0.743440 [ 1440/ 3200]\n",
            "loss: 1.141036 [ 1600/ 3200]\n",
            "loss: 1.189206 [ 1760/ 3200]\n",
            "loss: 0.926726 [ 1920/ 3200]\n",
            "loss: 0.778878 [ 2080/ 3200]\n",
            "loss: 0.785029 [ 2240/ 3200]\n",
            "loss: 1.104474 [ 2400/ 3200]\n",
            "loss: 1.146519 [ 2560/ 3200]\n",
            "loss: 1.091490 [ 2720/ 3200]\n",
            "loss: 1.042922 [ 2880/ 3200]\n",
            "loss: 0.801419 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "loss: 0.901183 [    0/ 3200]\n",
            "loss: 1.396601 [  160/ 3200]\n",
            "loss: 0.740880 [  320/ 3200]\n",
            "loss: 0.905589 [  480/ 3200]\n",
            "loss: 1.084461 [  640/ 3200]\n",
            "loss: 0.887103 [  800/ 3200]\n",
            "loss: 1.130810 [  960/ 3200]\n",
            "loss: 1.076929 [ 1120/ 3200]\n",
            "loss: 1.100292 [ 1280/ 3200]\n",
            "loss: 0.766128 [ 1440/ 3200]\n",
            "loss: 0.712507 [ 1600/ 3200]\n",
            "loss: 1.009201 [ 1760/ 3200]\n",
            "loss: 0.963520 [ 1920/ 3200]\n",
            "loss: 1.142996 [ 2080/ 3200]\n",
            "loss: 0.526486 [ 2240/ 3200]\n",
            "loss: 1.026027 [ 2400/ 3200]\n",
            "loss: 1.131421 [ 2560/ 3200]\n",
            "loss: 1.134517 [ 2720/ 3200]\n",
            "loss: 1.052549 [ 2880/ 3200]\n",
            "loss: 0.886843 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "loss: 1.011501 [    0/ 3200]\n",
            "loss: 0.885771 [  160/ 3200]\n",
            "loss: 0.706865 [  320/ 3200]\n",
            "loss: 0.841753 [  480/ 3200]\n",
            "loss: 1.086713 [  640/ 3200]\n",
            "loss: 0.765062 [  800/ 3200]\n",
            "loss: 1.108318 [  960/ 3200]\n",
            "loss: 1.240659 [ 1120/ 3200]\n",
            "loss: 0.978360 [ 1280/ 3200]\n",
            "loss: 0.982435 [ 1440/ 3200]\n",
            "loss: 0.858943 [ 1600/ 3200]\n",
            "loss: 0.818848 [ 1760/ 3200]\n",
            "loss: 0.701559 [ 1920/ 3200]\n",
            "loss: 0.828051 [ 2080/ 3200]\n",
            "loss: 0.772869 [ 2240/ 3200]\n",
            "loss: 1.017854 [ 2400/ 3200]\n",
            "loss: 0.673690 [ 2560/ 3200]\n",
            "loss: 1.319282 [ 2720/ 3200]\n",
            "loss: 1.187923 [ 2880/ 3200]\n",
            "loss: 0.899648 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "loss: 0.889418 [    0/ 3200]\n",
            "loss: 0.854074 [  160/ 3200]\n",
            "loss: 0.852153 [  320/ 3200]\n",
            "loss: 1.127612 [  480/ 3200]\n",
            "loss: 0.784968 [  640/ 3200]\n",
            "loss: 0.858557 [  800/ 3200]\n",
            "loss: 0.901781 [  960/ 3200]\n",
            "loss: 0.791813 [ 1120/ 3200]\n",
            "loss: 1.066227 [ 1280/ 3200]\n",
            "loss: 0.990728 [ 1440/ 3200]\n",
            "loss: 0.713311 [ 1600/ 3200]\n",
            "loss: 0.994952 [ 1760/ 3200]\n",
            "loss: 0.756165 [ 1920/ 3200]\n",
            "loss: 1.069488 [ 2080/ 3200]\n",
            "loss: 0.963664 [ 2240/ 3200]\n",
            "loss: 1.095652 [ 2400/ 3200]\n",
            "loss: 0.958729 [ 2560/ 3200]\n",
            "loss: 0.678300 [ 2720/ 3200]\n",
            "loss: 1.427319 [ 2880/ 3200]\n",
            "loss: 0.839393 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "loss: 0.986859 [    0/ 3200]\n",
            "loss: 0.958935 [  160/ 3200]\n",
            "loss: 0.871468 [  320/ 3200]\n",
            "loss: 1.081399 [  480/ 3200]\n",
            "loss: 0.908602 [  640/ 3200]\n",
            "loss: 0.807612 [  800/ 3200]\n",
            "loss: 0.896356 [  960/ 3200]\n",
            "loss: 1.048855 [ 1120/ 3200]\n",
            "loss: 0.932060 [ 1280/ 3200]\n",
            "loss: 0.771751 [ 1440/ 3200]\n",
            "loss: 0.923455 [ 1600/ 3200]\n",
            "loss: 1.742370 [ 1760/ 3200]\n",
            "loss: 0.919572 [ 1920/ 3200]\n",
            "loss: 0.892002 [ 2080/ 3200]\n",
            "loss: 0.922571 [ 2240/ 3200]\n",
            "loss: 1.087784 [ 2400/ 3200]\n",
            "loss: 0.807071 [ 2560/ 3200]\n",
            "loss: 1.082824 [ 2720/ 3200]\n",
            "loss: 1.252379 [ 2880/ 3200]\n",
            "loss: 0.854798 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "loss: 1.163446 [    0/ 3200]\n",
            "loss: 0.750516 [  160/ 3200]\n",
            "loss: 1.051979 [  320/ 3200]\n",
            "loss: 1.025299 [  480/ 3200]\n",
            "loss: 1.024814 [  640/ 3200]\n",
            "loss: 0.929975 [  800/ 3200]\n",
            "loss: 0.931196 [  960/ 3200]\n",
            "loss: 1.009063 [ 1120/ 3200]\n",
            "loss: 1.121069 [ 1280/ 3200]\n",
            "loss: 0.971997 [ 1440/ 3200]\n",
            "loss: 0.861070 [ 1600/ 3200]\n",
            "loss: 0.842267 [ 1760/ 3200]\n",
            "loss: 0.744011 [ 1920/ 3200]\n",
            "loss: 0.914885 [ 2080/ 3200]\n",
            "loss: 0.717106 [ 2240/ 3200]\n",
            "loss: 0.736682 [ 2400/ 3200]\n",
            "loss: 0.734700 [ 2560/ 3200]\n",
            "loss: 0.864730 [ 2720/ 3200]\n",
            "loss: 1.338963 [ 2880/ 3200]\n",
            "loss: 0.851290 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "loss: 0.800456 [    0/ 3200]\n",
            "loss: 1.111016 [  160/ 3200]\n",
            "loss: 0.650821 [  320/ 3200]\n",
            "loss: 0.880837 [  480/ 3200]\n",
            "loss: 0.838322 [  640/ 3200]\n",
            "loss: 0.784644 [  800/ 3200]\n",
            "loss: 0.998210 [  960/ 3200]\n",
            "loss: 1.370686 [ 1120/ 3200]\n",
            "loss: 1.027762 [ 1280/ 3200]\n",
            "loss: 1.069127 [ 1440/ 3200]\n",
            "loss: 0.925206 [ 1600/ 3200]\n",
            "loss: 0.543989 [ 1760/ 3200]\n",
            "loss: 0.808697 [ 1920/ 3200]\n",
            "loss: 0.831143 [ 2080/ 3200]\n",
            "loss: 0.996644 [ 2240/ 3200]\n",
            "loss: 1.017360 [ 2400/ 3200]\n",
            "loss: 0.922979 [ 2560/ 3200]\n",
            "loss: 0.962208 [ 2720/ 3200]\n",
            "loss: 0.883206 [ 2880/ 3200]\n",
            "loss: 0.708052 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "loss: 1.132378 [    0/ 3200]\n",
            "loss: 0.981881 [  160/ 3200]\n",
            "loss: 1.149561 [  320/ 3200]\n",
            "loss: 1.000508 [  480/ 3200]\n",
            "loss: 1.303377 [  640/ 3200]\n",
            "loss: 0.799185 [  800/ 3200]\n",
            "loss: 0.796819 [  960/ 3200]\n",
            "loss: 0.985242 [ 1120/ 3200]\n",
            "loss: 1.126511 [ 1280/ 3200]\n",
            "loss: 0.662830 [ 1440/ 3200]\n",
            "loss: 0.869929 [ 1600/ 3200]\n",
            "loss: 1.093800 [ 1760/ 3200]\n",
            "loss: 0.905560 [ 1920/ 3200]\n",
            "loss: 1.120020 [ 2080/ 3200]\n",
            "loss: 0.817466 [ 2240/ 3200]\n",
            "loss: 0.969231 [ 2400/ 3200]\n",
            "loss: 0.847701 [ 2560/ 3200]\n",
            "loss: 0.760697 [ 2720/ 3200]\n",
            "loss: 1.236647 [ 2880/ 3200]\n",
            "loss: 0.722270 [ 3040/ 3200]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "start_test_cpu_time = time.time()\n",
        "loss,f1,accuracy,cm = testFNN(test_dataloader,lossfun,fnn_model_cpu)\n",
        "test_cpu_time = time.time() - start_test_cpu_time\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oOV_Uj6imfaP",
        "outputId": "fd06a4f3-ae40-48cc-ce9f-0c5c45078e4b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.062031\n",
            "Accuracy: 60.3%\n",
            "F1 macro averaged: 0.604463\n",
            "Confusion matrix: [[187  15  14  81]\n",
            " [  7 286  11  52]\n",
            " [ 14  59 235  91]\n",
            " [  9 115  78 122]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 6: Network training with GPU***"
      ],
      "metadata": {
        "id": "KNRm6nzzdgor"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainFNN_GPU(num_epochs,optimizer,dataloader,lossfun,fnn):\n",
        "  size = len(dataloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "    for i, (X, labels) in enumerate(dataloader):\n",
        "        X = X.type(torch.FloatTensor)\n",
        "\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        output = fnn(X)\n",
        "        loss = lossfun(output, labels)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          loss,current = loss.item(), i*len(X)\n",
        "          print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(\"\\n\")\n",
        "  return fnn"
      ],
      "metadata": {
        "id": "_Xs14K5Y7njc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def testFNN_GPU(dataloader,lossfun,fnn):\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  # Iterate through test dataset\n",
        "  with torch.no_grad():\n",
        "    for X, labels in dataloader:\n",
        "      X = X.type(torch.float)\n",
        "\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      output = fnn(X)\n",
        "      test_loss += lossfun(output, labels).item()\n",
        "      correct += (output.argmax(1)==labels).type(torch.float).sum().item()\n",
        "      y_true.append(labels)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_true = torch.cat(y_true)\n",
        "  y_pred = torch.cat(y_pred)\n",
        "\n",
        "  correct /= size\n",
        "  accuracy = 100*correct\n",
        "  test_loss /= size\n",
        "\n",
        "  y_true = y_true.cpu()\n",
        "  y_pred = y_pred.cpu()\n",
        "\n",
        "  f1 = f1_score(y_true,y_pred,average='macro')\n",
        "  cm = confusion_matrix(y_true,y_pred)\n",
        "  return test_loss,f1,accuracy,cm"
      ],
      "metadata": {
        "id": "5s34Z-247-Dv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cuda'\n",
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "\n",
        "#load model to gpu\n",
        "fnn_model_gpu = FNN().to(device)\n",
        "\n",
        "optimizer = torch.optim.SGD(fnn_model_gpu.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "start_train_gpu_time = time.time()\n",
        "fnn_model_gpu = trainFNN_GPU(num_epochs,optimizer,train_dataloader,lossfun,fnn_model_gpu)\n",
        "train_gpu_time = time.time() - start_train_gpu_time"
      ],
      "metadata": {
        "id": "DVXG3T3-dooN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e77e4807-a25e-41bb-dbc0-17a7a95171a9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "loss: 2.193876 [    0/ 3200]\n",
            "loss: 1.464325 [  160/ 3200]\n",
            "loss: 1.402948 [  320/ 3200]\n",
            "loss: 1.469823 [  480/ 3200]\n",
            "loss: 1.381014 [  640/ 3200]\n",
            "loss: 1.412704 [  800/ 3200]\n",
            "loss: 1.405490 [  960/ 3200]\n",
            "loss: 1.403006 [ 1120/ 3200]\n",
            "loss: 1.435978 [ 1280/ 3200]\n",
            "loss: 1.347246 [ 1440/ 3200]\n",
            "loss: 1.510694 [ 1600/ 3200]\n",
            "loss: 1.429876 [ 1760/ 3200]\n",
            "loss: 1.397493 [ 1920/ 3200]\n",
            "loss: 1.457436 [ 2080/ 3200]\n",
            "loss: 1.350357 [ 2240/ 3200]\n",
            "loss: 1.390645 [ 2400/ 3200]\n",
            "loss: 1.405657 [ 2560/ 3200]\n",
            "loss: 1.319173 [ 2720/ 3200]\n",
            "loss: 1.405421 [ 2880/ 3200]\n",
            "loss: 1.437520 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "loss: 1.393511 [    0/ 3200]\n",
            "loss: 1.399652 [  160/ 3200]\n",
            "loss: 1.422883 [  320/ 3200]\n",
            "loss: 1.393435 [  480/ 3200]\n",
            "loss: 1.366210 [  640/ 3200]\n",
            "loss: 1.519250 [  800/ 3200]\n",
            "loss: 1.435773 [  960/ 3200]\n",
            "loss: 1.347331 [ 1120/ 3200]\n",
            "loss: 1.397486 [ 1280/ 3200]\n",
            "loss: 1.359713 [ 1440/ 3200]\n",
            "loss: 1.389682 [ 1600/ 3200]\n",
            "loss: 1.381011 [ 1760/ 3200]\n",
            "loss: 1.410150 [ 1920/ 3200]\n",
            "loss: 1.364506 [ 2080/ 3200]\n",
            "loss: 1.348889 [ 2240/ 3200]\n",
            "loss: 1.395611 [ 2400/ 3200]\n",
            "loss: 1.398722 [ 2560/ 3200]\n",
            "loss: 1.374063 [ 2720/ 3200]\n",
            "loss: 1.382790 [ 2880/ 3200]\n",
            "loss: 1.355568 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "loss: 1.361202 [    0/ 3200]\n",
            "loss: 1.382023 [  160/ 3200]\n",
            "loss: 1.391558 [  320/ 3200]\n",
            "loss: 1.371008 [  480/ 3200]\n",
            "loss: 1.394871 [  640/ 3200]\n",
            "loss: 1.370785 [  800/ 3200]\n",
            "loss: 1.465203 [  960/ 3200]\n",
            "loss: 1.390887 [ 1120/ 3200]\n",
            "loss: 1.397082 [ 1280/ 3200]\n",
            "loss: 1.417096 [ 1440/ 3200]\n",
            "loss: 1.355078 [ 1600/ 3200]\n",
            "loss: 1.341846 [ 1760/ 3200]\n",
            "loss: 1.358526 [ 1920/ 3200]\n",
            "loss: 1.351635 [ 2080/ 3200]\n",
            "loss: 1.373779 [ 2240/ 3200]\n",
            "loss: 1.457752 [ 2400/ 3200]\n",
            "loss: 1.333420 [ 2560/ 3200]\n",
            "loss: 1.488084 [ 2720/ 3200]\n",
            "loss: 1.402425 [ 2880/ 3200]\n",
            "loss: 1.379166 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "loss: 1.461156 [    0/ 3200]\n",
            "loss: 1.378651 [  160/ 3200]\n",
            "loss: 1.424006 [  320/ 3200]\n",
            "loss: 1.340054 [  480/ 3200]\n",
            "loss: 1.318286 [  640/ 3200]\n",
            "loss: 1.320514 [  800/ 3200]\n",
            "loss: 1.418553 [  960/ 3200]\n",
            "loss: 1.458280 [ 1120/ 3200]\n",
            "loss: 1.381250 [ 1280/ 3200]\n",
            "loss: 1.334909 [ 1440/ 3200]\n",
            "loss: 1.346292 [ 1600/ 3200]\n",
            "loss: 1.340126 [ 1760/ 3200]\n",
            "loss: 1.361692 [ 1920/ 3200]\n",
            "loss: 1.279127 [ 2080/ 3200]\n",
            "loss: 1.327781 [ 2240/ 3200]\n",
            "loss: 1.361798 [ 2400/ 3200]\n",
            "loss: 1.391262 [ 2560/ 3200]\n",
            "loss: 1.447373 [ 2720/ 3200]\n",
            "loss: 1.417208 [ 2880/ 3200]\n",
            "loss: 1.385382 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "loss: 1.363844 [    0/ 3200]\n",
            "loss: 1.339453 [  160/ 3200]\n",
            "loss: 1.337031 [  320/ 3200]\n",
            "loss: 1.299644 [  480/ 3200]\n",
            "loss: 1.347623 [  640/ 3200]\n",
            "loss: 1.270028 [  800/ 3200]\n",
            "loss: 1.373877 [  960/ 3200]\n",
            "loss: 1.303418 [ 1120/ 3200]\n",
            "loss: 1.350237 [ 1280/ 3200]\n",
            "loss: 1.402042 [ 1440/ 3200]\n",
            "loss: 1.344576 [ 1600/ 3200]\n",
            "loss: 1.334244 [ 1760/ 3200]\n",
            "loss: 1.409773 [ 1920/ 3200]\n",
            "loss: 1.412238 [ 2080/ 3200]\n",
            "loss: 1.279599 [ 2240/ 3200]\n",
            "loss: 1.360060 [ 2400/ 3200]\n",
            "loss: 1.331271 [ 2560/ 3200]\n",
            "loss: 1.305464 [ 2720/ 3200]\n",
            "loss: 1.310884 [ 2880/ 3200]\n",
            "loss: 1.313815 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "loss: 1.398034 [    0/ 3200]\n",
            "loss: 1.339177 [  160/ 3200]\n",
            "loss: 1.339751 [  320/ 3200]\n",
            "loss: 1.469905 [  480/ 3200]\n",
            "loss: 1.319635 [  640/ 3200]\n",
            "loss: 1.315262 [  800/ 3200]\n",
            "loss: 1.341866 [  960/ 3200]\n",
            "loss: 1.318436 [ 1120/ 3200]\n",
            "loss: 1.432550 [ 1280/ 3200]\n",
            "loss: 1.259942 [ 1440/ 3200]\n",
            "loss: 1.368856 [ 1600/ 3200]\n",
            "loss: 1.356006 [ 1760/ 3200]\n",
            "loss: 1.292039 [ 1920/ 3200]\n",
            "loss: 1.372734 [ 2080/ 3200]\n",
            "loss: 1.329621 [ 2240/ 3200]\n",
            "loss: 1.300151 [ 2400/ 3200]\n",
            "loss: 1.288010 [ 2560/ 3200]\n",
            "loss: 1.317156 [ 2720/ 3200]\n",
            "loss: 1.352264 [ 2880/ 3200]\n",
            "loss: 1.297265 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "loss: 1.337607 [    0/ 3200]\n",
            "loss: 1.356081 [  160/ 3200]\n",
            "loss: 1.334470 [  320/ 3200]\n",
            "loss: 1.293227 [  480/ 3200]\n",
            "loss: 1.325188 [  640/ 3200]\n",
            "loss: 1.271614 [  800/ 3200]\n",
            "loss: 1.241846 [  960/ 3200]\n",
            "loss: 1.347631 [ 1120/ 3200]\n",
            "loss: 1.289366 [ 1280/ 3200]\n",
            "loss: 1.408608 [ 1440/ 3200]\n",
            "loss: 1.280454 [ 1600/ 3200]\n",
            "loss: 1.284807 [ 1760/ 3200]\n",
            "loss: 1.251791 [ 1920/ 3200]\n",
            "loss: 1.330849 [ 2080/ 3200]\n",
            "loss: 1.353943 [ 2240/ 3200]\n",
            "loss: 1.265531 [ 2400/ 3200]\n",
            "loss: 1.313148 [ 2560/ 3200]\n",
            "loss: 1.300685 [ 2720/ 3200]\n",
            "loss: 1.281589 [ 2880/ 3200]\n",
            "loss: 1.370258 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "loss: 1.238851 [    0/ 3200]\n",
            "loss: 1.249830 [  160/ 3200]\n",
            "loss: 1.246134 [  320/ 3200]\n",
            "loss: 1.358941 [  480/ 3200]\n",
            "loss: 1.253986 [  640/ 3200]\n",
            "loss: 1.210411 [  800/ 3200]\n",
            "loss: 1.244704 [  960/ 3200]\n",
            "loss: 1.246064 [ 1120/ 3200]\n",
            "loss: 1.235538 [ 1280/ 3200]\n",
            "loss: 1.249101 [ 1440/ 3200]\n",
            "loss: 1.186136 [ 1600/ 3200]\n",
            "loss: 1.222596 [ 1760/ 3200]\n",
            "loss: 1.133659 [ 1920/ 3200]\n",
            "loss: 1.273333 [ 2080/ 3200]\n",
            "loss: 1.363782 [ 2240/ 3200]\n",
            "loss: 1.242222 [ 2400/ 3200]\n",
            "loss: 1.233494 [ 2560/ 3200]\n",
            "loss: 1.214193 [ 2720/ 3200]\n",
            "loss: 1.134737 [ 2880/ 3200]\n",
            "loss: 1.272204 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "loss: 1.221654 [    0/ 3200]\n",
            "loss: 1.298633 [  160/ 3200]\n",
            "loss: 1.253903 [  320/ 3200]\n",
            "loss: 1.223416 [  480/ 3200]\n",
            "loss: 1.238769 [  640/ 3200]\n",
            "loss: 1.248831 [  800/ 3200]\n",
            "loss: 1.206542 [  960/ 3200]\n",
            "loss: 1.133792 [ 1120/ 3200]\n",
            "loss: 1.211427 [ 1280/ 3200]\n",
            "loss: 1.316186 [ 1440/ 3200]\n",
            "loss: 1.166307 [ 1600/ 3200]\n",
            "loss: 1.224318 [ 1760/ 3200]\n",
            "loss: 1.244163 [ 1920/ 3200]\n",
            "loss: 1.144432 [ 2080/ 3200]\n",
            "loss: 1.215927 [ 2240/ 3200]\n",
            "loss: 1.332457 [ 2400/ 3200]\n",
            "loss: 1.276972 [ 2560/ 3200]\n",
            "loss: 1.291592 [ 2720/ 3200]\n",
            "loss: 1.192812 [ 2880/ 3200]\n",
            "loss: 1.269227 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "loss: 1.268029 [    0/ 3200]\n",
            "loss: 1.174195 [  160/ 3200]\n",
            "loss: 1.193423 [  320/ 3200]\n",
            "loss: 1.204652 [  480/ 3200]\n",
            "loss: 1.351615 [  640/ 3200]\n",
            "loss: 1.280030 [  800/ 3200]\n",
            "loss: 1.200274 [  960/ 3200]\n",
            "loss: 1.262564 [ 1120/ 3200]\n",
            "loss: 1.221996 [ 1280/ 3200]\n",
            "loss: 1.251202 [ 1440/ 3200]\n",
            "loss: 1.199334 [ 1600/ 3200]\n",
            "loss: 1.220595 [ 1760/ 3200]\n",
            "loss: 1.199412 [ 1920/ 3200]\n",
            "loss: 1.264890 [ 2080/ 3200]\n",
            "loss: 1.146610 [ 2240/ 3200]\n",
            "loss: 1.120143 [ 2400/ 3200]\n",
            "loss: 1.167637 [ 2560/ 3200]\n",
            "loss: 1.118238 [ 2720/ 3200]\n",
            "loss: 1.094082 [ 2880/ 3200]\n",
            "loss: 1.140630 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "loss: 1.153252 [    0/ 3200]\n",
            "loss: 1.230426 [  160/ 3200]\n",
            "loss: 1.191488 [  320/ 3200]\n",
            "loss: 1.312526 [  480/ 3200]\n",
            "loss: 1.288444 [  640/ 3200]\n",
            "loss: 1.190629 [  800/ 3200]\n",
            "loss: 1.247007 [  960/ 3200]\n",
            "loss: 1.109701 [ 1120/ 3200]\n",
            "loss: 1.274754 [ 1280/ 3200]\n",
            "loss: 1.185724 [ 1440/ 3200]\n",
            "loss: 1.128353 [ 1600/ 3200]\n",
            "loss: 1.108381 [ 1760/ 3200]\n",
            "loss: 1.133880 [ 1920/ 3200]\n",
            "loss: 1.318506 [ 2080/ 3200]\n",
            "loss: 1.219795 [ 2240/ 3200]\n",
            "loss: 1.239362 [ 2400/ 3200]\n",
            "loss: 1.213128 [ 2560/ 3200]\n",
            "loss: 1.310184 [ 2720/ 3200]\n",
            "loss: 1.106453 [ 2880/ 3200]\n",
            "loss: 1.270254 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "loss: 1.074375 [    0/ 3200]\n",
            "loss: 1.232558 [  160/ 3200]\n",
            "loss: 1.294845 [  320/ 3200]\n",
            "loss: 0.949777 [  480/ 3200]\n",
            "loss: 1.160806 [  640/ 3200]\n",
            "loss: 1.214075 [  800/ 3200]\n",
            "loss: 1.125924 [  960/ 3200]\n",
            "loss: 1.237526 [ 1120/ 3200]\n",
            "loss: 1.194908 [ 1280/ 3200]\n",
            "loss: 1.261629 [ 1440/ 3200]\n",
            "loss: 1.206375 [ 1600/ 3200]\n",
            "loss: 0.978699 [ 1760/ 3200]\n",
            "loss: 1.051355 [ 1920/ 3200]\n",
            "loss: 1.058885 [ 2080/ 3200]\n",
            "loss: 1.002708 [ 2240/ 3200]\n",
            "loss: 1.112061 [ 2400/ 3200]\n",
            "loss: 0.993893 [ 2560/ 3200]\n",
            "loss: 1.103268 [ 2720/ 3200]\n",
            "loss: 1.112207 [ 2880/ 3200]\n",
            "loss: 1.110000 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "loss: 1.058929 [    0/ 3200]\n",
            "loss: 1.089493 [  160/ 3200]\n",
            "loss: 1.178324 [  320/ 3200]\n",
            "loss: 1.146771 [  480/ 3200]\n",
            "loss: 1.172567 [  640/ 3200]\n",
            "loss: 1.260162 [  800/ 3200]\n",
            "loss: 1.320766 [  960/ 3200]\n",
            "loss: 1.283822 [ 1120/ 3200]\n",
            "loss: 1.166181 [ 1280/ 3200]\n",
            "loss: 1.115919 [ 1440/ 3200]\n",
            "loss: 1.219962 [ 1600/ 3200]\n",
            "loss: 0.991366 [ 1760/ 3200]\n",
            "loss: 1.121708 [ 1920/ 3200]\n",
            "loss: 1.149385 [ 2080/ 3200]\n",
            "loss: 1.165515 [ 2240/ 3200]\n",
            "loss: 0.941695 [ 2400/ 3200]\n",
            "loss: 1.075642 [ 2560/ 3200]\n",
            "loss: 1.121103 [ 2720/ 3200]\n",
            "loss: 1.143361 [ 2880/ 3200]\n",
            "loss: 1.169857 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "loss: 1.200187 [    0/ 3200]\n",
            "loss: 1.167747 [  160/ 3200]\n",
            "loss: 1.157644 [  320/ 3200]\n",
            "loss: 1.109270 [  480/ 3200]\n",
            "loss: 0.992092 [  640/ 3200]\n",
            "loss: 1.121765 [  800/ 3200]\n",
            "loss: 1.066667 [  960/ 3200]\n",
            "loss: 1.149158 [ 1120/ 3200]\n",
            "loss: 1.113898 [ 1280/ 3200]\n",
            "loss: 0.953128 [ 1440/ 3200]\n",
            "loss: 1.071516 [ 1600/ 3200]\n",
            "loss: 1.114382 [ 1760/ 3200]\n",
            "loss: 0.960775 [ 1920/ 3200]\n",
            "loss: 1.166377 [ 2080/ 3200]\n",
            "loss: 1.158467 [ 2240/ 3200]\n",
            "loss: 1.053374 [ 2400/ 3200]\n",
            "loss: 1.119815 [ 2560/ 3200]\n",
            "loss: 1.070934 [ 2720/ 3200]\n",
            "loss: 1.247691 [ 2880/ 3200]\n",
            "loss: 1.006620 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "loss: 1.129728 [    0/ 3200]\n",
            "loss: 1.191693 [  160/ 3200]\n",
            "loss: 1.026538 [  320/ 3200]\n",
            "loss: 1.275704 [  480/ 3200]\n",
            "loss: 0.904615 [  640/ 3200]\n",
            "loss: 1.207878 [  800/ 3200]\n",
            "loss: 1.023600 [  960/ 3200]\n",
            "loss: 1.104333 [ 1120/ 3200]\n",
            "loss: 0.945865 [ 1280/ 3200]\n",
            "loss: 1.076832 [ 1440/ 3200]\n",
            "loss: 1.004716 [ 1600/ 3200]\n",
            "loss: 1.276737 [ 1760/ 3200]\n",
            "loss: 1.095533 [ 1920/ 3200]\n",
            "loss: 1.123724 [ 2080/ 3200]\n",
            "loss: 1.107061 [ 2240/ 3200]\n",
            "loss: 1.005065 [ 2400/ 3200]\n",
            "loss: 0.979469 [ 2560/ 3200]\n",
            "loss: 1.247960 [ 2720/ 3200]\n",
            "loss: 1.187705 [ 2880/ 3200]\n",
            "loss: 0.927354 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "loss: 1.009046 [    0/ 3200]\n",
            "loss: 1.034320 [  160/ 3200]\n",
            "loss: 0.978088 [  320/ 3200]\n",
            "loss: 0.958381 [  480/ 3200]\n",
            "loss: 1.006048 [  640/ 3200]\n",
            "loss: 1.062842 [  800/ 3200]\n",
            "loss: 1.193921 [  960/ 3200]\n",
            "loss: 0.924838 [ 1120/ 3200]\n",
            "loss: 1.138658 [ 1280/ 3200]\n",
            "loss: 0.929445 [ 1440/ 3200]\n",
            "loss: 1.076033 [ 1600/ 3200]\n",
            "loss: 1.322542 [ 1760/ 3200]\n",
            "loss: 1.239122 [ 1920/ 3200]\n",
            "loss: 1.026393 [ 2080/ 3200]\n",
            "loss: 1.215871 [ 2240/ 3200]\n",
            "loss: 0.949791 [ 2400/ 3200]\n",
            "loss: 0.841875 [ 2560/ 3200]\n",
            "loss: 1.089801 [ 2720/ 3200]\n",
            "loss: 1.051821 [ 2880/ 3200]\n",
            "loss: 0.962997 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "loss: 1.105015 [    0/ 3200]\n",
            "loss: 1.032687 [  160/ 3200]\n",
            "loss: 1.055315 [  320/ 3200]\n",
            "loss: 1.048790 [  480/ 3200]\n",
            "loss: 0.975857 [  640/ 3200]\n",
            "loss: 0.931196 [  800/ 3200]\n",
            "loss: 1.076607 [  960/ 3200]\n",
            "loss: 1.278412 [ 1120/ 3200]\n",
            "loss: 1.025613 [ 1280/ 3200]\n",
            "loss: 0.928686 [ 1440/ 3200]\n",
            "loss: 1.153101 [ 1600/ 3200]\n",
            "loss: 1.149053 [ 1760/ 3200]\n",
            "loss: 0.860981 [ 1920/ 3200]\n",
            "loss: 1.049732 [ 2080/ 3200]\n",
            "loss: 0.921920 [ 2240/ 3200]\n",
            "loss: 0.960274 [ 2400/ 3200]\n",
            "loss: 1.040778 [ 2560/ 3200]\n",
            "loss: 0.909165 [ 2720/ 3200]\n",
            "loss: 1.021196 [ 2880/ 3200]\n",
            "loss: 0.981484 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "loss: 1.248451 [    0/ 3200]\n",
            "loss: 1.047022 [  160/ 3200]\n",
            "loss: 0.966417 [  320/ 3200]\n",
            "loss: 1.141022 [  480/ 3200]\n",
            "loss: 1.212790 [  640/ 3200]\n",
            "loss: 1.454725 [  800/ 3200]\n",
            "loss: 1.012788 [  960/ 3200]\n",
            "loss: 0.998397 [ 1120/ 3200]\n",
            "loss: 1.094145 [ 1280/ 3200]\n",
            "loss: 0.942640 [ 1440/ 3200]\n",
            "loss: 0.938607 [ 1600/ 3200]\n",
            "loss: 1.184969 [ 1760/ 3200]\n",
            "loss: 1.008304 [ 1920/ 3200]\n",
            "loss: 1.094656 [ 2080/ 3200]\n",
            "loss: 0.967439 [ 2240/ 3200]\n",
            "loss: 1.130783 [ 2400/ 3200]\n",
            "loss: 1.005753 [ 2560/ 3200]\n",
            "loss: 0.867647 [ 2720/ 3200]\n",
            "loss: 1.093010 [ 2880/ 3200]\n",
            "loss: 1.155892 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "loss: 0.941022 [    0/ 3200]\n",
            "loss: 0.856380 [  160/ 3200]\n",
            "loss: 0.945395 [  320/ 3200]\n",
            "loss: 0.811521 [  480/ 3200]\n",
            "loss: 1.437075 [  640/ 3200]\n",
            "loss: 0.974651 [  800/ 3200]\n",
            "loss: 0.977943 [  960/ 3200]\n",
            "loss: 0.815036 [ 1120/ 3200]\n",
            "loss: 1.018751 [ 1280/ 3200]\n",
            "loss: 0.872895 [ 1440/ 3200]\n",
            "loss: 0.812495 [ 1600/ 3200]\n",
            "loss: 1.151387 [ 1760/ 3200]\n",
            "loss: 0.896088 [ 1920/ 3200]\n",
            "loss: 1.036633 [ 2080/ 3200]\n",
            "loss: 1.086110 [ 2240/ 3200]\n",
            "loss: 1.054719 [ 2400/ 3200]\n",
            "loss: 0.872921 [ 2560/ 3200]\n",
            "loss: 0.979560 [ 2720/ 3200]\n",
            "loss: 1.059508 [ 2880/ 3200]\n",
            "loss: 0.857296 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "loss: 0.856129 [    0/ 3200]\n",
            "loss: 1.191433 [  160/ 3200]\n",
            "loss: 1.204974 [  320/ 3200]\n",
            "loss: 0.904956 [  480/ 3200]\n",
            "loss: 1.131153 [  640/ 3200]\n",
            "loss: 1.119262 [  800/ 3200]\n",
            "loss: 1.186054 [  960/ 3200]\n",
            "loss: 1.028663 [ 1120/ 3200]\n",
            "loss: 0.873667 [ 1280/ 3200]\n",
            "loss: 1.064676 [ 1440/ 3200]\n",
            "loss: 0.969043 [ 1600/ 3200]\n",
            "loss: 1.096317 [ 1760/ 3200]\n",
            "loss: 0.898317 [ 1920/ 3200]\n",
            "loss: 1.139471 [ 2080/ 3200]\n",
            "loss: 0.900080 [ 2240/ 3200]\n",
            "loss: 0.922420 [ 2400/ 3200]\n",
            "loss: 1.031703 [ 2560/ 3200]\n",
            "loss: 1.146164 [ 2720/ 3200]\n",
            "loss: 0.985400 [ 2880/ 3200]\n",
            "loss: 0.781154 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "loss: 0.908720 [    0/ 3200]\n",
            "loss: 1.150109 [  160/ 3200]\n",
            "loss: 1.114638 [  320/ 3200]\n",
            "loss: 1.136365 [  480/ 3200]\n",
            "loss: 0.826411 [  640/ 3200]\n",
            "loss: 1.030861 [  800/ 3200]\n",
            "loss: 1.087202 [  960/ 3200]\n",
            "loss: 0.863569 [ 1120/ 3200]\n",
            "loss: 1.011166 [ 1280/ 3200]\n",
            "loss: 0.970869 [ 1440/ 3200]\n",
            "loss: 1.035575 [ 1600/ 3200]\n",
            "loss: 1.006843 [ 1760/ 3200]\n",
            "loss: 1.025340 [ 1920/ 3200]\n",
            "loss: 1.218792 [ 2080/ 3200]\n",
            "loss: 0.858084 [ 2240/ 3200]\n",
            "loss: 1.160773 [ 2400/ 3200]\n",
            "loss: 0.963819 [ 2560/ 3200]\n",
            "loss: 1.251913 [ 2720/ 3200]\n",
            "loss: 1.012707 [ 2880/ 3200]\n",
            "loss: 1.068327 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "loss: 1.146898 [    0/ 3200]\n",
            "loss: 0.919929 [  160/ 3200]\n",
            "loss: 0.945924 [  320/ 3200]\n",
            "loss: 1.201201 [  480/ 3200]\n",
            "loss: 1.090745 [  640/ 3200]\n",
            "loss: 0.759006 [  800/ 3200]\n",
            "loss: 0.619595 [  960/ 3200]\n",
            "loss: 1.263801 [ 1120/ 3200]\n",
            "loss: 1.065220 [ 1280/ 3200]\n",
            "loss: 0.875980 [ 1440/ 3200]\n",
            "loss: 0.953141 [ 1600/ 3200]\n",
            "loss: 1.117811 [ 1760/ 3200]\n",
            "loss: 0.923465 [ 1920/ 3200]\n",
            "loss: 0.886461 [ 2080/ 3200]\n",
            "loss: 1.129076 [ 2240/ 3200]\n",
            "loss: 1.091689 [ 2400/ 3200]\n",
            "loss: 0.823085 [ 2560/ 3200]\n",
            "loss: 0.899271 [ 2720/ 3200]\n",
            "loss: 0.822773 [ 2880/ 3200]\n",
            "loss: 1.080427 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "loss: 1.011504 [    0/ 3200]\n",
            "loss: 1.091865 [  160/ 3200]\n",
            "loss: 0.805740 [  320/ 3200]\n",
            "loss: 1.004591 [  480/ 3200]\n",
            "loss: 0.848690 [  640/ 3200]\n",
            "loss: 1.200647 [  800/ 3200]\n",
            "loss: 0.933292 [  960/ 3200]\n",
            "loss: 1.086700 [ 1120/ 3200]\n",
            "loss: 1.174209 [ 1280/ 3200]\n",
            "loss: 0.891627 [ 1440/ 3200]\n",
            "loss: 0.928329 [ 1600/ 3200]\n",
            "loss: 0.890341 [ 1760/ 3200]\n",
            "loss: 0.980056 [ 1920/ 3200]\n",
            "loss: 0.865484 [ 2080/ 3200]\n",
            "loss: 1.090676 [ 2240/ 3200]\n",
            "loss: 0.932209 [ 2400/ 3200]\n",
            "loss: 0.720282 [ 2560/ 3200]\n",
            "loss: 0.700481 [ 2720/ 3200]\n",
            "loss: 0.886964 [ 2880/ 3200]\n",
            "loss: 0.760317 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "loss: 1.022471 [    0/ 3200]\n",
            "loss: 0.972046 [  160/ 3200]\n",
            "loss: 1.068051 [  320/ 3200]\n",
            "loss: 0.980828 [  480/ 3200]\n",
            "loss: 1.156572 [  640/ 3200]\n",
            "loss: 0.667153 [  800/ 3200]\n",
            "loss: 0.758114 [  960/ 3200]\n",
            "loss: 1.104351 [ 1120/ 3200]\n",
            "loss: 0.964423 [ 1280/ 3200]\n",
            "loss: 1.063185 [ 1440/ 3200]\n",
            "loss: 0.967886 [ 1600/ 3200]\n",
            "loss: 1.058068 [ 1760/ 3200]\n",
            "loss: 0.923479 [ 1920/ 3200]\n",
            "loss: 1.041603 [ 2080/ 3200]\n",
            "loss: 1.065261 [ 2240/ 3200]\n",
            "loss: 0.832428 [ 2400/ 3200]\n",
            "loss: 0.875189 [ 2560/ 3200]\n",
            "loss: 0.990342 [ 2720/ 3200]\n",
            "loss: 0.844222 [ 2880/ 3200]\n",
            "loss: 0.857649 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "loss: 0.904483 [    0/ 3200]\n",
            "loss: 1.013066 [  160/ 3200]\n",
            "loss: 0.843667 [  320/ 3200]\n",
            "loss: 0.896399 [  480/ 3200]\n",
            "loss: 0.927276 [  640/ 3200]\n",
            "loss: 0.950782 [  800/ 3200]\n",
            "loss: 0.766650 [  960/ 3200]\n",
            "loss: 0.940374 [ 1120/ 3200]\n",
            "loss: 0.953213 [ 1280/ 3200]\n",
            "loss: 1.057992 [ 1440/ 3200]\n",
            "loss: 1.091205 [ 1600/ 3200]\n",
            "loss: 0.865028 [ 1760/ 3200]\n",
            "loss: 1.103395 [ 1920/ 3200]\n",
            "loss: 1.112412 [ 2080/ 3200]\n",
            "loss: 0.871397 [ 2240/ 3200]\n",
            "loss: 1.155519 [ 2400/ 3200]\n",
            "loss: 1.112360 [ 2560/ 3200]\n",
            "loss: 0.951854 [ 2720/ 3200]\n",
            "loss: 1.050877 [ 2880/ 3200]\n",
            "loss: 0.937915 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "loss: 1.045565 [    0/ 3200]\n",
            "loss: 1.037834 [  160/ 3200]\n",
            "loss: 0.847512 [  320/ 3200]\n",
            "loss: 0.905708 [  480/ 3200]\n",
            "loss: 0.772382 [  640/ 3200]\n",
            "loss: 1.074286 [  800/ 3200]\n",
            "loss: 1.134497 [  960/ 3200]\n",
            "loss: 0.991134 [ 1120/ 3200]\n",
            "loss: 0.874113 [ 1280/ 3200]\n",
            "loss: 1.191982 [ 1440/ 3200]\n",
            "loss: 0.967177 [ 1600/ 3200]\n",
            "loss: 0.901041 [ 1760/ 3200]\n",
            "loss: 0.686404 [ 1920/ 3200]\n",
            "loss: 1.341300 [ 2080/ 3200]\n",
            "loss: 0.862970 [ 2240/ 3200]\n",
            "loss: 1.022480 [ 2400/ 3200]\n",
            "loss: 0.827148 [ 2560/ 3200]\n",
            "loss: 0.838816 [ 2720/ 3200]\n",
            "loss: 0.569730 [ 2880/ 3200]\n",
            "loss: 0.963401 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "loss: 0.793611 [    0/ 3200]\n",
            "loss: 1.086851 [  160/ 3200]\n",
            "loss: 0.877891 [  320/ 3200]\n",
            "loss: 0.898104 [  480/ 3200]\n",
            "loss: 1.110024 [  640/ 3200]\n",
            "loss: 1.183373 [  800/ 3200]\n",
            "loss: 0.947483 [  960/ 3200]\n",
            "loss: 0.835204 [ 1120/ 3200]\n",
            "loss: 0.765288 [ 1280/ 3200]\n",
            "loss: 1.044002 [ 1440/ 3200]\n",
            "loss: 0.782067 [ 1600/ 3200]\n",
            "loss: 1.220637 [ 1760/ 3200]\n",
            "loss: 1.015340 [ 1920/ 3200]\n",
            "loss: 0.825164 [ 2080/ 3200]\n",
            "loss: 0.951950 [ 2240/ 3200]\n",
            "loss: 1.109861 [ 2400/ 3200]\n",
            "loss: 0.807893 [ 2560/ 3200]\n",
            "loss: 1.287182 [ 2720/ 3200]\n",
            "loss: 1.558876 [ 2880/ 3200]\n",
            "loss: 0.654153 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "loss: 0.791568 [    0/ 3200]\n",
            "loss: 0.773854 [  160/ 3200]\n",
            "loss: 0.935526 [  320/ 3200]\n",
            "loss: 0.687866 [  480/ 3200]\n",
            "loss: 1.081784 [  640/ 3200]\n",
            "loss: 1.009764 [  800/ 3200]\n",
            "loss: 1.241083 [  960/ 3200]\n",
            "loss: 0.823843 [ 1120/ 3200]\n",
            "loss: 0.838535 [ 1280/ 3200]\n",
            "loss: 1.179577 [ 1440/ 3200]\n",
            "loss: 0.688585 [ 1600/ 3200]\n",
            "loss: 0.886657 [ 1760/ 3200]\n",
            "loss: 0.820448 [ 1920/ 3200]\n",
            "loss: 0.757756 [ 2080/ 3200]\n",
            "loss: 1.001682 [ 2240/ 3200]\n",
            "loss: 0.886283 [ 2400/ 3200]\n",
            "loss: 0.714238 [ 2560/ 3200]\n",
            "loss: 0.658417 [ 2720/ 3200]\n",
            "loss: 1.209650 [ 2880/ 3200]\n",
            "loss: 1.150116 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "loss: 1.045335 [    0/ 3200]\n",
            "loss: 0.855698 [  160/ 3200]\n",
            "loss: 0.916244 [  320/ 3200]\n",
            "loss: 0.946729 [  480/ 3200]\n",
            "loss: 1.021337 [  640/ 3200]\n",
            "loss: 0.713595 [  800/ 3200]\n",
            "loss: 0.915089 [  960/ 3200]\n",
            "loss: 0.649253 [ 1120/ 3200]\n",
            "loss: 0.810478 [ 1280/ 3200]\n",
            "loss: 0.741402 [ 1440/ 3200]\n",
            "loss: 0.852222 [ 1600/ 3200]\n",
            "loss: 1.018296 [ 1760/ 3200]\n",
            "loss: 1.334559 [ 1920/ 3200]\n",
            "loss: 0.948793 [ 2080/ 3200]\n",
            "loss: 0.934804 [ 2240/ 3200]\n",
            "loss: 0.797267 [ 2400/ 3200]\n",
            "loss: 0.942826 [ 2560/ 3200]\n",
            "loss: 1.384111 [ 2720/ 3200]\n",
            "loss: 0.769141 [ 2880/ 3200]\n",
            "loss: 1.110904 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "loss: 0.821647 [    0/ 3200]\n",
            "loss: 1.037528 [  160/ 3200]\n",
            "loss: 0.702151 [  320/ 3200]\n",
            "loss: 0.674775 [  480/ 3200]\n",
            "loss: 1.125335 [  640/ 3200]\n",
            "loss: 0.720240 [  800/ 3200]\n",
            "loss: 1.017080 [  960/ 3200]\n",
            "loss: 0.895944 [ 1120/ 3200]\n",
            "loss: 1.093033 [ 1280/ 3200]\n",
            "loss: 1.003481 [ 1440/ 3200]\n",
            "loss: 0.711751 [ 1600/ 3200]\n",
            "loss: 1.124117 [ 1760/ 3200]\n",
            "loss: 1.063595 [ 1920/ 3200]\n",
            "loss: 1.213134 [ 2080/ 3200]\n",
            "loss: 0.867171 [ 2240/ 3200]\n",
            "loss: 1.130399 [ 2400/ 3200]\n",
            "loss: 0.808220 [ 2560/ 3200]\n",
            "loss: 0.923248 [ 2720/ 3200]\n",
            "loss: 1.014609 [ 2880/ 3200]\n",
            "loss: 0.848547 [ 3040/ 3200]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "start_test_gpu_time = time.time()\n",
        "loss,f1,accuracy,cm = testFNN_GPU(test_dataloader,lossfun,fnn_model_gpu)\n",
        "test_gpu_time = time.time() - start_test_gpu_time\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zNyc1C6q4kQC",
        "outputId": "33a89be6-62c2-4d4b-c1cb-090d087b7b79"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.059280\n",
            "Accuracy: 61.0%\n",
            "F1 macro averaged: 0.573426\n",
            "Confusion matrix: [[223  12  47  15]\n",
            " [ 12 246  60  38]\n",
            " [ 23  37 328  11]\n",
            " [ 22  78 181  43]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print difference in time execution\n",
        "\n",
        "print(\"-------TRAIN-------\")\n",
        "print(f\"Train cpu: {train_cpu_time}, Train gpu: {train_gpu_time}\")\n",
        "print(f\"Difference in time: {abs(train_cpu_time-train_gpu_time)}\\n\")\n",
        "\n",
        "print(\"-------TEST-------\")\n",
        "print(f\"Test cpu: {test_cpu_time}, Test gpu: {test_gpu_time}\")\n",
        "print(f\"Difference in time: {abs(test_cpu_time-test_gpu_time)}\\n\")"
      ],
      "metadata": {
        "id": "5Q6Wdk4i9ug4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "64bb4266-f0b0-49d5-b68e-1dbeee5f5508"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "-------TRAIN-------\n",
            "Train cpu: 4.008782863616943, Train gpu: 7.631873607635498\n",
            "Difference in time: 3.6230907440185547\n",
            "\n",
            "-------TEST-------\n",
            "Test cpu: 0.0382542610168457, Test gpu: 0.06455135345458984\n",
            "Difference in time: 0.02629709243774414\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 7: Choose model***"
      ],
      "metadata": {
        "id": "_VmjMInPdsbW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss1,f1_cpu,accuracy1,cm1 = testFNN(val_dataloader,lossfun,fnn_model_cpu)\n",
        "loss2,f1_gpu,accuracy2,cm2 = testFNN_GPU(val_dataloader,lossfun,fnn_model_gpu)\n",
        "\n",
        "print(f\"f1 score in cpu: {f1_cpu}\")\n",
        "print(f\"f1 score in gpu: {f1_gpu}\")\n",
        "bestf1 = max(f1_cpu,f1_gpu)\n",
        "print(f\"Best f1 score for validation set: {bestf1}\")"
      ],
      "metadata": {
        "id": "zCrPhPNjdyl1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "27796499-2999-4500-d96b-104774fb78cb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "f1 score in cpu: 0.6261058573818516\n",
            "f1 score in gpu: 0.6142824251385467\n",
            "Best f1 score for validation set: 0.6261058573818516\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "cpu model for calculating accuracy in test set:"
      ],
      "metadata": {
        "id": "1bg-35ay6sR1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainVal(num_epochs,optimizer,trainloader,validloader,lossfun,fnn):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    fnn.train()\n",
        "    for i, (X, labels) in enumerate(trainloader):\n",
        "\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = fnn(X.to(device))\n",
        "      loss = lossfun(output, labels)\n",
        "\n",
        "      # backpropagation\n",
        "      optimizer.zero_grad()\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "      train_loss += loss.item()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    fnn.eval()\n",
        "    for X, labels in validloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = fnn(X.to(device))\n",
        "      loss = lossfun(output, labels)\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(trainloader)} \\t\\t Validation Loss: {valid_loss / len(validloader)}')\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "        print(f'Validation Loss Decreased({min_valid_loss:.6f}--->{valid_loss:.6f}) \\t Saving The Model')\n",
        "        min_valid_loss = valid_loss\n",
        "\n",
        "        # Saving State Dict\n",
        "        torch.save(fnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return fnn"
      ],
      "metadata": {
        "id": "t-LeOQ-C6OOz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss().to(device)\n",
        "learning_rate = 0.002\n",
        "fnn = FNN().to(device)\n",
        "optimizer = torch.optim.SGD(fnn.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "fnn = trainVal(num_epochs,optimizer,train_dataloader,val_dataloader,lossfun,fnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BY2NNNx36rUZ",
        "outputId": "4c4bc01b-f531-4da8-a6a3-6af4ed9449fe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1 \t\t Training Loss: 1.380192672610283 \t\t Validation Loss: 1.3761721324920655\n",
            "Validation Loss Decreased(inf--->68.808607) \t Saving The Model\n",
            "Epoch 2 \t\t Training Loss: 1.3591811960935594 \t\t Validation Loss: 1.3543713688850403\n",
            "Validation Loss Decreased(68.808607--->67.718568) \t Saving The Model\n",
            "Epoch 3 \t\t Training Loss: 1.335960300564766 \t\t Validation Loss: 1.3225020670890808\n",
            "Validation Loss Decreased(67.718568--->66.125103) \t Saving The Model\n",
            "Epoch 4 \t\t Training Loss: 1.311060591340065 \t\t Validation Loss: 1.3000379252433776\n",
            "Validation Loss Decreased(66.125103--->65.001896) \t Saving The Model\n",
            "Epoch 5 \t\t Training Loss: 1.2875935423374176 \t\t Validation Loss: 1.2848359370231628\n",
            "Validation Loss Decreased(65.001896--->64.241797) \t Saving The Model\n",
            "Epoch 6 \t\t Training Loss: 1.260884621143341 \t\t Validation Loss: 1.266658868789673\n",
            "Validation Loss Decreased(64.241797--->63.332943) \t Saving The Model\n",
            "Epoch 7 \t\t Training Loss: 1.231283233165741 \t\t Validation Loss: 1.2241529297828675\n",
            "Validation Loss Decreased(63.332943--->61.207646) \t Saving The Model\n",
            "Epoch 8 \t\t Training Loss: 1.2049213314056397 \t\t Validation Loss: 1.1914734315872193\n",
            "Validation Loss Decreased(61.207646--->59.573672) \t Saving The Model\n",
            "Epoch 9 \t\t Training Loss: 1.1742262622714044 \t\t Validation Loss: 1.1629763126373291\n",
            "Validation Loss Decreased(59.573672--->58.148816) \t Saving The Model\n",
            "Epoch 10 \t\t Training Loss: 1.1473706912994386 \t\t Validation Loss: 1.1419625401496887\n",
            "Validation Loss Decreased(58.148816--->57.098127) \t Saving The Model\n",
            "Epoch 11 \t\t Training Loss: 1.1171190321445466 \t\t Validation Loss: 1.0912788379192353\n",
            "Validation Loss Decreased(57.098127--->54.563942) \t Saving The Model\n",
            "Epoch 12 \t\t Training Loss: 1.0937781012058259 \t\t Validation Loss: 1.0715236711502074\n",
            "Validation Loss Decreased(54.563942--->53.576184) \t Saving The Model\n",
            "Epoch 13 \t\t Training Loss: 1.0732101464271546 \t\t Validation Loss: 1.0587428736686706\n",
            "Validation Loss Decreased(53.576184--->52.937144) \t Saving The Model\n",
            "Epoch 14 \t\t Training Loss: 1.0554460176825524 \t\t Validation Loss: 1.027870502471924\n",
            "Validation Loss Decreased(52.937144--->51.393525) \t Saving The Model\n",
            "Epoch 15 \t\t Training Loss: 1.0351928663253784 \t\t Validation Loss: 0.9982959806919098\n",
            "Validation Loss Decreased(51.393525--->49.914799) \t Saving The Model\n",
            "Epoch 16 \t\t Training Loss: 1.0189081090688705 \t\t Validation Loss: 1.0560825490951538\n",
            "Epoch 17 \t\t Training Loss: 1.0131004840135573 \t\t Validation Loss: 0.9887616741657257\n",
            "Validation Loss Decreased(49.914799--->49.438084) \t Saving The Model\n",
            "Epoch 18 \t\t Training Loss: 1.0014569261670112 \t\t Validation Loss: 0.951047019958496\n",
            "Validation Loss Decreased(49.438084--->47.552351) \t Saving The Model\n",
            "Epoch 19 \t\t Training Loss: 0.9874288982152939 \t\t Validation Loss: 0.9435640239715576\n",
            "Validation Loss Decreased(47.552351--->47.178201) \t Saving The Model\n",
            "Epoch 20 \t\t Training Loss: 0.9801444521546364 \t\t Validation Loss: 0.9392370975017548\n",
            "Validation Loss Decreased(47.178201--->46.961855) \t Saving The Model\n",
            "Epoch 21 \t\t Training Loss: 0.9723724180459976 \t\t Validation Loss: 0.9156471300125122\n",
            "Validation Loss Decreased(46.961855--->45.782357) \t Saving The Model\n",
            "Epoch 22 \t\t Training Loss: 0.9676173514127732 \t\t Validation Loss: 0.9262737989425659\n",
            "Epoch 23 \t\t Training Loss: 0.9535735028982163 \t\t Validation Loss: 0.9193859696388245\n",
            "Epoch 24 \t\t Training Loss: 0.950967225432396 \t\t Validation Loss: 0.8954604518413544\n",
            "Validation Loss Decreased(45.782357--->44.773023) \t Saving The Model\n",
            "Epoch 25 \t\t Training Loss: 0.9483831346035003 \t\t Validation Loss: 0.9080204224586487\n",
            "Epoch 26 \t\t Training Loss: 0.9451948139071464 \t\t Validation Loss: 0.896908836364746\n",
            "Epoch 27 \t\t Training Loss: 0.9372265389561654 \t\t Validation Loss: 0.8845013582706451\n",
            "Validation Loss Decreased(44.773023--->44.225068) \t Saving The Model\n",
            "Epoch 28 \t\t Training Loss: 0.9409787762165069 \t\t Validation Loss: 0.8959461021423339\n",
            "Epoch 29 \t\t Training Loss: 0.9313977052271366 \t\t Validation Loss: 0.8889926028251648\n",
            "Epoch 30 \t\t Training Loss: 0.9240168471634388 \t\t Validation Loss: 0.8705377554893494\n",
            "Validation Loss Decreased(44.225068--->43.526888) \t Saving The Model\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "loss,f1,accuracy,cm = testFNN(test_dataloader,lossfun,fnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aIPMRBusCUTD",
        "outputId": "00179e3c-7d12-4b90-ac67-ef26ac11df60"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.059802\n",
            "Accuracy: 62.1%\n",
            "F1 macro averaged: 0.603796\n",
            "Confusion matrix: [[228  19  14  36]\n",
            " [ 11 300  10  35]\n",
            " [ 23  69 247  60]\n",
            " [ 21 130  94  79]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q2: Convolutional Neural Network**"
      ],
      "metadata": {
        "id": "4p_SQ-_ld6E4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 1: Load data (spectrograms)***"
      ],
      "metadata": {
        "id": "twfVEtewnH_-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import glob\n",
        "from google.colab import drive\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from torch.utils.data import DataLoader\n",
        "import matplotlib.pyplot as plt\n",
        "import librosa\n",
        "import librosa.display\n",
        "import random\n",
        "\n",
        "#load files as arrays\n",
        "train_melgrams_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/train/melgrams/X.npy\")\n",
        "train_melgrams_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/train/melgrams/labels.npy\")\n",
        "test_melgrams_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/test/melgrams/X.npy\")\n",
        "test_melgrams_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/test/melgrams/labels.npy\")\n",
        "val_melgrams_X = np.load(\"/content/drive/MyDrive/music_genre_data_di/val/melgrams/X.npy\")\n",
        "val_melgrams_labels = np.load(\"/content/drive/MyDrive/music_genre_data_di/val/melgrams/labels.npy\")\n",
        "\n",
        "#give a number to each category\n",
        "categories = [\"classical\",\"hiphop\",\"rock_metal_hardrock\",\"blues\"]\n",
        "num_categories = dict(zip(categories,range(0,4)))\n",
        "\n",
        "#for test set\n",
        "test_labels = (pd.Series(test_melgrams_labels)).map(num_categories)\n",
        "test_labels = list(test_labels)\n",
        "testdata = list(zip(test_melgrams_X,test_labels))\n",
        "\n",
        "#for train set\n",
        "train_labels = (pd.Series(train_melgrams_labels)).map(num_categories)\n",
        "train_labels = list(train_labels)\n",
        "traindata = list(zip(train_melgrams_X,train_labels))\n",
        "\n",
        "#for val set\n",
        "val_labels = (pd.Series(val_melgrams_labels)).map(num_categories)\n",
        "val_labels = list(val_labels)\n",
        "valdata = list(zip(val_melgrams_X,val_labels))\n",
        "\n",
        "#Dataloaders\n",
        "traindataloader = DataLoader(traindata,batch_size = 16, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=16,shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=16,shuffle=True)\n",
        "\n",
        "\n",
        "#visualisation\n",
        "classical = []\n",
        "hiphop = []\n",
        "rock = []\n",
        "blues = []\n",
        "\n",
        "for i in range(0,len(traindata)):\n",
        "  if traindata[i][1] == 0:\n",
        "    classical.append(train_melgrams_X[i])\n",
        "\n",
        "for i in range(0,len(traindata)):\n",
        "  if traindata[i][1] == 1:\n",
        "    hiphop.append(train_melgrams_X[i])\n",
        "\n",
        "for i in range(0,len(traindata)):\n",
        "  if traindata[i][1] == 2:\n",
        "    rock.append(train_melgrams_X[i])\n",
        "\n",
        "for i in range(0,len(traindata)):\n",
        "  if traindata[i][1] == 3:\n",
        "    blues.append(train_melgrams_X[i])\n",
        "\n",
        "#from train set\n",
        "\n",
        "#classical music\n",
        "number = random.randint(0,len(classical)-1)\n",
        "plt.figure()\n",
        "librosa.display.specshow(classical[number])\n",
        "plt.colorbar()\n",
        "plt.title(\"Classical melgram visualisation\")\n",
        "\n",
        "#hiphop\n",
        "number = random.randint(0,len(hiphop)-1)\n",
        "plt.figure()\n",
        "librosa.display.specshow(hiphop[number])\n",
        "plt.colorbar()\n",
        "plt.title(\"Hiphop melgram visualisation\")\n",
        "\n",
        "#rock-metal\n",
        "number = random.randint(0,len(rock)-1)\n",
        "plt.figure()\n",
        "librosa.display.specshow(rock[number])\n",
        "plt.colorbar()\n",
        "plt.title(\"Rock melgram visualisation\")\n",
        "\n",
        "#blues\n",
        "number = random.randint(0,len(blues)-1)\n",
        "plt.figure()\n",
        "librosa.display.specshow(blues[number])\n",
        "plt.colorbar()\n",
        "plt.title(\"Blues melgram visualisation\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bmMZ3n_QnWxz",
        "outputId": "330b645b-7e80-4179-c290-58fd9c23be70"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0.5, 1.0, 'Blues melgram visualisation')"
            ]
          },
          "metadata": {},
          "execution_count": 30
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Define Neural Network***"
      ],
      "metadata": {
        "id": "mcJrw66F-11r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "Q-8FhB13-kk3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: Network Training***"
      ],
      "metadata": {
        "id": "Jth9F4svNeP7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def trainCNN(num_epochs,optimizer,trainloader,validloader,lossfun,cnn):\n",
        "  min_valid_loss = np.inf\n",
        "  size = len(trainloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "    train_loss = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(trainloader):\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        output = cnn(X)\n",
        "        loss = lossfun(output, labels)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          loss,current = loss.item(), i*len(X)\n",
        "          print(f\"loss: {loss:>7f} [{current:>5d}/{size:>5d}]\")\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in validloader:\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X)\n",
        "      loss = lossfun(output, labels)\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "        min_valid_loss = valid_loss\n",
        "\n",
        "        # Saving State Dict\n",
        "        torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "  return cnn\n",
        "\n",
        "def testCNN(dataloader,lossfun,cnn):\n",
        "\n",
        "  cnn.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  # Iterate through test dataset\n",
        "  with torch.no_grad():\n",
        "    for X, labels in dataloader:\n",
        "      X = X.type(torch.float)\n",
        "      output = cnn(X)\n",
        "      test_loss += lossfun(output, labels).item()\n",
        "      correct += (output.argmax(1)==labels).type(torch.float).sum().item()\n",
        "      y_true.append(labels)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_true = torch.cat(y_true)\n",
        "  y_pred = torch.cat(y_pred)\n",
        "\n",
        "  correct /= size\n",
        "  accuracy = 100*correct\n",
        "  test_loss /= size\n",
        "  f1 = f1_score(y_true,y_pred,average='macro')\n",
        "  cm = confusion_matrix(y_true,y_pred)\n",
        "  return test_loss,f1,accuracy,cm"
      ],
      "metadata": {
        "id": "z-KQfm5XNkqZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "cnn_model_cpu = CNN()\n",
        "optimizer = torch.optim.SGD(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "cnn_model_cpu = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 381
        },
        "id": "DF_BskjcO931",
        "outputId": "ff89a37f-1ae2-4ad6-fb8b-f40f1ba89259"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "RuntimeError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-33-f1aa83115c64>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m#train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mcnn_model_cpu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrainCNN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_epochs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraindataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvaldataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlossfun\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcnn_model_cpu\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-32-b0ac0878f182>\u001b[0m in \u001b[0;36mtrainCNN\u001b[0;34m(num_epochs, optimizer, trainloader, validloader, lossfun, cnn)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrainloader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcnn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlossfun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-31-78ba9f0dbe5e>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     34\u001b[0m       \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 36\u001b[0;31m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m       \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfc2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1128\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1131\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1132\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    112\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 114\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    116\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (16x71680 and 1024x1024)"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Network cannot be trained. We need pooling and padding metrics:"
      ],
      "metadata": {
        "id": "WmTga0abpiQ5"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 4: Pooling and padding***"
      ],
      "metadata": {
        "id": "StQsrekKP6Gl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "class CNNnew(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "R-s6wuUFQAxj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "cnn_model_cpu = CNNnew()\n",
        "optimizer = torch.optim.SGD(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "\n",
        "#train\n",
        "cnn_model_cpu = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CwhT8NcnQ-40",
        "outputId": "5aefcdbf-6afb-4052-ec0d-b021d095199d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "loss: 1.301586 [    0/ 3200]\n",
            "loss: 1.379985 [  160/ 3200]\n",
            "loss: 1.368546 [  320/ 3200]\n",
            "loss: 1.498016 [  480/ 3200]\n",
            "loss: 1.368024 [  640/ 3200]\n",
            "loss: 1.344743 [  800/ 3200]\n",
            "loss: 1.305199 [  960/ 3200]\n",
            "loss: 1.426642 [ 1120/ 3200]\n",
            "loss: 1.474251 [ 1280/ 3200]\n",
            "loss: 1.326331 [ 1440/ 3200]\n",
            "loss: 1.279463 [ 1600/ 3200]\n",
            "loss: 1.274236 [ 1760/ 3200]\n",
            "loss: 1.199026 [ 1920/ 3200]\n",
            "loss: 1.288182 [ 2080/ 3200]\n",
            "loss: 1.189274 [ 2240/ 3200]\n",
            "loss: 1.278201 [ 2400/ 3200]\n",
            "loss: 1.360150 [ 2560/ 3200]\n",
            "loss: 1.308775 [ 2720/ 3200]\n",
            "loss: 1.197020 [ 2880/ 3200]\n",
            "loss: 1.218017 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "loss: 1.216144 [    0/ 3200]\n",
            "loss: 1.208267 [  160/ 3200]\n",
            "loss: 1.447653 [  320/ 3200]\n",
            "loss: 1.461327 [  480/ 3200]\n",
            "loss: 1.294999 [  640/ 3200]\n",
            "loss: 1.175988 [  800/ 3200]\n",
            "loss: 1.404074 [  960/ 3200]\n",
            "loss: 1.123342 [ 1120/ 3200]\n",
            "loss: 1.039132 [ 1280/ 3200]\n",
            "loss: 1.120132 [ 1440/ 3200]\n",
            "loss: 1.238570 [ 1600/ 3200]\n",
            "loss: 1.521890 [ 1760/ 3200]\n",
            "loss: 1.087793 [ 1920/ 3200]\n",
            "loss: 1.182742 [ 2080/ 3200]\n",
            "loss: 1.198641 [ 2240/ 3200]\n",
            "loss: 1.081676 [ 2400/ 3200]\n",
            "loss: 1.483722 [ 2560/ 3200]\n",
            "loss: 1.248693 [ 2720/ 3200]\n",
            "loss: 1.087224 [ 2880/ 3200]\n",
            "loss: 1.151675 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "loss: 0.948594 [    0/ 3200]\n",
            "loss: 0.985453 [  160/ 3200]\n",
            "loss: 0.854796 [  320/ 3200]\n",
            "loss: 1.108684 [  480/ 3200]\n",
            "loss: 1.035162 [  640/ 3200]\n",
            "loss: 0.812024 [  800/ 3200]\n",
            "loss: 1.000622 [  960/ 3200]\n",
            "loss: 1.168722 [ 1120/ 3200]\n",
            "loss: 0.887996 [ 1280/ 3200]\n",
            "loss: 1.029806 [ 1440/ 3200]\n",
            "loss: 1.181031 [ 1600/ 3200]\n",
            "loss: 1.033902 [ 1760/ 3200]\n",
            "loss: 1.293375 [ 1920/ 3200]\n",
            "loss: 0.721302 [ 2080/ 3200]\n",
            "loss: 1.106788 [ 2240/ 3200]\n",
            "loss: 0.716084 [ 2400/ 3200]\n",
            "loss: 1.045369 [ 2560/ 3200]\n",
            "loss: 0.916842 [ 2720/ 3200]\n",
            "loss: 1.021516 [ 2880/ 3200]\n",
            "loss: 0.830256 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "loss: 0.779222 [    0/ 3200]\n",
            "loss: 0.942649 [  160/ 3200]\n",
            "loss: 0.807441 [  320/ 3200]\n",
            "loss: 0.958471 [  480/ 3200]\n",
            "loss: 0.693598 [  640/ 3200]\n",
            "loss: 0.990505 [  800/ 3200]\n",
            "loss: 0.733026 [  960/ 3200]\n",
            "loss: 0.893424 [ 1120/ 3200]\n",
            "loss: 0.753496 [ 1280/ 3200]\n",
            "loss: 0.721735 [ 1440/ 3200]\n",
            "loss: 0.870450 [ 1600/ 3200]\n",
            "loss: 0.922469 [ 1760/ 3200]\n",
            "loss: 0.843400 [ 1920/ 3200]\n",
            "loss: 0.811427 [ 2080/ 3200]\n",
            "loss: 0.763642 [ 2240/ 3200]\n",
            "loss: 0.886503 [ 2400/ 3200]\n",
            "loss: 0.896485 [ 2560/ 3200]\n",
            "loss: 0.815181 [ 2720/ 3200]\n",
            "loss: 0.836684 [ 2880/ 3200]\n",
            "loss: 0.814330 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "loss: 1.328615 [    0/ 3200]\n",
            "loss: 0.873206 [  160/ 3200]\n",
            "loss: 0.984019 [  320/ 3200]\n",
            "loss: 1.042509 [  480/ 3200]\n",
            "loss: 1.016211 [  640/ 3200]\n",
            "loss: 0.786108 [  800/ 3200]\n",
            "loss: 0.812632 [  960/ 3200]\n",
            "loss: 0.926932 [ 1120/ 3200]\n",
            "loss: 0.908857 [ 1280/ 3200]\n",
            "loss: 1.122576 [ 1440/ 3200]\n",
            "loss: 0.895040 [ 1600/ 3200]\n",
            "loss: 0.859926 [ 1760/ 3200]\n",
            "loss: 0.865565 [ 1920/ 3200]\n",
            "loss: 0.996023 [ 2080/ 3200]\n",
            "loss: 0.711495 [ 2240/ 3200]\n",
            "loss: 0.967658 [ 2400/ 3200]\n",
            "loss: 0.846542 [ 2560/ 3200]\n",
            "loss: 0.793933 [ 2720/ 3200]\n",
            "loss: 0.961526 [ 2880/ 3200]\n",
            "loss: 0.721271 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "loss: 0.881034 [    0/ 3200]\n",
            "loss: 0.723364 [  160/ 3200]\n",
            "loss: 0.892667 [  320/ 3200]\n",
            "loss: 0.829078 [  480/ 3200]\n",
            "loss: 0.885011 [  640/ 3200]\n",
            "loss: 0.751649 [  800/ 3200]\n",
            "loss: 0.593620 [  960/ 3200]\n",
            "loss: 1.056048 [ 1120/ 3200]\n",
            "loss: 0.953102 [ 1280/ 3200]\n",
            "loss: 0.893706 [ 1440/ 3200]\n",
            "loss: 1.279933 [ 1600/ 3200]\n",
            "loss: 0.904553 [ 1760/ 3200]\n",
            "loss: 0.817659 [ 1920/ 3200]\n",
            "loss: 0.627571 [ 2080/ 3200]\n",
            "loss: 0.510124 [ 2240/ 3200]\n",
            "loss: 1.112183 [ 2400/ 3200]\n",
            "loss: 0.992951 [ 2560/ 3200]\n",
            "loss: 0.793126 [ 2720/ 3200]\n",
            "loss: 0.942705 [ 2880/ 3200]\n",
            "loss: 0.578058 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "loss: 0.626374 [    0/ 3200]\n",
            "loss: 0.804395 [  160/ 3200]\n",
            "loss: 0.478506 [  320/ 3200]\n",
            "loss: 0.599974 [  480/ 3200]\n",
            "loss: 1.324645 [  640/ 3200]\n",
            "loss: 0.925764 [  800/ 3200]\n",
            "loss: 0.930355 [  960/ 3200]\n",
            "loss: 0.595163 [ 1120/ 3200]\n",
            "loss: 0.979109 [ 1280/ 3200]\n",
            "loss: 0.936260 [ 1440/ 3200]\n",
            "loss: 0.903656 [ 1600/ 3200]\n",
            "loss: 0.645990 [ 1760/ 3200]\n",
            "loss: 0.475120 [ 1920/ 3200]\n",
            "loss: 1.530496 [ 2080/ 3200]\n",
            "loss: 0.869717 [ 2240/ 3200]\n",
            "loss: 0.929065 [ 2400/ 3200]\n",
            "loss: 0.734202 [ 2560/ 3200]\n",
            "loss: 0.642217 [ 2720/ 3200]\n",
            "loss: 0.773689 [ 2880/ 3200]\n",
            "loss: 0.798627 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "loss: 0.697590 [    0/ 3200]\n",
            "loss: 0.452898 [  160/ 3200]\n",
            "loss: 0.635907 [  320/ 3200]\n",
            "loss: 0.712129 [  480/ 3200]\n",
            "loss: 0.720336 [  640/ 3200]\n",
            "loss: 0.567716 [  800/ 3200]\n",
            "loss: 0.842703 [  960/ 3200]\n",
            "loss: 0.717792 [ 1120/ 3200]\n",
            "loss: 0.646467 [ 1280/ 3200]\n",
            "loss: 0.876345 [ 1440/ 3200]\n",
            "loss: 0.823054 [ 1600/ 3200]\n",
            "loss: 1.076195 [ 1760/ 3200]\n",
            "loss: 0.641560 [ 1920/ 3200]\n",
            "loss: 0.868727 [ 2080/ 3200]\n",
            "loss: 0.599124 [ 2240/ 3200]\n",
            "loss: 0.464876 [ 2400/ 3200]\n",
            "loss: 0.875136 [ 2560/ 3200]\n",
            "loss: 0.625884 [ 2720/ 3200]\n",
            "loss: 0.798296 [ 2880/ 3200]\n",
            "loss: 0.710589 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "loss: 0.641558 [    0/ 3200]\n",
            "loss: 1.129676 [  160/ 3200]\n",
            "loss: 0.707896 [  320/ 3200]\n",
            "loss: 0.587613 [  480/ 3200]\n",
            "loss: 0.754579 [  640/ 3200]\n",
            "loss: 0.796141 [  800/ 3200]\n",
            "loss: 0.701936 [  960/ 3200]\n",
            "loss: 0.660946 [ 1120/ 3200]\n",
            "loss: 0.399330 [ 1280/ 3200]\n",
            "loss: 0.390429 [ 1440/ 3200]\n",
            "loss: 0.629554 [ 1600/ 3200]\n",
            "loss: 0.762604 [ 1760/ 3200]\n",
            "loss: 0.898699 [ 1920/ 3200]\n",
            "loss: 0.660287 [ 2080/ 3200]\n",
            "loss: 0.818693 [ 2240/ 3200]\n",
            "loss: 1.143990 [ 2400/ 3200]\n",
            "loss: 0.676711 [ 2560/ 3200]\n",
            "loss: 0.482358 [ 2720/ 3200]\n",
            "loss: 0.659500 [ 2880/ 3200]\n",
            "loss: 0.633060 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "loss: 0.940699 [    0/ 3200]\n",
            "loss: 0.536152 [  160/ 3200]\n",
            "loss: 0.998468 [  320/ 3200]\n",
            "loss: 0.606540 [  480/ 3200]\n",
            "loss: 0.777219 [  640/ 3200]\n",
            "loss: 0.470729 [  800/ 3200]\n",
            "loss: 0.654553 [  960/ 3200]\n",
            "loss: 0.519915 [ 1120/ 3200]\n",
            "loss: 0.815909 [ 1280/ 3200]\n",
            "loss: 0.331619 [ 1440/ 3200]\n",
            "loss: 0.554366 [ 1600/ 3200]\n",
            "loss: 0.725664 [ 1760/ 3200]\n",
            "loss: 0.791881 [ 1920/ 3200]\n",
            "loss: 0.546434 [ 2080/ 3200]\n",
            "loss: 0.711081 [ 2240/ 3200]\n",
            "loss: 0.749230 [ 2400/ 3200]\n",
            "loss: 0.654367 [ 2560/ 3200]\n",
            "loss: 0.409752 [ 2720/ 3200]\n",
            "loss: 0.507506 [ 2880/ 3200]\n",
            "loss: 0.730354 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "loss: 0.611660 [    0/ 3200]\n",
            "loss: 0.735265 [  160/ 3200]\n",
            "loss: 0.659566 [  320/ 3200]\n",
            "loss: 0.569374 [  480/ 3200]\n",
            "loss: 0.697338 [  640/ 3200]\n",
            "loss: 1.040864 [  800/ 3200]\n",
            "loss: 0.839814 [  960/ 3200]\n",
            "loss: 0.625705 [ 1120/ 3200]\n",
            "loss: 0.373632 [ 1280/ 3200]\n",
            "loss: 0.831567 [ 1440/ 3200]\n",
            "loss: 1.016502 [ 1600/ 3200]\n",
            "loss: 0.490413 [ 1760/ 3200]\n",
            "loss: 0.692480 [ 1920/ 3200]\n",
            "loss: 1.069116 [ 2080/ 3200]\n",
            "loss: 0.656602 [ 2240/ 3200]\n",
            "loss: 0.867468 [ 2400/ 3200]\n",
            "loss: 0.732492 [ 2560/ 3200]\n",
            "loss: 0.498893 [ 2720/ 3200]\n",
            "loss: 0.504858 [ 2880/ 3200]\n",
            "loss: 0.453990 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "loss: 0.480628 [    0/ 3200]\n",
            "loss: 0.536077 [  160/ 3200]\n",
            "loss: 0.393521 [  320/ 3200]\n",
            "loss: 0.504158 [  480/ 3200]\n",
            "loss: 0.530624 [  640/ 3200]\n",
            "loss: 0.509276 [  800/ 3200]\n",
            "loss: 0.532341 [  960/ 3200]\n",
            "loss: 0.557366 [ 1120/ 3200]\n",
            "loss: 0.711351 [ 1280/ 3200]\n",
            "loss: 0.568918 [ 1440/ 3200]\n",
            "loss: 1.081304 [ 1600/ 3200]\n",
            "loss: 0.444062 [ 1760/ 3200]\n",
            "loss: 0.975297 [ 1920/ 3200]\n",
            "loss: 0.319876 [ 2080/ 3200]\n",
            "loss: 0.916470 [ 2240/ 3200]\n",
            "loss: 0.645394 [ 2400/ 3200]\n",
            "loss: 0.411221 [ 2560/ 3200]\n",
            "loss: 0.507829 [ 2720/ 3200]\n",
            "loss: 0.485404 [ 2880/ 3200]\n",
            "loss: 0.484833 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "loss: 0.545406 [    0/ 3200]\n",
            "loss: 0.491756 [  160/ 3200]\n",
            "loss: 0.518559 [  320/ 3200]\n",
            "loss: 0.254853 [  480/ 3200]\n",
            "loss: 0.896273 [  640/ 3200]\n",
            "loss: 0.488785 [  800/ 3200]\n",
            "loss: 0.409506 [  960/ 3200]\n",
            "loss: 0.422672 [ 1120/ 3200]\n",
            "loss: 0.672546 [ 1280/ 3200]\n",
            "loss: 0.621030 [ 1440/ 3200]\n",
            "loss: 0.987859 [ 1600/ 3200]\n",
            "loss: 0.748553 [ 1760/ 3200]\n",
            "loss: 0.520831 [ 1920/ 3200]\n",
            "loss: 0.674646 [ 2080/ 3200]\n",
            "loss: 0.569095 [ 2240/ 3200]\n",
            "loss: 0.537188 [ 2400/ 3200]\n",
            "loss: 0.509679 [ 2560/ 3200]\n",
            "loss: 0.285734 [ 2720/ 3200]\n",
            "loss: 0.268013 [ 2880/ 3200]\n",
            "loss: 0.449993 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "loss: 0.287290 [    0/ 3200]\n",
            "loss: 0.937989 [  160/ 3200]\n",
            "loss: 0.429038 [  320/ 3200]\n",
            "loss: 0.891067 [  480/ 3200]\n",
            "loss: 0.482800 [  640/ 3200]\n",
            "loss: 0.430352 [  800/ 3200]\n",
            "loss: 0.232257 [  960/ 3200]\n",
            "loss: 0.517657 [ 1120/ 3200]\n",
            "loss: 0.719675 [ 1280/ 3200]\n",
            "loss: 0.411549 [ 1440/ 3200]\n",
            "loss: 0.457453 [ 1600/ 3200]\n",
            "loss: 0.666643 [ 1760/ 3200]\n",
            "loss: 0.248916 [ 1920/ 3200]\n",
            "loss: 0.257799 [ 2080/ 3200]\n",
            "loss: 0.725555 [ 2240/ 3200]\n",
            "loss: 0.880998 [ 2400/ 3200]\n",
            "loss: 0.590924 [ 2560/ 3200]\n",
            "loss: 0.417336 [ 2720/ 3200]\n",
            "loss: 0.480245 [ 2880/ 3200]\n",
            "loss: 0.693536 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "loss: 0.591040 [    0/ 3200]\n",
            "loss: 0.305781 [  160/ 3200]\n",
            "loss: 0.465675 [  320/ 3200]\n",
            "loss: 0.509119 [  480/ 3200]\n",
            "loss: 0.308902 [  640/ 3200]\n",
            "loss: 0.415949 [  800/ 3200]\n",
            "loss: 0.409348 [  960/ 3200]\n",
            "loss: 0.497216 [ 1120/ 3200]\n",
            "loss: 0.768232 [ 1280/ 3200]\n",
            "loss: 0.803025 [ 1440/ 3200]\n",
            "loss: 0.644665 [ 1600/ 3200]\n",
            "loss: 0.669972 [ 1760/ 3200]\n",
            "loss: 0.296114 [ 1920/ 3200]\n",
            "loss: 0.625063 [ 2080/ 3200]\n",
            "loss: 0.332866 [ 2240/ 3200]\n",
            "loss: 0.714712 [ 2400/ 3200]\n",
            "loss: 0.976543 [ 2560/ 3200]\n",
            "loss: 0.226509 [ 2720/ 3200]\n",
            "loss: 0.442241 [ 2880/ 3200]\n",
            "loss: 0.793581 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "loss: 0.679532 [    0/ 3200]\n",
            "loss: 0.373931 [  160/ 3200]\n",
            "loss: 0.416036 [  320/ 3200]\n",
            "loss: 0.649323 [  480/ 3200]\n",
            "loss: 0.682193 [  640/ 3200]\n",
            "loss: 0.445888 [  800/ 3200]\n",
            "loss: 0.462761 [  960/ 3200]\n",
            "loss: 0.290780 [ 1120/ 3200]\n",
            "loss: 0.953289 [ 1280/ 3200]\n",
            "loss: 0.749789 [ 1440/ 3200]\n",
            "loss: 0.670306 [ 1600/ 3200]\n",
            "loss: 0.799979 [ 1760/ 3200]\n",
            "loss: 0.642386 [ 1920/ 3200]\n",
            "loss: 0.338545 [ 2080/ 3200]\n",
            "loss: 0.421412 [ 2240/ 3200]\n",
            "loss: 0.401673 [ 2400/ 3200]\n",
            "loss: 0.258284 [ 2560/ 3200]\n",
            "loss: 0.685772 [ 2720/ 3200]\n",
            "loss: 0.461561 [ 2880/ 3200]\n",
            "loss: 0.339385 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "loss: 0.474149 [    0/ 3200]\n",
            "loss: 0.562173 [  160/ 3200]\n",
            "loss: 0.251076 [  320/ 3200]\n",
            "loss: 0.302917 [  480/ 3200]\n",
            "loss: 0.303206 [  640/ 3200]\n",
            "loss: 0.266407 [  800/ 3200]\n",
            "loss: 0.987897 [  960/ 3200]\n",
            "loss: 0.585273 [ 1120/ 3200]\n",
            "loss: 0.413039 [ 1280/ 3200]\n",
            "loss: 0.402439 [ 1440/ 3200]\n",
            "loss: 0.379078 [ 1600/ 3200]\n",
            "loss: 0.633243 [ 1760/ 3200]\n",
            "loss: 0.871725 [ 1920/ 3200]\n",
            "loss: 0.546372 [ 2080/ 3200]\n",
            "loss: 0.423151 [ 2240/ 3200]\n",
            "loss: 0.378492 [ 2400/ 3200]\n",
            "loss: 0.307673 [ 2560/ 3200]\n",
            "loss: 0.571735 [ 2720/ 3200]\n",
            "loss: 0.824740 [ 2880/ 3200]\n",
            "loss: 0.607024 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "loss: 0.598592 [    0/ 3200]\n",
            "loss: 0.408661 [  160/ 3200]\n",
            "loss: 0.461472 [  320/ 3200]\n",
            "loss: 0.546342 [  480/ 3200]\n",
            "loss: 0.528163 [  640/ 3200]\n",
            "loss: 0.470033 [  800/ 3200]\n",
            "loss: 0.303854 [  960/ 3200]\n",
            "loss: 0.386199 [ 1120/ 3200]\n",
            "loss: 0.608722 [ 1280/ 3200]\n",
            "loss: 0.469837 [ 1440/ 3200]\n",
            "loss: 0.356173 [ 1600/ 3200]\n",
            "loss: 0.352059 [ 1760/ 3200]\n",
            "loss: 0.479729 [ 1920/ 3200]\n",
            "loss: 0.445906 [ 2080/ 3200]\n",
            "loss: 0.533159 [ 2240/ 3200]\n",
            "loss: 0.501872 [ 2400/ 3200]\n",
            "loss: 0.278203 [ 2560/ 3200]\n",
            "loss: 0.604455 [ 2720/ 3200]\n",
            "loss: 0.488285 [ 2880/ 3200]\n",
            "loss: 0.615974 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "loss: 0.365296 [    0/ 3200]\n",
            "loss: 0.428124 [  160/ 3200]\n",
            "loss: 0.514612 [  320/ 3200]\n",
            "loss: 0.363298 [  480/ 3200]\n",
            "loss: 0.490133 [  640/ 3200]\n",
            "loss: 0.422815 [  800/ 3200]\n",
            "loss: 0.142279 [  960/ 3200]\n",
            "loss: 0.610145 [ 1120/ 3200]\n",
            "loss: 0.197997 [ 1280/ 3200]\n",
            "loss: 0.257686 [ 1440/ 3200]\n",
            "loss: 0.451898 [ 1600/ 3200]\n",
            "loss: 0.440846 [ 1760/ 3200]\n",
            "loss: 0.157598 [ 1920/ 3200]\n",
            "loss: 0.519286 [ 2080/ 3200]\n",
            "loss: 0.260788 [ 2240/ 3200]\n",
            "loss: 0.376943 [ 2400/ 3200]\n",
            "loss: 0.361729 [ 2560/ 3200]\n",
            "loss: 0.471824 [ 2720/ 3200]\n",
            "loss: 0.322183 [ 2880/ 3200]\n",
            "loss: 0.335418 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "loss: 0.198823 [    0/ 3200]\n",
            "loss: 0.397451 [  160/ 3200]\n",
            "loss: 0.307843 [  320/ 3200]\n",
            "loss: 0.628402 [  480/ 3200]\n",
            "loss: 0.196553 [  640/ 3200]\n",
            "loss: 0.660898 [  800/ 3200]\n",
            "loss: 0.454008 [  960/ 3200]\n",
            "loss: 0.209287 [ 1120/ 3200]\n",
            "loss: 0.213965 [ 1280/ 3200]\n",
            "loss: 0.270475 [ 1440/ 3200]\n",
            "loss: 0.334845 [ 1600/ 3200]\n",
            "loss: 0.132615 [ 1760/ 3200]\n",
            "loss: 0.400726 [ 1920/ 3200]\n",
            "loss: 0.346281 [ 2080/ 3200]\n",
            "loss: 0.358410 [ 2240/ 3200]\n",
            "loss: 0.273728 [ 2400/ 3200]\n",
            "loss: 0.339857 [ 2560/ 3200]\n",
            "loss: 0.492750 [ 2720/ 3200]\n",
            "loss: 0.434541 [ 2880/ 3200]\n",
            "loss: 0.254010 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "loss: 0.195920 [    0/ 3200]\n",
            "loss: 0.132823 [  160/ 3200]\n",
            "loss: 0.319517 [  320/ 3200]\n",
            "loss: 0.440706 [  480/ 3200]\n",
            "loss: 0.690695 [  640/ 3200]\n",
            "loss: 0.253691 [  800/ 3200]\n",
            "loss: 0.235454 [  960/ 3200]\n",
            "loss: 0.302957 [ 1120/ 3200]\n",
            "loss: 0.334782 [ 1280/ 3200]\n",
            "loss: 0.399660 [ 1440/ 3200]\n",
            "loss: 0.307438 [ 1600/ 3200]\n",
            "loss: 0.237840 [ 1760/ 3200]\n",
            "loss: 0.303981 [ 1920/ 3200]\n",
            "loss: 0.206341 [ 2080/ 3200]\n",
            "loss: 0.532133 [ 2240/ 3200]\n",
            "loss: 0.231743 [ 2400/ 3200]\n",
            "loss: 0.179855 [ 2560/ 3200]\n",
            "loss: 0.375366 [ 2720/ 3200]\n",
            "loss: 0.655598 [ 2880/ 3200]\n",
            "loss: 0.529750 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "loss: 0.358996 [    0/ 3200]\n",
            "loss: 0.199573 [  160/ 3200]\n",
            "loss: 0.348278 [  320/ 3200]\n",
            "loss: 0.156648 [  480/ 3200]\n",
            "loss: 0.912434 [  640/ 3200]\n",
            "loss: 0.195025 [  800/ 3200]\n",
            "loss: 0.226712 [  960/ 3200]\n",
            "loss: 0.131960 [ 1120/ 3200]\n",
            "loss: 0.395252 [ 1280/ 3200]\n",
            "loss: 0.151324 [ 1440/ 3200]\n",
            "loss: 0.392940 [ 1600/ 3200]\n",
            "loss: 0.297910 [ 1760/ 3200]\n",
            "loss: 0.360955 [ 1920/ 3200]\n",
            "loss: 0.255626 [ 2080/ 3200]\n",
            "loss: 0.231049 [ 2240/ 3200]\n",
            "loss: 0.151316 [ 2400/ 3200]\n",
            "loss: 0.227243 [ 2560/ 3200]\n",
            "loss: 0.358696 [ 2720/ 3200]\n",
            "loss: 0.310888 [ 2880/ 3200]\n",
            "loss: 0.596497 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "loss: 0.273595 [    0/ 3200]\n",
            "loss: 0.442004 [  160/ 3200]\n",
            "loss: 0.355298 [  320/ 3200]\n",
            "loss: 0.406456 [  480/ 3200]\n",
            "loss: 0.409787 [  640/ 3200]\n",
            "loss: 0.176771 [  800/ 3200]\n",
            "loss: 0.431249 [  960/ 3200]\n",
            "loss: 0.197624 [ 1120/ 3200]\n",
            "loss: 0.283985 [ 1280/ 3200]\n",
            "loss: 0.282270 [ 1440/ 3200]\n",
            "loss: 0.498964 [ 1600/ 3200]\n",
            "loss: 0.130142 [ 1760/ 3200]\n",
            "loss: 0.322209 [ 1920/ 3200]\n",
            "loss: 0.266610 [ 2080/ 3200]\n",
            "loss: 0.283306 [ 2240/ 3200]\n",
            "loss: 0.211738 [ 2400/ 3200]\n",
            "loss: 0.353389 [ 2560/ 3200]\n",
            "loss: 0.503369 [ 2720/ 3200]\n",
            "loss: 0.172089 [ 2880/ 3200]\n",
            "loss: 0.178329 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "loss: 0.065414 [    0/ 3200]\n",
            "loss: 0.277527 [  160/ 3200]\n",
            "loss: 0.180665 [  320/ 3200]\n",
            "loss: 0.279191 [  480/ 3200]\n",
            "loss: 0.198450 [  640/ 3200]\n",
            "loss: 0.165943 [  800/ 3200]\n",
            "loss: 0.126019 [  960/ 3200]\n",
            "loss: 0.242482 [ 1120/ 3200]\n",
            "loss: 0.148766 [ 1280/ 3200]\n",
            "loss: 0.346592 [ 1440/ 3200]\n",
            "loss: 0.319180 [ 1600/ 3200]\n",
            "loss: 0.249207 [ 1760/ 3200]\n",
            "loss: 0.083111 [ 1920/ 3200]\n",
            "loss: 0.193584 [ 2080/ 3200]\n",
            "loss: 0.096191 [ 2240/ 3200]\n",
            "loss: 0.115374 [ 2400/ 3200]\n",
            "loss: 0.287113 [ 2560/ 3200]\n",
            "loss: 0.319077 [ 2720/ 3200]\n",
            "loss: 0.277925 [ 2880/ 3200]\n",
            "loss: 0.265337 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "loss: 0.065797 [    0/ 3200]\n",
            "loss: 0.346336 [  160/ 3200]\n",
            "loss: 0.203609 [  320/ 3200]\n",
            "loss: 0.112010 [  480/ 3200]\n",
            "loss: 0.192501 [  640/ 3200]\n",
            "loss: 0.218763 [  800/ 3200]\n",
            "loss: 0.125265 [  960/ 3200]\n",
            "loss: 0.351708 [ 1120/ 3200]\n",
            "loss: 0.414705 [ 1280/ 3200]\n",
            "loss: 0.188366 [ 1440/ 3200]\n",
            "loss: 0.199436 [ 1600/ 3200]\n",
            "loss: 0.100886 [ 1760/ 3200]\n",
            "loss: 0.156053 [ 1920/ 3200]\n",
            "loss: 0.214286 [ 2080/ 3200]\n",
            "loss: 0.093970 [ 2240/ 3200]\n",
            "loss: 0.133680 [ 2400/ 3200]\n",
            "loss: 0.039940 [ 2560/ 3200]\n",
            "loss: 0.513260 [ 2720/ 3200]\n",
            "loss: 0.376514 [ 2880/ 3200]\n",
            "loss: 0.078203 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "loss: 0.107983 [    0/ 3200]\n",
            "loss: 0.155867 [  160/ 3200]\n",
            "loss: 0.152960 [  320/ 3200]\n",
            "loss: 0.236660 [  480/ 3200]\n",
            "loss: 0.259013 [  640/ 3200]\n",
            "loss: 0.127138 [  800/ 3200]\n",
            "loss: 0.168439 [  960/ 3200]\n",
            "loss: 0.060458 [ 1120/ 3200]\n",
            "loss: 0.052038 [ 1280/ 3200]\n",
            "loss: 0.157195 [ 1440/ 3200]\n",
            "loss: 0.117034 [ 1600/ 3200]\n",
            "loss: 0.272792 [ 1760/ 3200]\n",
            "loss: 0.050476 [ 1920/ 3200]\n",
            "loss: 0.148669 [ 2080/ 3200]\n",
            "loss: 0.095351 [ 2240/ 3200]\n",
            "loss: 0.144348 [ 2400/ 3200]\n",
            "loss: 0.172522 [ 2560/ 3200]\n",
            "loss: 0.235570 [ 2720/ 3200]\n",
            "loss: 0.065741 [ 2880/ 3200]\n",
            "loss: 0.103245 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "loss: 0.383790 [    0/ 3200]\n",
            "loss: 0.099631 [  160/ 3200]\n",
            "loss: 0.019393 [  320/ 3200]\n",
            "loss: 0.228772 [  480/ 3200]\n",
            "loss: 0.054124 [  640/ 3200]\n",
            "loss: 0.125351 [  800/ 3200]\n",
            "loss: 0.238540 [  960/ 3200]\n",
            "loss: 0.073222 [ 1120/ 3200]\n",
            "loss: 0.056432 [ 1280/ 3200]\n",
            "loss: 0.081951 [ 1440/ 3200]\n",
            "loss: 0.071984 [ 1600/ 3200]\n",
            "loss: 0.142508 [ 1760/ 3200]\n",
            "loss: 0.119325 [ 1920/ 3200]\n",
            "loss: 0.200038 [ 2080/ 3200]\n",
            "loss: 0.228837 [ 2240/ 3200]\n",
            "loss: 0.041602 [ 2400/ 3200]\n",
            "loss: 0.092001 [ 2560/ 3200]\n",
            "loss: 0.046755 [ 2720/ 3200]\n",
            "loss: 0.108628 [ 2880/ 3200]\n",
            "loss: 0.270164 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "loss: 0.111416 [    0/ 3200]\n",
            "loss: 0.114935 [  160/ 3200]\n",
            "loss: 0.092716 [  320/ 3200]\n",
            "loss: 0.062106 [  480/ 3200]\n",
            "loss: 0.085774 [  640/ 3200]\n",
            "loss: 0.033424 [  800/ 3200]\n",
            "loss: 0.089042 [  960/ 3200]\n",
            "loss: 0.091399 [ 1120/ 3200]\n",
            "loss: 0.025139 [ 1280/ 3200]\n",
            "loss: 0.038242 [ 1440/ 3200]\n",
            "loss: 0.045996 [ 1600/ 3200]\n",
            "loss: 0.193730 [ 1760/ 3200]\n",
            "loss: 0.131529 [ 1920/ 3200]\n",
            "loss: 0.074538 [ 2080/ 3200]\n",
            "loss: 0.085651 [ 2240/ 3200]\n",
            "loss: 0.072710 [ 2400/ 3200]\n",
            "loss: 0.028896 [ 2560/ 3200]\n",
            "loss: 0.035263 [ 2720/ 3200]\n",
            "loss: 0.056201 [ 2880/ 3200]\n",
            "loss: 0.109809 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "loss: 0.017859 [    0/ 3200]\n",
            "loss: 0.038770 [  160/ 3200]\n",
            "loss: 0.010035 [  320/ 3200]\n",
            "loss: 0.022830 [  480/ 3200]\n",
            "loss: 0.135139 [  640/ 3200]\n",
            "loss: 0.054654 [  800/ 3200]\n",
            "loss: 0.010119 [  960/ 3200]\n",
            "loss: 0.157281 [ 1120/ 3200]\n",
            "loss: 0.033155 [ 1280/ 3200]\n",
            "loss: 0.118363 [ 1440/ 3200]\n",
            "loss: 0.131558 [ 1600/ 3200]\n",
            "loss: 0.026530 [ 1760/ 3200]\n",
            "loss: 0.078444 [ 1920/ 3200]\n",
            "loss: 0.044003 [ 2080/ 3200]\n",
            "loss: 0.062275 [ 2240/ 3200]\n",
            "loss: 0.078497 [ 2400/ 3200]\n",
            "loss: 0.027438 [ 2560/ 3200]\n",
            "loss: 0.052743 [ 2720/ 3200]\n",
            "loss: 0.061227 [ 2880/ 3200]\n",
            "loss: 0.058342 [ 3040/ 3200]\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "loss: 0.030712 [    0/ 3200]\n",
            "loss: 0.009780 [  160/ 3200]\n",
            "loss: 0.011595 [  320/ 3200]\n",
            "loss: 0.009701 [  480/ 3200]\n",
            "loss: 0.039744 [  640/ 3200]\n",
            "loss: 0.067610 [  800/ 3200]\n",
            "loss: 0.031608 [  960/ 3200]\n",
            "loss: 0.040815 [ 1120/ 3200]\n",
            "loss: 0.154745 [ 1280/ 3200]\n",
            "loss: 0.025329 [ 1440/ 3200]\n",
            "loss: 0.026506 [ 1600/ 3200]\n",
            "loss: 0.056810 [ 1760/ 3200]\n",
            "loss: 0.105815 [ 1920/ 3200]\n",
            "loss: 0.050536 [ 2080/ 3200]\n",
            "loss: 0.062423 [ 2240/ 3200]\n",
            "loss: 0.057062 [ 2400/ 3200]\n",
            "loss: 0.010916 [ 2560/ 3200]\n",
            "loss: 0.008223 [ 2720/ 3200]\n",
            "loss: 0.028048 [ 2880/ 3200]\n",
            "loss: 0.050577 [ 3040/ 3200]\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "loss,f1,accuracy,cm = testCNN(testdataloader,lossfun,cnn_model_cpu)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "V7itBEpSqkTr",
        "outputId": "1b34ca1e-fd5f-405e-db0c-84c4d3354107"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.080100\n",
            "Accuracy: 70.1%\n",
            "F1 macro averaged: 0.706391\n",
            "Confusion matrix: [[252   6   7  32]\n",
            " [  7 279  39  31]\n",
            " [ 15  48 242  94]\n",
            " [ 16  42  75 191]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 5: Optimization algorithms***"
      ],
      "metadata": {
        "id": "WxiWWUmio-PK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def trainCNN(num_epochs,optimizer,trainloader,validloader,lossfun,cnn):\n",
        "  min_valid_loss = np.inf\n",
        "  size = len(trainloader.dataset)\n",
        "  for epoch in range(num_epochs):\n",
        "    train_loss = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(trainloader):\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        output = cnn(X)\n",
        "        loss = lossfun(output, labels)\n",
        "\n",
        "        # backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          loss,current = loss.item(), i*len(X)\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in validloader:\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X)\n",
        "      loss = lossfun(output, labels)\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "  return cnn\n",
        "\n",
        "def testCNN(dataloader,lossfun,cnn):\n",
        "\n",
        "  cnn.eval()\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  size = len(dataloader.dataset)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  # Iterate through test dataset\n",
        "  with torch.no_grad():\n",
        "    for X, labels in dataloader:\n",
        "      X = X.type(torch.float)\n",
        "      output = cnn(X)\n",
        "      test_loss += lossfun(output, labels).item()\n",
        "      correct += (output.argmax(1)==labels).type(torch.float).sum().item()\n",
        "      y_true.append(labels)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_true = torch.cat(y_true)\n",
        "  y_pred = torch.cat(y_pred)\n",
        "\n",
        "  correct /= size\n",
        "  accuracy = 100*correct\n",
        "  test_loss /= size\n",
        "  f1 = f1_score(y_true,y_pred,average='macro')\n",
        "  cm = confusion_matrix(y_true,y_pred)\n",
        "  return test_loss,f1,accuracy,cm"
      ],
      "metadata": {
        "id": "vGeOOoKqgDLZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "num_epochs = 30\n",
        "lossfun = nn.CrossEntropyLoss()\n",
        "learning_rate = 0.002\n",
        "cnn_model_cpu = CNNnew()\n",
        "\n",
        "#adadelta\n",
        "optimizer = torch.optim.Adadelta(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_adadelta = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_adadelta,accuracy_adadelta,cm = testCNN(testdataloader,lossfun,cnn_model_opt_adadelta)\n",
        "\n",
        "#adagrad\n",
        "optimizer = torch.optim.Adagrad(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_adagrad = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_adagrad,accuracy_adagrad,cm = testCNN(testdataloader,lossfun,cnn_model_opt_adagrad)\n",
        "\n",
        "#adam\n",
        "optimizer = torch.optim.Adam(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_adam = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_adam,accuracy_adam,cm = testCNN(testdataloader,lossfun,cnn_model_opt_adam)\n",
        "\n",
        "#adamw\n",
        "optimizer = torch.optim.AdamW(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_adamw = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_adamw,accuracy_adamw,cm = testCNN(testdataloader,lossfun,cnn_model_opt_adamw)\n",
        "\n",
        "\n",
        "#adamax\n",
        "optimizer = torch.optim.Adamax(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_adamax = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_adamax,accuracy_adamax,cm = testCNN(testdataloader,lossfun,cnn_model_opt_adamax)\n",
        "\n",
        "#ASGD\n",
        "optimizer = torch.optim.ASGD(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_ASGD = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_ASGD,accuracy_ASGD,cm = testCNN(testdataloader,lossfun,cnn_model_opt_ASGD)\n",
        "\n",
        "\n",
        "#Nadam\n",
        "optimizer = torch.optim.NAdam(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_nadam = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_nadam,accuracy_nadam,cm = testCNN(testdataloader,lossfun,cnn_model_opt_nadam)\n",
        "\n",
        "#radam\n",
        "optimizer = torch.optim.RAdam(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_radam = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_radam,accuracy_radam,cm = testCNN(testdataloader,lossfun,cnn_model_opt_radam)\n",
        "\n",
        "#rmsprop\n",
        "optimizer = torch.optim.RMSprop(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_rmsprop = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_rmsprop,accuracy_rmsprop,cm = testCNN(testdataloader,lossfun,cnn_model_opt_rmsprop)\n",
        "\n",
        "#rprop\n",
        "optimizer = torch.optim.Rprop(cnn_model_cpu.parameters(), lr=learning_rate)\n",
        "#train\n",
        "cnn_model_opt_rprop = trainCNN(num_epochs,optimizer,traindataloader,valdataloader,lossfun,cnn_model_cpu)\n",
        "#test\n",
        "loss,f1_rprop,accuracy_rprop,cm = testCNN(testdataloader,lossfun,cnn_model_opt_rprop)"
      ],
      "metadata": {
        "id": "4eoPM4ympKdz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#print table\n",
        "print (\"{:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20}\".format('Acc-f1/opt','Adadelta','Adagrad','Adam','AdamW','Adamax','ASGD','NAdam','RAdam','RMSprop','Rprop','SGD'))\n",
        "print(\"{:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} \".format('Accuracy',accuracy_adadelta,accuracy_adagrad,accuracy_adam,accuracy_adamw,accuracy_adamax,accuracy_ASGD,accuracy_nadam,accuracy_radam,accuracy_rmsprop,accuracy_rprop,accuracy))\n",
        "print(\"{:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} {:<20} \".format('F1',f1_adadelta,f1_adagrad,f1_adam,f1_adamw,f1_adamax,f1_ASGD,f1_nadam,f1_radam,f1_rmsprop,f1_rprop,f1))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47XiS8ODk9RA",
        "outputId": "ed72df0f-14db-4685-a52c-b5deb76be98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Acc-f1/opt           Adadelta             Adagrad              Adam                 AdamW                Adamax               ASGD                 NAdam                RAdam                RMSprop              Rprop                SGD                 \n",
            "Accuracy             65.26162790697676    73.69186046511628    63.372093023255815   71.51162790697676    72.31104651162791    72.38372093023256    61.19186046511628    21.584302325581394   21.584302325581394   21.584302325581394   71.43895348837209    \n",
            "F1                   0.6632756534164601   0.742461357845221    0.5874540628799221   0.7162392817526956   0.7248816838309695   0.725684533389867    0.6183975304279408   0.08876270173341302  0.08876270173341302  0.08876270173341302  0.7206613355256255   \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Accuracy and f1 score of adagrad optimizer are the best."
      ],
      "metadata": {
        "id": "KBwlAFQjAnuZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q3: Improving Performance**"
      ],
      "metadata": {
        "id": "NN9yNVvkHc0Y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 1: Reproducibility***"
      ],
      "metadata": {
        "id": "XykspwH6Hss1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import random,os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import f1_score, confusion_matrix\n",
        "\n",
        "def torch_seed(seed=0):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "\n",
        "def train(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNnew().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "def test(cnn,device='cuda'):\n",
        "\n",
        "  correct = 0\n",
        "  total = 0\n",
        "  test_loss = 0\n",
        "  size = len(testdataloader.dataset)\n",
        "  lossfun = nn.CrossEntropyLoss().to(device)\n",
        "  y_true = []\n",
        "  y_pred = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X, labels in testdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.float)\n",
        "      output = cnn(X)\n",
        "      test_loss += lossfun(output, labels).item()\n",
        "      correct += (output.argmax(1)==labels).type(torch.float).sum().item()\n",
        "      y_true.append(labels)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_true = torch.cat(y_true)\n",
        "  y_pred = torch.cat(y_pred)\n",
        "\n",
        "  correct /= size\n",
        "  accuracy = 100*correct\n",
        "  test_loss /= size\n",
        "\n",
        "  y_true = y_true.cpu()\n",
        "  y_pred = y_pred.cpu()\n",
        "\n",
        "  f1 = f1_score(y_true,y_pred,average='macro')\n",
        "  cm = confusion_matrix(y_true,y_pred)\n",
        "  return test_loss,f1,accuracy,cm"
      ],
      "metadata": {
        "id": "Bc-_Yox6H1q-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = train(reproducibility=False)"
      ],
      "metadata": {
        "id": "5mP7ZZBe-mXr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "77fd81cb-92dd-4552-ee5c-b0672de1d9e1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.4187\n",
            "[11, 200] loss: 16.6434\n",
            "[21, 200] loss: 9.5362\n",
            "[31, 200] loss: 6.9246\n",
            "[41, 200] loss: 5.5738\n",
            "[51, 200] loss: 4.7502\n",
            "[61, 200] loss: 4.1961\n",
            "[71, 200] loss: 3.7984\n",
            "[81, 200] loss: 3.4971\n",
            "[91, 200] loss: 3.2553\n",
            "[101, 200] loss: 3.0633\n",
            "[111, 200] loss: 2.9022\n",
            "[121, 200] loss: 2.7688\n",
            "[131, 200] loss: 2.6546\n",
            "[141, 200] loss: 2.5552\n",
            "[151, 200] loss: 2.4712\n",
            "[161, 200] loss: 2.3968\n",
            "[171, 200] loss: 2.3297\n",
            "[181, 200] loss: 2.2710\n",
            "[191, 200] loss: 2.2153\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3310\n",
            "[11, 200] loss: 28.1066\n",
            "[21, 200] loss: 15.4677\n",
            "[31, 200] loss: 10.9218\n",
            "[41, 200] loss: 8.5987\n",
            "[51, 200] loss: 7.1793\n",
            "[61, 200] loss: 6.2238\n",
            "[71, 200] loss: 5.5410\n",
            "[81, 200] loss: 5.0196\n",
            "[91, 200] loss: 4.6150\n",
            "[101, 200] loss: 4.2885\n",
            "[111, 200] loss: 4.0191\n",
            "[121, 200] loss: 3.7927\n",
            "[131, 200] loss: 3.6042\n",
            "[141, 200] loss: 3.4368\n",
            "[151, 200] loss: 3.2920\n",
            "[161, 200] loss: 3.1605\n",
            "[171, 200] loss: 3.0480\n",
            "[181, 200] loss: 2.9464\n",
            "[191, 200] loss: 2.8555\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.4537\n",
            "[11, 200] loss: 30.7740\n",
            "[21, 200] loss: 16.8065\n",
            "[31, 200] loss: 11.8319\n",
            "[41, 200] loss: 9.2818\n",
            "[51, 200] loss: 7.7132\n",
            "[61, 200] loss: 6.6597\n",
            "[71, 200] loss: 5.8974\n",
            "[81, 200] loss: 5.3261\n",
            "[91, 200] loss: 4.8838\n",
            "[101, 200] loss: 4.5233\n",
            "[111, 200] loss: 4.2219\n",
            "[121, 200] loss: 3.9692\n",
            "[131, 200] loss: 3.7620\n",
            "[141, 200] loss: 3.5777\n",
            "[151, 200] loss: 3.4137\n",
            "[161, 200] loss: 3.2757\n",
            "[171, 200] loss: 3.1479\n",
            "[181, 200] loss: 3.0365\n",
            "[191, 200] loss: 2.9366\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3704\n",
            "[11, 200] loss: 6.2091\n",
            "[21, 200] loss: 3.9333\n",
            "[31, 200] loss: 3.1098\n",
            "[41, 200] loss: 2.6723\n",
            "[51, 200] loss: 2.4044\n",
            "[61, 200] loss: 2.2208\n",
            "[71, 200] loss: 2.0912\n",
            "[81, 200] loss: 1.9919\n",
            "[91, 200] loss: 1.9084\n",
            "[101, 200] loss: 1.8361\n",
            "[111, 200] loss: 1.7844\n",
            "[121, 200] loss: 1.7309\n",
            "[131, 200] loss: 1.6860\n",
            "[141, 200] loss: 1.6486\n",
            "[151, 200] loss: 1.6119\n",
            "[161, 200] loss: 1.5806\n",
            "[171, 200] loss: 1.5572\n",
            "[181, 200] loss: 1.5320\n",
            "[191, 200] loss: 1.5036\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3994\n",
            "[11, 200] loss: 11.3773\n",
            "[21, 200] loss: 6.6068\n",
            "[31, 200] loss: 4.9130\n",
            "[41, 200] loss: 4.0485\n",
            "[51, 200] loss: 3.5148\n",
            "[61, 200] loss: 3.1594\n",
            "[71, 200] loss: 2.9014\n",
            "[81, 200] loss: 2.7042\n",
            "[91, 200] loss: 2.5487\n",
            "[101, 200] loss: 2.4232\n",
            "[111, 200] loss: 2.3178\n",
            "[121, 200] loss: 2.2267\n",
            "[131, 200] loss: 2.1493\n",
            "[141, 200] loss: 2.0860\n",
            "[151, 200] loss: 2.0280\n",
            "[161, 200] loss: 1.9766\n",
            "[171, 200] loss: 1.9300\n",
            "[181, 200] loss: 1.8884\n",
            "[191, 200] loss: 1.8503\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 12.6740\n",
            "[21, 200] loss: 7.3107\n",
            "[31, 200] loss: 5.3965\n",
            "[41, 200] loss: 4.4238\n",
            "[51, 200] loss: 3.8218\n",
            "[61, 200] loss: 3.4167\n",
            "[71, 200] loss: 3.1179\n",
            "[81, 200] loss: 2.8982\n",
            "[91, 200] loss: 2.7232\n",
            "[101, 200] loss: 2.5828\n",
            "[111, 200] loss: 2.4635\n",
            "[121, 200] loss: 2.3625\n",
            "[131, 200] loss: 2.2753\n",
            "[141, 200] loss: 2.1990\n",
            "[151, 200] loss: 2.1312\n",
            "[161, 200] loss: 2.0751\n",
            "[171, 200] loss: 2.0243\n",
            "[181, 200] loss: 1.9775\n",
            "[191, 200] loss: 1.9407\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3994\n",
            "[11, 200] loss: 7.5083\n",
            "[21, 200] loss: 4.6416\n",
            "[31, 200] loss: 3.5925\n",
            "[41, 200] loss: 3.0609\n",
            "[51, 200] loss: 2.7159\n",
            "[61, 200] loss: 2.4886\n",
            "[71, 200] loss: 2.3202\n",
            "[81, 200] loss: 2.1930\n",
            "[91, 200] loss: 2.0945\n",
            "[101, 200] loss: 2.0069\n",
            "[111, 200] loss: 1.9375\n",
            "[121, 200] loss: 1.8752\n",
            "[131, 200] loss: 1.8214\n",
            "[141, 200] loss: 1.7747\n",
            "[151, 200] loss: 1.7296\n",
            "[161, 200] loss: 1.6939\n",
            "[171, 200] loss: 1.6577\n",
            "[181, 200] loss: 1.6326\n",
            "[191, 200] loss: 1.6067\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3778\n",
            "[11, 200] loss: 24.3076\n",
            "[21, 200] loss: 13.4675\n",
            "[31, 200] loss: 9.5961\n",
            "[41, 200] loss: 7.5999\n",
            "[51, 200] loss: 6.3805\n",
            "[61, 200] loss: 5.5589\n",
            "[71, 200] loss: 4.9682\n",
            "[81, 200] loss: 4.5184\n",
            "[91, 200] loss: 4.1676\n",
            "[101, 200] loss: 3.8812\n",
            "[111, 200] loss: 3.6418\n",
            "[121, 200] loss: 3.4438\n",
            "[131, 200] loss: 3.2784\n",
            "[141, 200] loss: 3.1380\n",
            "[151, 200] loss: 3.0118\n",
            "[161, 200] loss: 2.9005\n",
            "[171, 200] loss: 2.8030\n",
            "[181, 200] loss: 2.7152\n",
            "[191, 200] loss: 2.6314\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.6595\n",
            "[11, 200] loss: 13.3419\n",
            "[21, 200] loss: 7.7394\n",
            "[31, 200] loss: 5.7035\n",
            "[41, 200] loss: 4.6575\n",
            "[51, 200] loss: 4.0150\n",
            "[61, 200] loss: 3.5628\n",
            "[71, 200] loss: 3.2460\n",
            "[81, 200] loss: 3.0044\n",
            "[91, 200] loss: 2.8093\n",
            "[101, 200] loss: 2.6526\n",
            "[111, 200] loss: 2.5284\n",
            "[121, 200] loss: 2.4193\n",
            "[131, 200] loss: 2.3257\n",
            "[141, 200] loss: 2.2452\n",
            "[151, 200] loss: 2.1800\n",
            "[161, 200] loss: 2.1136\n",
            "[171, 200] loss: 2.0580\n",
            "[181, 200] loss: 2.0083\n",
            "[191, 200] loss: 1.9618\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3815\n",
            "[11, 200] loss: 10.7908\n",
            "[21, 200] loss: 6.3206\n",
            "[31, 200] loss: 4.6991\n",
            "[41, 200] loss: 3.8624\n",
            "[51, 200] loss: 3.3646\n",
            "[61, 200] loss: 3.0143\n",
            "[71, 200] loss: 2.7690\n",
            "[81, 200] loss: 2.5737\n",
            "[91, 200] loss: 2.4158\n",
            "[101, 200] loss: 2.2862\n",
            "[111, 200] loss: 2.1963\n",
            "[121, 200] loss: 2.1080\n",
            "[131, 200] loss: 2.0310\n",
            "[141, 200] loss: 1.9774\n",
            "[151, 200] loss: 1.9214\n",
            "[161, 200] loss: 1.8723\n",
            "[171, 200] loss: 1.8255\n",
            "[181, 200] loss: 1.7801\n",
            "[191, 200] loss: 1.7390\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.7226\n",
            "[11, 200] loss: 12.4678\n",
            "[21, 200] loss: 7.1897\n",
            "[31, 200] loss: 5.2987\n",
            "[41, 200] loss: 4.3347\n",
            "[51, 200] loss: 3.7406\n",
            "[61, 200] loss: 3.3491\n",
            "[71, 200] loss: 3.0532\n",
            "[81, 200] loss: 2.8286\n",
            "[91, 200] loss: 2.6503\n",
            "[101, 200] loss: 2.5151\n",
            "[111, 200] loss: 2.3910\n",
            "[121, 200] loss: 2.2942\n",
            "[131, 200] loss: 2.2031\n",
            "[141, 200] loss: 2.1313\n",
            "[151, 200] loss: 2.0723\n",
            "[161, 200] loss: 2.0119\n",
            "[171, 200] loss: 1.9532\n",
            "[181, 200] loss: 1.9056\n",
            "[191, 200] loss: 1.8623\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3979\n",
            "[11, 200] loss: 15.3191\n",
            "[21, 200] loss: 8.7353\n",
            "[31, 200] loss: 6.3921\n",
            "[41, 200] loss: 5.1664\n",
            "[51, 200] loss: 4.4160\n",
            "[61, 200] loss: 3.9066\n",
            "[71, 200] loss: 3.5407\n",
            "[81, 200] loss: 3.2657\n",
            "[91, 200] loss: 3.0441\n",
            "[101, 200] loss: 2.8637\n",
            "[111, 200] loss: 2.7146\n",
            "[121, 200] loss: 2.5825\n",
            "[131, 200] loss: 2.4744\n",
            "[141, 200] loss: 2.3757\n",
            "[151, 200] loss: 2.2993\n",
            "[161, 200] loss: 2.2262\n",
            "[171, 200] loss: 2.1632\n",
            "[181, 200] loss: 2.1024\n",
            "[191, 200] loss: 2.0497\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.5773\n",
            "[11, 200] loss: 28.0469\n",
            "[21, 200] loss: 15.4888\n",
            "[31, 200] loss: 10.9402\n",
            "[41, 200] loss: 8.6133\n",
            "[51, 200] loss: 7.1902\n",
            "[61, 200] loss: 6.2170\n",
            "[71, 200] loss: 5.5183\n",
            "[81, 200] loss: 5.0028\n",
            "[91, 200] loss: 4.5899\n",
            "[101, 200] loss: 4.2600\n",
            "[111, 200] loss: 3.9830\n",
            "[121, 200] loss: 3.7591\n",
            "[131, 200] loss: 3.5683\n",
            "[141, 200] loss: 3.4039\n",
            "[151, 200] loss: 3.2612\n",
            "[161, 200] loss: 3.1323\n",
            "[171, 200] loss: 3.0173\n",
            "[181, 200] loss: 2.9183\n",
            "[191, 200] loss: 2.8321\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.4724\n",
            "[11, 200] loss: 18.7373\n",
            "[21, 200] loss: 10.5913\n",
            "[31, 200] loss: 7.6166\n",
            "[41, 200] loss: 6.0864\n",
            "[51, 200] loss: 5.1491\n",
            "[61, 200] loss: 4.5210\n",
            "[71, 200] loss: 4.0689\n",
            "[81, 200] loss: 3.7184\n",
            "[91, 200] loss: 3.4526\n",
            "[101, 200] loss: 3.2398\n",
            "[111, 200] loss: 3.0617\n",
            "[121, 200] loss: 2.9094\n",
            "[131, 200] loss: 2.7822\n",
            "[141, 200] loss: 2.6713\n",
            "[151, 200] loss: 2.5733\n",
            "[161, 200] loss: 2.4893\n",
            "[171, 200] loss: 2.4118\n",
            "[181, 200] loss: 2.3412\n",
            "[191, 200] loss: 2.2794\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.4268\n",
            "[11, 200] loss: 17.9413\n",
            "[21, 200] loss: 10.1406\n",
            "[31, 200] loss: 7.3286\n",
            "[41, 200] loss: 5.8717\n",
            "[51, 200] loss: 4.9815\n",
            "[61, 200] loss: 4.3775\n",
            "[71, 200] loss: 3.9442\n",
            "[81, 200] loss: 3.6217\n",
            "[91, 200] loss: 3.3663\n",
            "[101, 200] loss: 3.1651\n",
            "[111, 200] loss: 2.9939\n",
            "[121, 200] loss: 2.8525\n",
            "[131, 200] loss: 2.7290\n",
            "[141, 200] loss: 2.6222\n",
            "[151, 200] loss: 2.5293\n",
            "[161, 200] loss: 2.4501\n",
            "[171, 200] loss: 2.3771\n",
            "[181, 200] loss: 2.3111\n",
            "[191, 200] loss: 2.2532\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3680\n",
            "[11, 200] loss: 19.5581\n",
            "[21, 200] loss: 11.0251\n",
            "[31, 200] loss: 7.8909\n",
            "[41, 200] loss: 6.2758\n",
            "[51, 200] loss: 5.3016\n",
            "[61, 200] loss: 4.6374\n",
            "[71, 200] loss: 4.1609\n",
            "[81, 200] loss: 3.7990\n",
            "[91, 200] loss: 3.5191\n",
            "[101, 200] loss: 3.2929\n",
            "[111, 200] loss: 3.1174\n",
            "[121, 200] loss: 2.9577\n",
            "[131, 200] loss: 2.8187\n",
            "[141, 200] loss: 2.6983\n",
            "[151, 200] loss: 2.5973\n",
            "[161, 200] loss: 2.5094\n",
            "[171, 200] loss: 2.4308\n",
            "[181, 200] loss: 2.3588\n",
            "[191, 200] loss: 2.2898\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3969\n",
            "[11, 200] loss: 10.1101\n",
            "[21, 200] loss: 6.0123\n",
            "[31, 200] loss: 4.5272\n",
            "[41, 200] loss: 3.7591\n",
            "[51, 200] loss: 3.2822\n",
            "[61, 200] loss: 2.9594\n",
            "[71, 200] loss: 2.7323\n",
            "[81, 200] loss: 2.5531\n",
            "[91, 200] loss: 2.4199\n",
            "[101, 200] loss: 2.3075\n",
            "[111, 200] loss: 2.2152\n",
            "[121, 200] loss: 2.1361\n",
            "[131, 200] loss: 2.0717\n",
            "[141, 200] loss: 2.0050\n",
            "[151, 200] loss: 1.9506\n",
            "[161, 200] loss: 1.9008\n",
            "[171, 200] loss: 1.8596\n",
            "[181, 200] loss: 1.8185\n",
            "[191, 200] loss: 1.7838\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.4066\n",
            "[11, 200] loss: 9.7668\n",
            "[21, 200] loss: 5.7776\n",
            "[31, 200] loss: 4.3574\n",
            "[41, 200] loss: 3.6270\n",
            "[51, 200] loss: 3.1820\n",
            "[61, 200] loss: 2.8705\n",
            "[71, 200] loss: 2.6462\n",
            "[81, 200] loss: 2.4774\n",
            "[91, 200] loss: 2.3400\n",
            "[101, 200] loss: 2.2290\n",
            "[111, 200] loss: 2.1362\n",
            "[121, 200] loss: 2.0537\n",
            "[131, 200] loss: 1.9839\n",
            "[141, 200] loss: 1.9247\n",
            "[151, 200] loss: 1.8694\n",
            "[161, 200] loss: 1.8197\n",
            "[171, 200] loss: 1.7758\n",
            "[181, 200] loss: 1.7336\n",
            "[191, 200] loss: 1.6962\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.4165\n",
            "[11, 200] loss: 8.0200\n",
            "[21, 200] loss: 4.8603\n",
            "[31, 200] loss: 3.7209\n",
            "[41, 200] loss: 3.1222\n",
            "[51, 200] loss: 2.7648\n",
            "[61, 200] loss: 2.5249\n",
            "[71, 200] loss: 2.3543\n",
            "[81, 200] loss: 2.2120\n",
            "[91, 200] loss: 2.0980\n",
            "[101, 200] loss: 2.0108\n",
            "[111, 200] loss: 1.9324\n",
            "[121, 200] loss: 1.8738\n",
            "[131, 200] loss: 1.8187\n",
            "[141, 200] loss: 1.7671\n",
            "[151, 200] loss: 1.7313\n",
            "[161, 200] loss: 1.6922\n",
            "[171, 200] loss: 1.6545\n",
            "[181, 200] loss: 1.6155\n",
            "[191, 200] loss: 1.5867\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3915\n",
            "[11, 200] loss: 13.2300\n",
            "[21, 200] loss: 7.6130\n",
            "[31, 200] loss: 5.6015\n",
            "[41, 200] loss: 4.5677\n",
            "[51, 200] loss: 3.9304\n",
            "[61, 200] loss: 3.5031\n",
            "[71, 200] loss: 3.1963\n",
            "[81, 200] loss: 2.9603\n",
            "[91, 200] loss: 2.7780\n",
            "[101, 200] loss: 2.6219\n",
            "[111, 200] loss: 2.4978\n",
            "[121, 200] loss: 2.3951\n",
            "[131, 200] loss: 2.3059\n",
            "[141, 200] loss: 2.2297\n",
            "[151, 200] loss: 2.1571\n",
            "[161, 200] loss: 2.0933\n",
            "[171, 200] loss: 2.0390\n",
            "[181, 200] loss: 1.9984\n",
            "[191, 200] loss: 1.9536\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.4355\n",
            "[11, 200] loss: 10.8116\n",
            "[21, 200] loss: 6.3576\n",
            "[31, 200] loss: 4.7503\n",
            "[41, 200] loss: 3.9168\n",
            "[51, 200] loss: 3.4122\n",
            "[61, 200] loss: 3.0684\n",
            "[71, 200] loss: 2.8192\n",
            "[81, 200] loss: 2.6319\n",
            "[91, 200] loss: 2.4840\n",
            "[101, 200] loss: 2.3615\n",
            "[111, 200] loss: 2.2603\n",
            "[121, 200] loss: 2.1757\n",
            "[131, 200] loss: 2.1013\n",
            "[141, 200] loss: 2.0378\n",
            "[151, 200] loss: 1.9880\n",
            "[161, 200] loss: 1.9371\n",
            "[171, 200] loss: 1.8911\n",
            "[181, 200] loss: 1.8495\n",
            "[191, 200] loss: 1.8110\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3465\n",
            "[11, 200] loss: 10.9229\n",
            "[21, 200] loss: 6.4495\n",
            "[31, 200] loss: 4.7863\n",
            "[41, 200] loss: 3.9354\n",
            "[51, 200] loss: 3.4202\n",
            "[61, 200] loss: 3.0743\n",
            "[71, 200] loss: 2.8202\n",
            "[81, 200] loss: 2.6302\n",
            "[91, 200] loss: 2.4791\n",
            "[101, 200] loss: 2.3595\n",
            "[111, 200] loss: 2.2538\n",
            "[121, 200] loss: 2.1641\n",
            "[131, 200] loss: 2.0933\n",
            "[141, 200] loss: 2.0283\n",
            "[151, 200] loss: 1.9739\n",
            "[161, 200] loss: 1.9234\n",
            "[171, 200] loss: 1.8745\n",
            "[181, 200] loss: 1.8312\n",
            "[191, 200] loss: 1.7914\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.4055\n",
            "[11, 200] loss: 18.4726\n",
            "[21, 200] loss: 10.3571\n",
            "[31, 200] loss: 7.4854\n",
            "[41, 200] loss: 5.9917\n",
            "[51, 200] loss: 5.0849\n",
            "[61, 200] loss: 4.4724\n",
            "[71, 200] loss: 4.0252\n",
            "[81, 200] loss: 3.6900\n",
            "[91, 200] loss: 3.4252\n",
            "[101, 200] loss: 3.2133\n",
            "[111, 200] loss: 3.0353\n",
            "[121, 200] loss: 2.8849\n",
            "[131, 200] loss: 2.7586\n",
            "[141, 200] loss: 2.6535\n",
            "[151, 200] loss: 2.5577\n",
            "[161, 200] loss: 2.4732\n",
            "[171, 200] loss: 2.3958\n",
            "[181, 200] loss: 2.3279\n",
            "[191, 200] loss: 2.2747\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.4126\n",
            "[11, 200] loss: 19.6239\n",
            "[21, 200] loss: 10.9682\n",
            "[31, 200] loss: 7.8380\n",
            "[41, 200] loss: 6.2439\n",
            "[51, 200] loss: 5.2626\n",
            "[61, 200] loss: 4.5928\n",
            "[71, 200] loss: 4.1308\n",
            "[81, 200] loss: 3.7632\n",
            "[91, 200] loss: 3.4778\n",
            "[101, 200] loss: 3.2657\n",
            "[111, 200] loss: 3.0678\n",
            "[121, 200] loss: 2.9146\n",
            "[131, 200] loss: 2.7839\n",
            "[141, 200] loss: 2.6623\n",
            "[151, 200] loss: 2.5598\n",
            "[161, 200] loss: 2.4668\n",
            "[171, 200] loss: 2.3808\n",
            "[181, 200] loss: 2.3096\n",
            "[191, 200] loss: 2.2465\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3656\n",
            "[11, 200] loss: 19.0850\n",
            "[21, 200] loss: 10.7092\n",
            "[31, 200] loss: 7.7183\n",
            "[41, 200] loss: 6.1842\n",
            "[51, 200] loss: 5.2452\n",
            "[61, 200] loss: 4.6107\n",
            "[71, 200] loss: 4.1570\n",
            "[81, 200] loss: 3.8095\n",
            "[91, 200] loss: 3.5370\n",
            "[101, 200] loss: 3.3207\n",
            "[111, 200] loss: 3.1423\n",
            "[121, 200] loss: 2.9927\n",
            "[131, 200] loss: 2.8644\n",
            "[141, 200] loss: 2.7519\n",
            "[151, 200] loss: 2.6539\n",
            "[161, 200] loss: 2.5669\n",
            "[171, 200] loss: 2.4900\n",
            "[181, 200] loss: 2.4191\n",
            "[191, 200] loss: 2.3593\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3699\n",
            "[11, 200] loss: 11.2639\n",
            "[21, 200] loss: 6.6200\n",
            "[31, 200] loss: 4.9307\n",
            "[41, 200] loss: 4.0660\n",
            "[51, 200] loss: 3.5336\n",
            "[61, 200] loss: 3.1713\n",
            "[71, 200] loss: 2.9107\n",
            "[81, 200] loss: 2.7084\n",
            "[91, 200] loss: 2.5571\n",
            "[101, 200] loss: 2.4305\n",
            "[111, 200] loss: 2.3212\n",
            "[121, 200] loss: 2.2334\n",
            "[131, 200] loss: 2.1596\n",
            "[141, 200] loss: 2.0944\n",
            "[151, 200] loss: 2.0318\n",
            "[161, 200] loss: 1.9791\n",
            "[171, 200] loss: 1.9312\n",
            "[181, 200] loss: 1.8916\n",
            "[191, 200] loss: 1.8557\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3939\n",
            "[11, 200] loss: 11.9816\n",
            "[21, 200] loss: 6.9728\n",
            "[31, 200] loss: 5.1832\n",
            "[41, 200] loss: 4.2607\n",
            "[51, 200] loss: 3.6962\n",
            "[61, 200] loss: 3.3046\n",
            "[71, 200] loss: 3.0272\n",
            "[81, 200] loss: 2.8122\n",
            "[91, 200] loss: 2.6463\n",
            "[101, 200] loss: 2.5111\n",
            "[111, 200] loss: 2.3957\n",
            "[121, 200] loss: 2.2980\n",
            "[131, 200] loss: 2.2121\n",
            "[141, 200] loss: 2.1395\n",
            "[151, 200] loss: 2.0777\n",
            "[161, 200] loss: 2.0213\n",
            "[171, 200] loss: 1.9672\n",
            "[181, 200] loss: 1.9252\n",
            "[191, 200] loss: 1.8793\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.4938\n",
            "[11, 200] loss: 16.7382\n",
            "[21, 200] loss: 9.4101\n",
            "[31, 200] loss: 6.8021\n",
            "[41, 200] loss: 5.4709\n",
            "[51, 200] loss: 4.6512\n",
            "[61, 200] loss: 4.0981\n",
            "[71, 200] loss: 3.7009\n",
            "[81, 200] loss: 3.3990\n",
            "[91, 200] loss: 3.1607\n",
            "[101, 200] loss: 2.9634\n",
            "[111, 200] loss: 2.8031\n",
            "[121, 200] loss: 2.6662\n",
            "[131, 200] loss: 2.5493\n",
            "[141, 200] loss: 2.4487\n",
            "[151, 200] loss: 2.3630\n",
            "[161, 200] loss: 2.2846\n",
            "[171, 200] loss: 2.2157\n",
            "[181, 200] loss: 2.1526\n",
            "[191, 200] loss: 2.0983\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.4035\n",
            "[11, 200] loss: 15.9899\n",
            "[21, 200] loss: 9.0460\n",
            "[31, 200] loss: 6.5597\n",
            "[41, 200] loss: 5.2896\n",
            "[51, 200] loss: 4.5084\n",
            "[61, 200] loss: 3.9822\n",
            "[71, 200] loss: 3.5965\n",
            "[81, 200] loss: 3.3071\n",
            "[91, 200] loss: 3.0751\n",
            "[101, 200] loss: 2.8973\n",
            "[111, 200] loss: 2.7482\n",
            "[121, 200] loss: 2.6229\n",
            "[131, 200] loss: 2.5156\n",
            "[141, 200] loss: 2.4171\n",
            "[151, 200] loss: 2.3386\n",
            "[161, 200] loss: 2.2666\n",
            "[171, 200] loss: 2.2012\n",
            "[181, 200] loss: 2.1420\n",
            "[191, 200] loss: 2.0880\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.4369\n",
            "[11, 200] loss: 17.5722\n",
            "[21, 200] loss: 9.8888\n",
            "[31, 200] loss: 7.1173\n",
            "[41, 200] loss: 5.7109\n",
            "[51, 200] loss: 4.8515\n",
            "[61, 200] loss: 4.2685\n",
            "[71, 200] loss: 3.8472\n",
            "[81, 200] loss: 3.5284\n",
            "[91, 200] loss: 3.2795\n",
            "[101, 200] loss: 3.0789\n",
            "[111, 200] loss: 2.9065\n",
            "[121, 200] loss: 2.7688\n",
            "[131, 200] loss: 2.6452\n",
            "[141, 200] loss: 2.5411\n",
            "[151, 200] loss: 2.4485\n",
            "[161, 200] loss: 2.3674\n",
            "[171, 200] loss: 2.2926\n",
            "[181, 200] loss: 2.2282\n",
            "[191, 200] loss: 2.1729\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jbMfXLoDJfI5",
        "outputId": "5b9a4cd5-2222-4fc7-d37a-1ff4463392fd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.066257\n",
            "Accuracy: 52.0%\n",
            "F1 macro averaged: 0.459466\n",
            "Confusion matrix: [[281  10   0   6]\n",
            " [  8 305   1  42]\n",
            " [ 42 194   7 156]\n",
            " [ 51 151   0 122]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With reproducibility=False we receive a different loss in each epoch. On the contrary:"
      ],
      "metadata": {
        "id": "YwF_eBxdPNgZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = train(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6U6VELTIFYim",
        "outputId": "1f97dc15-0f08-467f-dc22-86150ea21602"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3874\n",
            "[11, 200] loss: 21.5548\n",
            "[21, 200] loss: 12.0052\n",
            "[31, 200] loss: 8.5890\n",
            "[41, 200] loss: 6.8233\n",
            "[51, 200] loss: 5.7557\n",
            "[61, 200] loss: 5.0319\n",
            "[71, 200] loss: 4.5135\n",
            "[81, 200] loss: 4.1183\n",
            "[91, 200] loss: 3.8103\n",
            "[101, 200] loss: 3.5636\n",
            "[111, 200] loss: 3.3610\n",
            "[121, 200] loss: 3.1906\n",
            "[131, 200] loss: 3.0440\n",
            "[141, 200] loss: 2.9188\n",
            "[151, 200] loss: 2.8095\n",
            "[161, 200] loss: 2.7118\n",
            "[171, 200] loss: 2.6274\n",
            "[181, 200] loss: 2.5524\n",
            "[191, 200] loss: 2.4831\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#test\n",
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "arMy_VwhNGid",
        "outputId": "8f51ec00-71d7-4414-a7eb-6c2adb8a1472"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.085846\n",
            "Accuracy: 42.2%\n",
            "F1 macro averaged: 0.328881\n",
            "Confusion matrix: [[225  72   0   0]\n",
            " [  7 348   0   1]\n",
            " [ 18 376   0   5]\n",
            " [ 20 297   0   7]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "With reproducibility=True we receive the same loss in each epoch."
      ],
      "metadata": {
        "id": "gEse7YoGPZNj"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Activation functions***"
      ],
      "metadata": {
        "id": "KDkkxCYtLozt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "\n",
        "#relu activation\n",
        "class CNNrelu(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def trainrelu(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNrelu().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "#softsign activation\n",
        "class CNNsoftsign(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.softsign(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.softsign(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.softsign(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.softsign(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.softsign(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.softsign(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.softsign(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def trainsoftsign(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNsoftsign().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "#elu activation\n",
        "class CNNelu(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.elu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.elu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.elu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.elu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.elu(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.elu(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.elu(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def trainelu(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNelu().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "#rrelu activation\n",
        "class CNNrrelu(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.rrelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.rrelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.rrelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.rrelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.rrelu(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.rrelu(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.rrelu(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def trainrrelu(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNrrelu().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "#gelu activation\n",
        "class CNNgelu(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.gelu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.gelu(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.gelu(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.gelu(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def traingelu(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNgelu().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "#hardshrink activation\n",
        "class CNNhardshrink(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = F.hardshrink(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = F.hardshrink(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = F.hardshrink(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = F.hardshrink(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.hardshrink(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.hardshrink(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.hardshrink(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def trainhardshrink(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNhardshrink().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),lr=0.002,weight_decay=0.0)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      optimizer.zero_grad()\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "      loss.backward()\n",
        "      optimizer.step()\n",
        "\n",
        "      if i%10 == 0:\n",
        "        print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "oVkHF-gMXY4G"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = trainrelu(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6EE2kElMaFGv",
        "outputId": "147f0603-9cf9-4995-e87a-c562a328f0c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4684\n",
            "[21, 200] loss: 1.4315\n",
            "[31, 200] loss: 1.4192\n",
            "[41, 200] loss: 1.4117\n",
            "[51, 200] loss: 1.4077\n",
            "[61, 200] loss: 1.4034\n",
            "[71, 200] loss: 1.4000\n",
            "[81, 200] loss: 1.3980\n",
            "[91, 200] loss: 1.3992\n",
            "[101, 200] loss: 1.3971\n",
            "[111, 200] loss: 1.3928\n",
            "[121, 200] loss: 1.3931\n",
            "[131, 200] loss: 1.3859\n",
            "[141, 200] loss: 1.3746\n",
            "[151, 200] loss: 1.3661\n",
            "[161, 200] loss: 1.3511\n",
            "[171, 200] loss: 1.3433\n",
            "[181, 200] loss: 1.3352\n",
            "[191, 200] loss: 1.3216\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1relu,accuracyrelu,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracyrelu:>0.1f}%\\nF1 macro averaged: {f1relu:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jJe0q9kDqECD",
        "outputId": "4976644b-8f0a-4e5f-a6d0-d8677a5f4a6d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.073233\n",
            "Accuracy: 42.8%\n",
            "F1 macro averaged: 0.363622\n",
            "Confusion matrix: [[202  34   0  61]\n",
            " [ 11 332   0  13]\n",
            " [ 21 339   0  39]\n",
            " [ 30 239   0  55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = trainrrelu(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VxvexuBapWH-",
        "outputId": "3f830dae-0ebd-4a66-f41d-a6777569af9e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3973\n",
            "[11, 200] loss: 1.6998\n",
            "[21, 200] loss: 1.5547\n",
            "[31, 200] loss: 1.5021\n",
            "[41, 200] loss: 1.4747\n",
            "[51, 200] loss: 1.4591\n",
            "[61, 200] loss: 1.4465\n",
            "[71, 200] loss: 1.4374\n",
            "[81, 200] loss: 1.4307\n",
            "[91, 200] loss: 1.4248\n",
            "[101, 200] loss: 1.4210\n",
            "[111, 200] loss: 1.4147\n",
            "[121, 200] loss: 1.4062\n",
            "[131, 200] loss: 1.3917\n",
            "[141, 200] loss: 1.3844\n",
            "[151, 200] loss: 1.3728\n",
            "[161, 200] loss: 1.3557\n",
            "[171, 200] loss: 1.3536\n",
            "[181, 200] loss: 1.3454\n",
            "[191, 200] loss: 1.3316\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1rrelu,accuracyrrelu,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracyrrelu:>0.1f}%\\nF1 macro averaged: {f1rrelu:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EH5E4S1MqHZu",
        "outputId": "93d75895-6985-46f7-a89a-0dc09ef9e192"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.074091\n",
            "Accuracy: 40.5%\n",
            "F1 macro averaged: 0.326928\n",
            "Confusion matrix: [[197  52   0  48]\n",
            " [ 20 334   0   2]\n",
            " [ 22 357   0  20]\n",
            " [ 31 267   0  26]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = traingelu(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0cwGoGWpiRo",
        "outputId": "8b16a1b6-33b3-4fce-e198-4865288e66cf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3975\n",
            "[11, 200] loss: 1.4755\n",
            "[21, 200] loss: 1.4334\n",
            "[31, 200] loss: 1.4174\n",
            "[41, 200] loss: 1.4150\n",
            "[51, 200] loss: 1.4062\n",
            "[61, 200] loss: 1.3962\n",
            "[71, 200] loss: 1.3912\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3672\n",
            "[101, 200] loss: 1.3649\n",
            "[111, 200] loss: 1.3665\n",
            "[121, 200] loss: 1.3660\n",
            "[131, 200] loss: 1.3599\n",
            "[141, 200] loss: 1.3525\n",
            "[151, 200] loss: 1.3415\n",
            "[161, 200] loss: 1.3230\n",
            "[171, 200] loss: 1.3140\n",
            "[181, 200] loss: 1.3045\n",
            "[191, 200] loss: 1.2870\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1gelu,accuracygelu,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracygelu:>0.1f}%\\nF1 macro averaged: {f1gelu:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "68oXbLZVqLEO",
        "outputId": "78734732-305b-46e5-af9e-12f305d5a6c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.078034\n",
            "Accuracy: 42.3%\n",
            "F1 macro averaged: 0.355436\n",
            "Confusion matrix: [[197  64   0  36]\n",
            " [  7 346   0   3]\n",
            " [ 11 350   5  33]\n",
            " [ 21 269   0  34]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = trainelu(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HX_LV414pjTt",
        "outputId": "7b8ef1e0-1f21-4641-c5da-57868dd82f97"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3950\n",
            "[11, 200] loss: 1.7439\n",
            "[21, 200] loss: 1.5815\n",
            "[31, 200] loss: 1.5189\n",
            "[41, 200] loss: 1.4853\n",
            "[51, 200] loss: 1.4635\n",
            "[61, 200] loss: 1.4401\n",
            "[71, 200] loss: 1.4444\n",
            "[81, 200] loss: 1.4279\n",
            "[91, 200] loss: 1.4145\n",
            "[101, 200] loss: 1.4036\n",
            "[111, 200] loss: 1.3875\n",
            "[121, 200] loss: 1.3651\n",
            "[131, 200] loss: 1.3490\n",
            "[141, 200] loss: 1.3297\n",
            "[151, 200] loss: 1.3101\n",
            "[161, 200] loss: 1.2864\n",
            "[171, 200] loss: 1.2751\n",
            "[181, 200] loss: 1.2612\n",
            "[191, 200] loss: 1.2425\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1elu,accuracyelu,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracyelu:>0.1f}%\\nF1 macro averaged: {f1elu:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqpxi8ABqOP0",
        "outputId": "68bf052e-ad96-4c73-9cba-fe74c83c35c1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.076074\n",
            "Accuracy: 45.3%\n",
            "F1 macro averaged: 0.415252\n",
            "Confusion matrix: [[154 130   0  13]\n",
            " [  8 347   0   1]\n",
            " [  9 258  94  38]\n",
            " [ 12 270  14  28]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = trainhardshrink(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jmNQVos-pj2D",
        "outputId": "5dbe071d-866e-4ce0-e167-481ede78d3b5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.3881\n",
            "[11, 200] loss: 13.5994\n",
            "[21, 200] loss: 7.7624\n",
            "[31, 200] loss: 5.6963\n",
            "[41, 200] loss: 4.6390\n",
            "[51, 200] loss: 3.9873\n",
            "[61, 200] loss: 3.5405\n",
            "[71, 200] loss: 3.2148\n",
            "[81, 200] loss: 2.9776\n",
            "[91, 200] loss: 2.7903\n",
            "[101, 200] loss: 2.6450\n",
            "[111, 200] loss: 2.5253\n",
            "[121, 200] loss: 2.4275\n",
            "[131, 200] loss: 2.3399\n",
            "[141, 200] loss: 2.2631\n",
            "[151, 200] loss: 2.1972\n",
            "[161, 200] loss: 2.1385\n",
            "[171, 200] loss: 2.0900\n",
            "[181, 200] loss: 2.0473\n",
            "[191, 200] loss: 2.0048\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1hardshrink,accuracyhardshrink,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracyhardshrink:>0.1f}%\\nF1 macro averaged: {f1hardshrink:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qlLLk0WMqTGP",
        "outputId": "92fb86fb-7766-4162-d8e7-3baf483b7a2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.083922\n",
            "Accuracy: 35.9%\n",
            "F1 macro averaged: 0.266586\n",
            "Confusion matrix: [[139 158   0   0]\n",
            " [  1 355   0   0]\n",
            " [  3 396   0   0]\n",
            " [ 11 313   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "cnn = trainsoftsign(reproducibility=True)"
      ],
      "metadata": {
        "id": "07WbWsSGpkk1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f246f1b2-2b41-49ab-bb7e-3cd375211a3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 2\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 3\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 4\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 5\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 6\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 7\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 8\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 9\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 10\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 11\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 12\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 13\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 14\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 15\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 16\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 17\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 18\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 19\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 20\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 21\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 22\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 23\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 24\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 25\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 26\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 27\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 28\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 29\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n",
            "Epoch: 30\n",
            "------------------\n",
            "[1, 200] loss: 1.4004\n",
            "[11, 200] loss: 1.4418\n",
            "[21, 200] loss: 1.4310\n",
            "[31, 200] loss: 1.4177\n",
            "[41, 200] loss: 1.4112\n",
            "[51, 200] loss: 1.4072\n",
            "[61, 200] loss: 1.4012\n",
            "[71, 200] loss: 1.4015\n",
            "[81, 200] loss: 1.3986\n",
            "[91, 200] loss: 1.3965\n",
            "[101, 200] loss: 1.3950\n",
            "[111, 200] loss: 1.3916\n",
            "[121, 200] loss: 1.3837\n",
            "[131, 200] loss: 1.3733\n",
            "[141, 200] loss: 1.3642\n",
            "[151, 200] loss: 1.3544\n",
            "[161, 200] loss: 1.3414\n",
            "[171, 200] loss: 1.3320\n",
            "[181, 200] loss: 1.3256\n",
            "[191, 200] loss: 1.3151\n",
            "\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1softsign,accuracysoftsign,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracysoftsign:>0.1f}%\\nF1 macro averaged: {f1softsign:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5P6PiLpmagom",
        "outputId": "02d3df09-f021-4929-dec9-3c5dc2551283"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.068166\n",
            "Accuracy: 47.0%\n",
            "F1 macro averaged: 0.376512\n",
            "Confusion matrix: [[274  16   0   7]\n",
            " [ 11 331   0  14]\n",
            " [ 76 277   0  46]\n",
            " [ 64 217   1  42]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#print table\n",
        "print (\"{:<20} {:<20} {:<20}\".format('activation/acc-f1','Accuracy','F1'))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('ReLU',accuracyrelu,f1relu))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('ELU',accuracyelu,f1elu))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('Hardshrink',accuracyhardshrink,f1hardshrink))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('RReLU',accuracyrrelu,f1rrelu))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('GELU',accuracygelu,f1gelu))\n",
        "print(\"{:<20} {:<20} {:<20}\".format('Softsign',accuracysoftsign,f1softsign))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwL2popYY6_J",
        "outputId": "ddce483a-dce3-4bf5-95ea-f974022aa143"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "activation/acc-f1    Accuracy             F1                  \n",
            "ReLU                 42.80523255813954    0.36362226725928304 \n",
            "ELU                  45.276162790697676   0.4152520959073677  \n",
            "Hardshrink           35.901162790697676   0.2665861527263734  \n",
            "RReLU                40.479651162790695   0.32692847975912886 \n",
            "GELU                 42.29651162790697    0.3554357517014028  \n",
            "Softsign             47.020348837209305   0.37651186025921335 \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: Learning rate scheduler***"
      ],
      "metadata": {
        "id": "yqpkwj52ctnl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.optim.lr_scheduler as lrscheduler\n",
        "\n",
        "def train(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNrelu().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "jBEx2fjgcsva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training with relu\n",
        "cnn = train(reproducibility=True)"
      ],
      "metadata": {
        "id": "Jt61M1selYhz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "540a45d9-185c-48e5-9fdc-8983eada9624"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 2\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 3\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 4\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 5\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 6\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 7\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 8\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 9\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 10\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 11\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 12\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 13\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 14\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 15\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 16\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 17\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 18\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 19\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 20\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 21\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 22\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 23\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 24\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 25\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 26\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 27\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 28\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 29\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 30\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.3981\n",
            "[11, 200] loss: 1.4024\n",
            "[21, 200] loss: 1.3969\n",
            "[31, 200] loss: 1.3950\n",
            "[41, 200] loss: 1.3929\n",
            "[51, 200] loss: 1.3915\n",
            "[61, 200] loss: 1.3862\n",
            "[71, 200] loss: 1.3824\n",
            "[81, 200] loss: 1.3800\n",
            "[91, 200] loss: 1.3773\n",
            "[101, 200] loss: 1.3756\n",
            "[111, 200] loss: 1.3714\n",
            "[121, 200] loss: 1.3661\n",
            "[131, 200] loss: 1.3605\n",
            "[141, 200] loss: 1.3546\n",
            "[151, 200] loss: 1.3502\n",
            "[161, 200] loss: 1.3431\n",
            "[171, 200] loss: 1.3398\n",
            "[181, 200] loss: 1.3376\n",
            "[191, 200] loss: 1.3320\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zvz068N9oT3v",
        "outputId": "adcf1bf7-b465-4c5b-fa50-eb09f8ef6736"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.078577\n",
            "Accuracy: 36.8%\n",
            "F1 macro averaged: 0.275876\n",
            "Confusion matrix: [[155 142   0   0]\n",
            " [  4 352   0   0]\n",
            " [  7 392   0   0]\n",
            " [ 14 310   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 4: Batch Normalization***"
      ],
      "metadata": {
        "id": "i0kA_Bw-siQc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu activation\n",
        "class CNNrelubatchnorm(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv1_bn = nn.BatchNorm2d(16)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv2_bn = nn.BatchNorm2d(32)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv3_bn = nn.BatchNorm2d(64)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "      self.conv4_bn = nn.BatchNorm2d(128)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = self.conv1_bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.conv2_bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv3_bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv4_bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc2(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc3(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x\n",
        "\n",
        "def train(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNrelubatchnorm().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "TTfunvW6sogH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#training with relu and batch normalization\n",
        "cnn = train(reproducibility=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NG1qQYnfi9fi",
        "outputId": "4e7091ea-1e35-4f4f-bc4c-a34dccb92f08"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 2\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 3\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 4\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 5\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 6\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 7\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 8\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 9\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 10\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 11\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 12\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 13\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 14\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 15\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 16\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 17\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 18\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 19\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 20\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 21\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 22\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 23\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 24\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 25\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 26\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 27\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 28\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 29\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 30\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4045\n",
            "[11, 200] loss: 1.3671\n",
            "[21, 200] loss: 1.3241\n",
            "[31, 200] loss: 1.2974\n",
            "[41, 200] loss: 1.2774\n",
            "[51, 200] loss: 1.2511\n",
            "[61, 200] loss: 1.2185\n",
            "[71, 200] loss: 1.1878\n",
            "[81, 200] loss: 1.1620\n",
            "[91, 200] loss: 1.1386\n",
            "[101, 200] loss: 1.1181\n",
            "[111, 200] loss: 1.0986\n",
            "[121, 200] loss: 1.0779\n",
            "[131, 200] loss: 1.0622\n",
            "[141, 200] loss: 1.0455\n",
            "[151, 200] loss: 1.0326\n",
            "[161, 200] loss: 1.0163\n",
            "[171, 200] loss: 1.0076\n",
            "[181, 200] loss: 1.0039\n",
            "[191, 200] loss: 0.9913\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lir6sH4hkyKA",
        "outputId": "4ba931f8-d3fa-4770-89c4-87626874142c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.047684\n",
            "Accuracy: 72.8%\n",
            "F1 macro averaged: 0.719506\n",
            "Confusion matrix: [[284   2   3   8]\n",
            " [  2 305  12  37]\n",
            " [ 24  34 274  67]\n",
            " [ 30  98  57 139]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 5: Regularization***"
      ],
      "metadata": {
        "id": "0KvchGTSRJm9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "i) weight_decay at optimizer:"
      ],
      "metadata": {
        "id": "CJb2VOnoTcJm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainweight(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNrelubatchnorm().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002,weight_decay=0.9)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    #loss_avg = 0.0\n",
        "    train_loss = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        #loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(traindataloader)} \\t\\t Validation Loss: {valid_loss / len(valdataloader)}')\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "M7gGhtUkTqsk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "ii) Dropout at linear layers:"
      ],
      "metadata": {
        "id": "1gU3O5bsTG4o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#relu activation\n",
        "class CNNdropout(nn.Module):\n",
        "    def __init__(self):\n",
        "      super().__init__()\n",
        "\n",
        "      #in/out channels: 1,16,32,64,128, kernel size = 5\n",
        "      self.conv1 = nn.Conv2d(1, 16, 5, padding=2)\n",
        "      self.conv1bn = nn.BatchNorm2d(16)\n",
        "      self.conv2 = nn.Conv2d(16, 32, 5, padding=2)\n",
        "      self.conv2bn = nn.BatchNorm2d(32)\n",
        "      self.conv3 = nn.Conv2d(32, 64, 5, padding=2)\n",
        "      self.conv3bn = nn.BatchNorm2d(64)\n",
        "      self.conv4 = nn.Conv2d(64, 128, 5, padding=2)\n",
        "      self.conv4bn = nn.BatchNorm2d(128)\n",
        "\n",
        "      self.max_pool = nn.MaxPool2d(kernel_size=2)\n",
        "\n",
        "      self.fc1 = nn.Linear(1024, 1024)\n",
        "      self.fc2 = nn.Linear(1024, 256)\n",
        "      self.fc3 = nn.Linear(256, 32)\n",
        "      self.fc4 = nn.Linear(32, 4)\n",
        "\n",
        "      self.dropout = nn.Dropout(0.25)\n",
        "\n",
        "    # x represents our data\n",
        "    def forward(self, x):\n",
        "\n",
        "      x = torch.unsqueeze(x,1)\n",
        "\n",
        "      x = self.conv1(x)\n",
        "      x = self.conv1bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv2(x)\n",
        "      x = self.conv2bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv3(x)\n",
        "      x = self.conv3bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x = self.conv4(x)\n",
        "      x = self.conv4bn(x)\n",
        "      x = F.relu(x)\n",
        "      x = self.max_pool(x)\n",
        "\n",
        "      x= x.view(x.size(0),-1)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc1(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc2(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc3(x)\n",
        "      x = F.relu(x)\n",
        "\n",
        "      x = self.dropout(x)\n",
        "      x = self.fc4(x)\n",
        "\n",
        "      return x"
      ],
      "metadata": {
        "id": "FFtrCRpQR-PO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only i)"
      ],
      "metadata": {
        "id": "vSjpJq5aXTV9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#training with relu and batch normalization\n",
        "cnn = trainweight(reproducibility=True,num_epochs=60,device='cuda')"
      ],
      "metadata": {
        "id": "ZPfgoULaXalJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9172af0c-442d-4805-b835-c1c788853c57"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.382854090332985 \t\t Validation Loss: 1.3803259134292603\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "id": "cQF1oDYMXpuZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ea1ae626-85f3-4d95-a925-e8462bada3c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.086543\n",
            "Accuracy: 39.0%\n",
            "F1 macro averaged: 0.288661\n",
            "Confusion matrix: [[197 100   0   0]\n",
            " [ 16 340   0   0]\n",
            " [ 31 368   0   0]\n",
            " [ 32 292   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Only ii)"
      ],
      "metadata": {
        "id": "2p099l5pXsO8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def traindropout(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "    print(f\"Epoch: {epoch+1}\\n------------------\")\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNdropout().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if i%10 == 0:\n",
        "          print('[%d,%4d] loss: %.4f' %(i+1,len(traindataloader),loss_avg))\n",
        "    print(\"\\n\")\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "cnn = traindropout(reproducibility=True,num_epochs=60,device='cuda')"
      ],
      "metadata": {
        "id": "sVk_xgReXrBk",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "269db13b-9982-47c0-b927-e4fc71150cf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 2\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 3\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 4\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 5\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 6\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 7\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 8\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 9\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 10\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 11\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 12\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 13\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 14\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 15\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 16\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 17\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 18\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 19\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 20\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 21\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 22\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 23\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 24\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 25\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 26\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 27\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 28\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 29\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 30\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 31\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 32\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 33\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 34\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 35\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 36\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 37\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 38\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 39\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 40\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 41\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 42\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 43\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 44\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 45\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 46\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 47\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 48\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 49\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 50\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 51\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 52\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 53\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 54\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 55\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 56\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 57\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 58\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 59\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch: 60\n",
            "------------------\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "[1, 200] loss: 1.4256\n",
            "[11, 200] loss: 1.3973\n",
            "[21, 200] loss: 1.3787\n",
            "[31, 200] loss: 1.3694\n",
            "[41, 200] loss: 1.3609\n",
            "[51, 200] loss: 1.3513\n",
            "[61, 200] loss: 1.3402\n",
            "[71, 200] loss: 1.3266\n",
            "[81, 200] loss: 1.3168\n",
            "[91, 200] loss: 1.3054\n",
            "[101, 200] loss: 1.2990\n",
            "[111, 200] loss: 1.2936\n",
            "[121, 200] loss: 1.2840\n",
            "[131, 200] loss: 1.2801\n",
            "[141, 200] loss: 1.2723\n",
            "[151, 200] loss: 1.2662\n",
            "[161, 200] loss: 1.2589\n",
            "[171, 200] loss: 1.2530\n",
            "[181, 200] loss: 1.2499\n",
            "[191, 200] loss: 1.2426\n",
            "\n",
            "\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "id": "hmrycXHsYEqj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4da93f18-d178-4b05-9550-a2848f9a3aaf"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.065307\n",
            "Accuracy: 67.2%\n",
            "F1 macro averaged: 0.628376\n",
            "Confusion matrix: [[279   4  14   0]\n",
            " [ 16 277  33  30]\n",
            " [ 34  26 313  26]\n",
            " [ 62  95 112  55]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Both i) and ii)"
      ],
      "metadata": {
        "id": "5ZThOUgaX0SH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNdropout().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002,weight_decay=0.9)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    #loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    train_loss = 0.0\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        #loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(traindataloader)} \\t\\t Validation Loss: {valid_loss / len(valdataloader)}')\n",
        "\n",
        "    if min_valid_loss > valid_loss:\n",
        "      min_valid_loss = valid_loss\n",
        "\n",
        "      # Saving State Dict\n",
        "      torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn\n",
        "\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')"
      ],
      "metadata": {
        "id": "uCAw6GGPX3b6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "83628b6e-d44c-4efd-c4af-531f1509da5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")"
      ],
      "metadata": {
        "id": "4Uma5iqKYGAZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8cf10d22-57d7-40b7-d0c8-dfe523e29be2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.086662\n",
            "Accuracy: 26.5%\n",
            "F1 macro averaged: 0.120715\n",
            "Confusion matrix: [[ 11 286   0   0]\n",
            " [  2 354   0   0]\n",
            " [  0 399   0   0]\n",
            " [  2 322   0   0]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 6: Training efficiency***"
      ],
      "metadata": {
        "id": "YtEiWt3LYuWw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^1\n",
        "traindataloader = DataLoader(traindata,batch_size = 2, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=2, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=2,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "id": "nnYX0FHOY4QL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "37574f2b-ef2c-4f93-c683-836dfb874600"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.386064910441637 \t\t Validation Loss: 1.3849024721980094\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HiBc0k7n41bt",
        "outputId": "f4a8f84f-9f10-492d-8283-623bc758dab5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.694164\n",
            "Accuracy: 25.9%\n",
            "F1 macro averaged: 0.102771\n",
            "Confusion matrix: [[  0 297   0   0]\n",
            " [  0 356   0   0]\n",
            " [  0 399   0   0]\n",
            " [  0 324   0   0]]\n",
            "Time to train model: 565.2042505741119\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^2\n",
        "traindataloader = DataLoader(traindata,batch_size = 4, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=4, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=4,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5vNSbkA1xrru",
        "outputId": "3628f9e4-59c9-4373-9c6b-f10871721aad"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.385731588602066 \t\t Validation Loss: 1.3813027930259705\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PVzjsqG7yrpg",
        "outputId": "2b532baa-a455-4fe1-dd53-766970b16137"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.346353\n",
            "Accuracy: 25.9%\n",
            "F1 macro averaged: 0.102771\n",
            "Confusion matrix: [[  0 297   0   0]\n",
            " [  0 356   0   0]\n",
            " [  0 399   0   0]\n",
            " [  0 324   0   0]]\n",
            "Time to train model: 296.3159604072571\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^3\n",
        "traindataloader = DataLoader(traindata,batch_size = 8, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=8, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=8,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pyh327lEx6De",
        "outputId": "d4c54192-394e-49f0-9c87-3ffa5ed42bbd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3843940129876138 \t\t Validation Loss: 1.3791736567020416\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7z7OonRayuEr",
        "outputId": "43626864-7dd9-4793-f58e-eca29c1a241d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.172971\n",
            "Accuracy: 25.9%\n",
            "F1 macro averaged: 0.102771\n",
            "Confusion matrix: [[  0 297   0   0]\n",
            " [  0 356   0   0]\n",
            " [  0 399   0   0]\n",
            " [  0 324   0   0]]\n",
            "Time to train model: 159.73313999176025\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^4 --> previous batch\n",
        "traindataloader = DataLoader(traindata,batch_size = 16, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=16, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=16,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UnhUgauvyFF9",
        "outputId": "3e48628a-241b-40b4-8c73-e372fb1249c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3880105781555176 \t\t Validation Loss: 1.3818868350982667\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UHNaHPrWyvC_",
        "outputId": "01c95073-2fcf-4a18-e172-27e172147638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.086662\n",
            "Accuracy: 26.5%\n",
            "F1 macro averaged: 0.120715\n",
            "Confusion matrix: [[ 11 286   0   0]\n",
            " [  2 354   0   0]\n",
            " [  0 399   0   0]\n",
            " [  2 322   0   0]]\n",
            "Time to train model: 82.49516177177429\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^5\n",
        "traindataloader = DataLoader(traindata,batch_size = 32, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=32, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=32,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XPGMPOXiyWSC",
        "outputId": "8f504c85-bc84-43e3-b361-ae3d55d39d78"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3885050678253175 \t\t Validation Loss: 1.3828732585906982\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iI7ZSUX1yv1v",
        "outputId": "1874f38e-372f-4ef2-9748-9de265e6d23d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.043358\n",
            "Accuracy: 26.4%\n",
            "F1 macro averaged: 0.120193\n",
            "Confusion matrix: [[ 11 286   0   0]\n",
            " [  4 352   0   0]\n",
            " [  5 394   0   0]\n",
            " [  6 318   0   0]]\n",
            "Time to train model: 47.02092146873474\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^6\n",
        "traindataloader = DataLoader(traindata,batch_size = 64, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=64, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=64,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ds2o-6HaycD6",
        "outputId": "f0a9ae45-05a3-441f-8940-d872c09d6947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3889763379096984 \t\t Validation Loss: 1.385788789162269\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HTzsb2rWywgn",
        "outputId": "4d8d8e45-12db-4130-a59d-5d9538076fa0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.022265\n",
            "Accuracy: 26.7%\n",
            "F1 macro averaged: 0.133166\n",
            "Confusion matrix: [[ 21 276   0   0]\n",
            " [ 10 346   0   0]\n",
            " [  5 394   0   0]\n",
            " [ 12 312   0   0]]\n",
            "Time to train model: 33.89110994338989\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "#Dataloaders --> batch size = 2^7\n",
        "traindataloader = DataLoader(traindata,batch_size = 128, shuffle=True)\n",
        "testdataloader = DataLoader(testdata,batch_size=128, shuffle=False)\n",
        "valdataloader = DataLoader(valdata,batch_size=128,shuffle=True)\n",
        "\n",
        "#train\n",
        "start_train = time.time()\n",
        "cnn = train(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FyAaDzzoyhn2",
        "outputId": "7ac3f97b-6321-43ba-ffc1-a3e54b6da7af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TW_mO1DryxJ0",
        "outputId": "9b227f4d-ed5b-4e72-ded5-b4eefdb87ce2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.011155\n",
            "Accuracy: 25.9%\n",
            "F1 macro averaged: 0.102771\n",
            "Confusion matrix: [[  0 297   0   0]\n",
            " [  0 356   0   0]\n",
            " [  0 399   0   0]\n",
            " [  0 324   0   0]]\n",
            "Time to train model: 27.92289972305298\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The bigger the batch size, the less time it takes to train out model."
      ],
      "metadata": {
        "id": "fAT0E41OUI-p"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def trainEarlyStopping(reproducibility=True, num_epochs=30, device='cuda'):\n",
        "  min_valid_loss = np.inf\n",
        "  patience = 2\n",
        "  for epoch in range(num_epochs):\n",
        "\n",
        "    if reproducibility:\n",
        "      torch_seed(seed=0)\n",
        "\n",
        "    cnn = CNNdropout().to(device)\n",
        "    lossfun = nn.CrossEntropyLoss().to(device)\n",
        "    optimizer = optim.Adagrad(cnn.parameters(),0.002,weight_decay=0.9)\n",
        "    scheduler1 = lrscheduler.ExponentialLR(optimizer,gamma=0.9,verbose=True)\n",
        "    scheduler2 = lrscheduler.MultiStepLR(optimizer,milestones=[30,80],gamma=0.1,verbose=True)\n",
        "    scheduler3 = lrscheduler.LinearLR(optimizer,verbose=True)\n",
        "    scheduler4 = lrscheduler.ConstantLR(optimizer,verbose=True)\n",
        "\n",
        "    #loss_avg = 0.0\n",
        "    cnn.train()\n",
        "    train_loss = 0.0\n",
        "    for i, (X, labels) in enumerate(traindataloader):\n",
        "        #gpu\n",
        "        X = X.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        X = X.type(torch.FloatTensor)\n",
        "        optimizer.zero_grad()\n",
        "        output = cnn(X.to(device))\n",
        "        loss = lossfun(output, labels.to(device))\n",
        "        #loss_avg = (loss_avg*i+loss)/(i+1)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss += loss.item()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "    scheduler3.step()\n",
        "    scheduler4.step()\n",
        "\n",
        "    valid_loss = 0.0\n",
        "    cnn.eval()\n",
        "    for X, labels in valdataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "      labels = labels.to(device)\n",
        "      X = X.type(torch.FloatTensor)\n",
        "      output = cnn(X.to(device))\n",
        "      loss = lossfun(output, labels.to(device))\n",
        "      valid_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch {epoch+1} \\t\\t Training Loss: {train_loss / len(traindataloader)} \\t\\t Validation Loss: {valid_loss / len(valdataloader)}')\n",
        "\n",
        "    # Early stopping\n",
        "    if valid_loss > min_valid_loss:\n",
        "        trigger_times += 1\n",
        "        if trigger_times >= patience:\n",
        "            print('Early stopping!\\nStart to test process.')\n",
        "            return cnn\n",
        "    else:\n",
        "        trigger_times = 0\n",
        "        min_valid_loss = valid_loss\n",
        "        # Saving State Dict\n",
        "        torch.save(cnn.state_dict(), 'saved_model.pth')\n",
        "\n",
        "  return cnn"
      ],
      "metadata": {
        "id": "VrdUfCQh5T29"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "start_train = time.time()\n",
        "cnn = trainEarlyStopping(reproducibility=True,num_epochs=60,device='cuda')\n",
        "end_train = time.time() - start_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5gxOGJas-NUY",
        "outputId": "0336293e-a33d-43b9-9ea7-989feacdb00b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 1 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 2 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 3 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 4 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 5 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 6 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 7 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 8 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 9 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 10 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 11 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 12 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 13 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 14 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 15 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 16 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 17 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 18 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 19 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 20 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 21 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 22 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 23 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 24 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 25 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 26 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 27 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 28 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 29 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 30 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 31 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 32 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 33 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 34 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 35 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 36 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 37 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 38 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 39 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 40 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 41 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 42 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 43 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 44 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 45 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 46 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 47 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 48 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 49 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 50 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 51 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 52 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 53 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 54 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 55 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 56 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 57 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 58 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 59 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 2.0000e-03.\n",
            "Adjusting learning rate of group 0 to 6.6667e-04.\n",
            "Adjusting learning rate of group 0 to 2.2222e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.0000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Adjusting learning rate of group 0 to 2.8000e-04.\n",
            "Epoch 60 \t\t Training Loss: 1.3890688610076904 \t\t Validation Loss: 1.3889995983668737\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "loss,f1,accuracy,cm = test(cnn)\n",
        "print(f\"Loss: {loss:>8f}\\nAccuracy: {accuracy:>0.1f}%\\nF1 macro averaged: {f1:>8f}\\nConfusion matrix: {cm}\")\n",
        "print(f\"Time to train model: {end_train}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kvj-cv2F_s7_",
        "outputId": "1ff9fb30-6402-4f09-f4d4-b8b648e06fbe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Loss: 0.011155\n",
            "Accuracy: 25.9%\n",
            "F1 macro averaged: 0.102771\n",
            "Confusion matrix: [[  0 297   0   0]\n",
            " [  0 356   0   0]\n",
            " [  0 399   0   0]\n",
            " [  0 324   0   0]]\n",
            "Time to train model: 31.42580246925354\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Q4: Testing**"
      ],
      "metadata": {
        "id": "mO6WRJyzuEiV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 1: Inference***"
      ],
      "metadata": {
        "id": "cFSQ8857uJbv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def predictions(dataloader,cnn,device='cuda'):\n",
        "\n",
        "  y_pred = []\n",
        "\n",
        "  with torch.no_grad():\n",
        "    for X in dataloader:\n",
        "      #gpu\n",
        "      X = X.to(device)\n",
        "\n",
        "      X = X.type(torch.float)\n",
        "      output = cnn(X)\n",
        "      y_pred.append(output.argmax(1))\n",
        "\n",
        "  y_pred = torch.cat(y_pred)\n",
        "  y_pred = y_pred.cpu()\n",
        "\n",
        "  return y_pred"
      ],
      "metadata": {
        "id": "rYP8ZczJuRUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 2: Downloading music***"
      ],
      "metadata": {
        "id": "dcfpLp7exlF1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!sudo apt-get update\n",
        "!sudo curl -L https://yt-dl.org/downloads/latest/youtube-dl -o /usr/local/bin/youtube-dl\n",
        "!sudo chmod a+rx /usr/local/bin/youtube-dl"
      ],
      "metadata": {
        "id": "_L3Ffd3Pxy2L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "import youtube as yt\n",
        "\n",
        "#classical\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=9E6b3swbnWg')\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "#pop\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=EDwb9jOVRtU')\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "#rock\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=OMaycNcPsHI')\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "#blues\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=l45f28PzfCI')\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")"
      ],
      "metadata": {
        "id": "ZQ-FcNNkDOfA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "***Step 3: Predictions***"
      ],
      "metadata": {
        "id": "gftJl9Bu4WaH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "import youtube as yt\n",
        "\n",
        "#classical\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=9E6b3swbnWg')\n",
        "classical_melgrams = np.load(\"youtube_melgrams.npy\")\n",
        "classicaloader = DataLoader(classical_melgrams,batch_size=16,shuffle=False)\n",
        "classical_pred = predictions(classicaloader,cnn)"
      ],
      "metadata": {
        "id": "GtuGU1nz4VsQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "60d76246-175e-4da0-8233-cf117fe24887"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "import youtube as yt\n",
        "\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "\n",
        "#pop\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=EDwb9jOVRtU')\n",
        "pop_melgrams = np.load(\"youtube_melgrams.npy\")\n",
        "poploader = DataLoader(pop_melgrams,batch_size=16,shuffle=False)\n",
        "pop_pred = predictions(poploader,cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-3edAf-Py_HP",
        "outputId": "f3889238-9668-410c-a0dd-a78dd6ab5b02"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "import youtube as yt\n",
        "\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "\n",
        "#rock\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=OMaycNcPsHI')\n",
        "rock_melgrams = np.load(\"youtube_melgrams.npy\")\n",
        "rockloader = DataLoader(rock_melgrams,batch_size=16,shuffle=False)\n",
        "rock_pred = predictions(rockloader,cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iJVh_dP8zlqY",
        "outputId": "418175d3-ea8f-4647-ad93-62ccc5b5607b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import sys\n",
        "import os\n",
        "sys.path.append(\"/content/drive/MyDrive\")\n",
        "import youtube as yt\n",
        "\n",
        "os.remove(\"youtube_melgrams.npy\")\n",
        "os.remove(\"temp.wav\")\n",
        "\n",
        "#blues\n",
        "yt.youtube_to_melgram('https://www.youtube.com/watch?v=l45f28PzfCI')\n",
        "blues_melgrams = np.load(\"youtube_melgrams.npy\")\n",
        "bluesloader = DataLoader(blues_melgrams,batch_size=16,shuffle=False)\n",
        "blues_pred = predictions(bluesloader,cnn)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VmCGP619zmdu",
        "outputId": "0294dd99-82a2-4428-8d77-80110abe9031"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/librosa/core/audio.py:165: UserWarning: PySoundFile failed. Trying audioread instead.\n",
            "  warnings.warn(\"PySoundFile failed. Trying audioread instead.\")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.figure(1)\n",
        "plt.ylim(0,3)\n",
        "plt.xlim(0,60)\n",
        "plt.ylabel(\"Music genres\")\n",
        "plt.xlabel(\"Timestamps - 1min\")\n",
        "plt.plot(classical_pred)\n",
        "\n",
        "plt.figure(2)\n",
        "plt.ylim(0,3)\n",
        "plt.xlim(0,60)\n",
        "plt.ylabel(\"Music genres\")\n",
        "plt.xlabel(\"Timestamps - 1min\")\n",
        "plt.plot(pop_pred)\n",
        "\n",
        "plt.figure(3)\n",
        "plt.ylim(0,3)\n",
        "plt.xlim(0,60)\n",
        "plt.ylabel(\"Music genres\")\n",
        "plt.xlabel(\"Timestamps - 1min\")\n",
        "plt.plot(rock_pred)\n",
        "\n",
        "plt.figure(4)\n",
        "plt.ylim(0,3)\n",
        "plt.xlim(0,60)\n",
        "plt.ylabel(\"Music genres\")\n",
        "plt.xlabel(\"Timestamps - 1min\")\n",
        "plt.plot(blues_pred)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "wHbM86sd2EfV",
        "outputId": "35894b7e-d9ca-4d16-9373-d1f1c528a409"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[<matplotlib.lines.Line2D at 0x7fb6a7644ad0>]"
            ]
          },
          "metadata": {},
          "execution_count": 139
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {
            "needs_background": "light"
          }
        }
      ]
    }
  ]
}